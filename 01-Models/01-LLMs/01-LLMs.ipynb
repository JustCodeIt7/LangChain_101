{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d84f7735edbc5c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ab62d0aaa46c81",
   "metadata": {},
   "source": [
    "# Chat with Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60ed0185134e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e3216054a83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"qwen2.5:0.5b\", temperature=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40e65c7aa3779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello! I'm just a large language model, so I don't have feelings or emotions in the same way that humans do. However, I can assist you with any questions or tasks you might have. How can I help you today?\" additional_kwargs={} response_metadata={'model': 'qwen2.5:0.5b', 'created_at': '2025-06-12T20:22:44.365071Z', 'done': True, 'done_reason': 'stop', 'total_duration': 15240882709, 'load_duration': 14797377084, 'prompt_eval_count': 36, 'prompt_eval_duration': 76149750, 'eval_count': 49, 'eval_duration': 361606208, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run--f4ede872-b5ad-4599-acad-069ccc0a72c5-0' usage_metadata={'input_tokens': 36, 'output_tokens': 49, 'total_tokens': 85}\n"
     ]
    }
   ],
   "source": [
    "r = llm.invoke(\"Hello, how are you today?\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af7ce997bcdb296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just a large language model, so I don't have feelings or emotions in the same way that humans do. However, I can assist you with any questions or tasks you might have. How can I help you today?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16318457690a31a",
   "metadata": {},
   "source": [
    "# Chat with OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592320046c916f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5, max_tokens=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b5fefa9644bbdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with anything you need. How can I assist you today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 14, 'total_tokens': 51, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bhijd9sRC86GNRXnJyOUT0clnQYwT', 'finish_reason': 'stop', 'logprobs': None} id='run--ee1bb555-3ede-49d1-afcd-f56230aa878a-0' usage_metadata={'input_tokens': 14, 'output_tokens': 37, 'total_tokens': 51, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "r = llm.invoke(\"Hello, how are you today?\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a66a84fc7cff23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with anything you need. How can I assist you today?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b56e18411e09bc",
   "metadata": {},
   "source": [
    "# OpenRouter Chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccacd36a4184379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:24:18.844406Z",
     "start_time": "2024-11-11T16:24:15.625847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just a language model, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to help you with any questions or tasks you have! How about you? How's your day going so far?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=getenv(\"OPENROUTER_API_KEY\"),\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    model=\"meta-llama/llama-3.2-3b-instruct:free\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "result = llm.invoke(\"Hello, how are you today?\")\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb450b3e827b6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello! I'm just a language model, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to help you with any questions or tasks you have! How about you? How's your day going so far?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 35, 'total_tokens': 87, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/llama-3.2-3b-instruct:free', 'system_fingerprint': None, 'id': 'gen-1749759765-oEGYa5ZPHWOsFJ9sYD16', 'finish_reason': 'stop', 'logprobs': None} id='run--62eb694d-d117-45ef-97ea-c76bffc5973e-0' usage_metadata={'input_tokens': 35, 'output_tokens': 52, 'total_tokens': 87, 'input_token_details': {}, 'output_token_details': {}}\n",
      "\n",
      " Hello! I'm just a language model, so I don't have feelings or emotions like humans do, but I'm functioning properly and ready to help you with any questions or tasks you have! How about you? How's your day going so far?\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "\n",
    "print(\"\\n\", result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcebc3e569b187ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/yjmck8kn59gc9w3kdklj2lt40000gn/T/ipykernel_34943/1395617360.py:1: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  result.schema()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'$defs': {'InputTokenDetails': {'description': 'Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"cache_creation\": 200,\\n            \"cache_read\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'cache_creation': {'title': 'Cache Creation', 'type': 'integer'},\n",
       "    'cache_read': {'title': 'Cache Read', 'type': 'integer'}},\n",
       "   'title': 'InputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'InvalidToolCall': {'description': 'Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Error'},\n",
       "    'type': {'const': 'invalid_tool_call', 'title': 'Type', 'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error'],\n",
       "   'title': 'InvalidToolCall',\n",
       "   'type': 'object'},\n",
       "  'OutputTokenDetails': {'description': 'Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"reasoning\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'reasoning': {'title': 'Reasoning', 'type': 'integer'}},\n",
       "   'title': 'OutputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'ToolCall': {'description': 'Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"name\": \"foo\",\\n            \"args\": {\"a\": 1},\\n            \"id\": \"123\"\\n        }\\n\\n    This represents a request to call the tool named \"foo\" with arguments {\"a\": 1}\\n    and an identifier of \"123\".',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'additionalProperties': True, 'title': 'Args', 'type': 'object'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'type': {'const': 'tool_call', 'title': 'Type', 'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id'],\n",
       "   'title': 'ToolCall',\n",
       "   'type': 'object'},\n",
       "  'UsageMetadata': {'description': 'Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"input_tokens\": 350,\\n            \"output_tokens\": 240,\\n            \"total_tokens\": 590,\\n            \"input_token_details\": {\\n                \"audio\": 10,\\n                \"cache_creation\": 200,\\n                \"cache_read\": 100,\\n            },\\n            \"output_token_details\": {\\n                \"audio\": 10,\\n                \"reasoning\": 200,\\n            }\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.',\n",
       "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
       "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
       "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'},\n",
       "    'input_token_details': {'$ref': '#/$defs/InputTokenDetails'},\n",
       "    'output_token_details': {'$ref': '#/$defs/OutputTokenDetails'}},\n",
       "   'required': ['input_tokens', 'output_tokens', 'total_tokens'],\n",
       "   'title': 'UsageMetadata',\n",
       "   'type': 'object'}},\n",
       " 'additionalProperties': True,\n",
       " 'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
       " 'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "    {'items': {'anyOf': [{'type': 'string'},\n",
       "       {'additionalProperties': True, 'type': 'object'}]},\n",
       "     'type': 'array'}],\n",
       "   'title': 'Content'},\n",
       "  'additional_kwargs': {'additionalProperties': True,\n",
       "   'title': 'Additional Kwargs',\n",
       "   'type': 'object'},\n",
       "  'response_metadata': {'additionalProperties': True,\n",
       "   'title': 'Response Metadata',\n",
       "   'type': 'object'},\n",
       "  'type': {'const': 'ai', 'default': 'ai', 'title': 'Type', 'type': 'string'},\n",
       "  'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'title': 'Name'},\n",
       "  'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'title': 'Id'},\n",
       "  'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "  'tool_calls': {'default': [],\n",
       "   'items': {'$ref': '#/$defs/ToolCall'},\n",
       "   'title': 'Tool Calls',\n",
       "   'type': 'array'},\n",
       "  'invalid_tool_calls': {'default': [],\n",
       "   'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "   'title': 'Invalid Tool Calls',\n",
       "   'type': 'array'},\n",
       "  'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "    {'type': 'null'}],\n",
       "   'default': None}},\n",
       " 'required': ['content'],\n",
       " 'title': 'AIMessage',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.schema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de3272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/yjmck8kn59gc9w3kdklj2lt40000gn/T/ipykernel_34943/1260096116.py:3: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 12:55:12) [Clang 14.0.6 ]\n",
      "Langchain version: 0.3.25\n",
      "\n",
      "Imported modules and their versions:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "dictionary changed size during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m imported_modules = {module_name: module \u001b[38;5;28;01mfor\u001b[39;00m module_name, module \u001b[38;5;129;01min\u001b[39;00m sys.modules.items() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m'\u001b[39m\u001b[33m__version__\u001b[39m\u001b[33m'\u001b[39m)}\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Add modules imported with 'from ... import ...' that might not be in sys.modules directly\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# but are in globals() and have a __version__\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m__module__\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m__name__\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule_name_from_val\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__module__\u001b[39;49m\n",
      "\u001b[31mRuntimeError\u001b[39m: dictionary changed size during iteration"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c96a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain-Ollama version: 0.3.0\n",
      "LangChain-OpenAI version: 0.3.11\n",
      "LangChain version: 0.3.25\n"
     ]
    }
   ],
   "source": [
    "# print langchain version\n",
    "import langchain\n",
    "import langchain_ollama\n",
    "import langchain_openai\n",
    "import importlib.metadata\n",
    "\n",
    "print(\"LangChain-Ollama version:\", langchain_ollama.__version__)\n",
    "print(\"LangChain-OpenAI version:\", importlib.metadata.version(\"langchain-openai\"))\n",
    "print(\"LangChain version:\", langchain.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1205ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
