{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e4270c-086b-4720-a802-9f08832c285c",
   "metadata": {},
   "source": [
    " # Multi Query Retrievers\n",
    " This tutorial demonstrates how to build and use multi-query retrievers with LangChain.\n",
    " We'll start by loading documents, splitting them into chunks, and using embeddings\n",
    " to enable similarity search and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "40e42ff1-c67e-46e4-a291-606c008f2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from rich import print\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5672f51-1ca8-4f60-8e07-f9948a7ea3c0",
   "metadata": {},
   "source": [
    " ## Step 1: Load Documents\n",
    " Here, we load the documents using `TextLoader`. You can replace the file paths\n",
    " with your own document paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "12301cf8-9fbe-4f1c-802d-2d5f407a03bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> documents\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loaded \u001b[1;36m2\u001b[0m documents\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'data/langchain.md'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Title: Chat models | ü¶úÔ∏èüîó LangChain\\n\\nURL Source: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://python.langchain.com/docs/concepts/chat_models/\\n\\nMarkdown </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Content:\\nOverview[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#overview \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Overview\")\\n-------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\n\\nLarge Language Models (LLMs) are advanced machine learning models that excel in a wide range of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language-related tasks such as text generation, translation, summarization, question answering, and more, without </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">needing task-specific fine tuning for every scenario.\\n\\nModern LLMs are typically accessed through a chat model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interface that takes a list of [messages](https://python.langchain.com/docs/concepts/messages/) as input and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">returns a [message](https://python.langchain.com/docs/concepts/messages/) as output.\\n\\nThe newest generation of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">chat models offer additional capabilities:\\n\\n*   [Tool </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">calling](https://python.langchain.com/docs/concepts/tool_calling/): Many popular chat models offer a native [tool </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">calling](https://python.langchain.com/docs/concepts/tool_calling/) API. This API allows developers to build rich </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">used to extract structured information from unstructured data and perform various other tasks.\\n*   [Structured </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output](https://python.langchain.com/docs/concepts/structured_outputs/): A technique to make a chat model respond </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in a structured format, such as JSON that matches a given schema.\\n*   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[Multimodality](https://python.langchain.com/docs/concepts/multimodality/): The ability to work with data other </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">than text; for example, images, audio, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">video.\\n\\nFeatures[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#features \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Features\")\\n-------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\n\\nLangChain provides a consistent interface for working with chat models from different providers while offering </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.\\n\\n*  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">integrations](https://python.langchain.com/docs/integrations/chat/) for an up-to-date list of supported models.\\n* </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Use either LangChain\\'s [messages](https://python.langchain.com/docs/concepts/messages/) format or OpenAI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">format.\\n*   Standard [tool calling API](https://python.langchain.com/docs/concepts/tool_calling/): standard </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interface for binding tools to models, accessing tool call requests made by models, and sending tool results back </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to the model.\\n*   Standard API for [structuring </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">outputs](https://python.langchain.com/docs/concepts/structured_outputs/#structured-output-method) via the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`with_structured_output` method.\\n*   Provides support for [async </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">programming](https://python.langchain.com/docs/concepts/async/), [efficient </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">batching](https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch), [a rich </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">streaming API](https://python.langchain.com/docs/concepts/streaming/).\\n*   Integration with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[LangSmith](https://docs.smith.langchain.com/) for monitoring and debugging production-grade applications based on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLMs.\\n*   Additional features like standardized [token </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">usage](https://python.langchain.com/docs/concepts/messages/#aimessage), [rate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limiting](https://python.langchain.com/docs/concepts/chat_models/#rate-limiting), </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[caching](https://python.langchain.com/docs/concepts/chat_models/#caching) and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">more.\\n\\nIntegrations[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#integrations \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Integrations\")\\n---------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">----------------\\n\\nLangChain has many chat model integrations that allow you to use a wide variety of models from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">different providers.\\n\\nThese integrations are one of two types:\\n\\n1.  **Official models**: These are models that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are officially supported by LangChain and/or model provider. You can find these models in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`langchain-&lt;provider&gt;` packages.\\n2.  **Community models**: There are models that are mostly contributed and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">supported by the community. You can find these models in the `langchain-community` package.\\n\\nLangChain chat </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models are named with a convention that prefixes \"Chat\" to their class names (e.g., `ChatOllama`, `ChatAnthropic`, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`ChatOpenAI`, etc.).\\n\\nPlease review the [chat model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">integrations](https://python.langchain.com/docs/integrations/chat/) for a list of supported </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models.\\n\\nnote\\n\\nModels that do **not** include the prefix \"Chat\" in their name or include \"LLM\" as a suffix in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">their name typically refer to older models that do not follow the chat model interface and instead use an interface</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that takes a string as input and returns a string as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output.\\n\\nInterface[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#interface \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Interface\")\\n------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">----\\n\\nLangChain chat models implement the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">_models.BaseChatModel.html) interface. Because `BaseChatModel` also implements the [Runnable </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Interface](https://python.langchain.com/docs/concepts/runnables/), chat models support a [standard streaming </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interface](https://python.langchain.com/docs/concepts/streaming/), [async </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">programming](https://python.langchain.com/docs/concepts/async/), optimized </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[batching](https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch), and more. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Please see the [Runnable Interface](https://python.langchain.com/docs/concepts/runnables/) for more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">details.\\n\\nMany of the key methods of chat models operate on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[messages](https://python.langchain.com/docs/concepts/messages/) as input and return messages as output.\\n\\nChat </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models offer a standard set of parameters that can be used to configure the model. These parameters are typically </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the response, and the maximum time to wait for a response. Please see the [standard </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parameters](https://python.langchain.com/docs/concepts/chat_models/#standard-parameters) section for more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">details.\\n\\nnote\\n\\nIn documentation, we will often use the terms \"LLM\" and \"Chat Model\" interchangeably. This is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">because most modern LLMs are exposed to users via a chat model interface.\\n\\nHowever, LangChain also has </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">implementations of older LLMs that do not follow the chat model interface and instead use an interface that takes a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">string as input and returns a string as output. These models are typically named without the \"Chat\" prefix (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`Ollama`, `Anthropic`, `OpenAI`, etc.). These models implement the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[BaseLLM](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.BaseL</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LM.html#langchain_core.language_models.llms.BaseLLM) interface and may be named with the \"LLM\" suffix (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`OllamaLLM`, `AnthropicLLM`, `OpenAILLM`, etc.). Generally, users should not use these models.\\n\\n### Key </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">methods[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#key-methods \"Direct link to Key </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">methods\")\\n\\nThe key methods of a chat model are:\\n\\n1.  **invoke**: The primary method for interacting with a chat</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model. It takes a list of [messages](https://python.langchain.com/docs/concepts/messages/) as input and returns a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">list of messages as output.\\n2.  **stream**: A method that allows you to stream the output of a chat model as it is</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generated.\\n3.  **batch**: A method that allows you to batch multiple requests to a chat model together for more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">efficient processing.\\n4.  **bind\\\\_tools**: A method that allows you to bind a tool to a chat model for use in the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model\\'s execution context.\\n5.  **with\\\\_structured\\\\_output**: A wrapper around the `invoke` method for models </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that natively support [structured output](https://python.langchain.com/docs/concepts/structured_outputs/).\\n\\nOther</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">important methods can be found in the [BaseChatModel API </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Reference](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_mode</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ls.BaseChatModel.html).\\n\\n### Inputs and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">outputs[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#inputs-and-outputs \"Direct link to Inputs </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and outputs\")\\n\\nModern LLMs are typically accessed through a chat model interface that takes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[messages](https://python.langchain.com/docs/concepts/messages/) as input and returns </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[messages](https://python.langchain.com/docs/concepts/messages/) as output. Messages are typically associated with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a role (e.g., \"system\", \"human\", \"assistant\") and one or more content blocks that contain text or potentially </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multimodal data (e.g., images, audio, video).\\n\\nLangChain supports two message formats to interact with chat </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models:\\n\\n1.  **LangChain Message Format**: LangChain\\'s own message format, which is used by default and is used </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">internally by LangChain.\\n2.  **OpenAI\\'s Message Format**: OpenAI\\'s message format.\\n\\n### Standard </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parameters[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#standard-parameters \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Standard parameters\")\\n\\nMany chat models have standardized parameters that can be used to configure the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model:\\n\\n| Parameter      | Description                                                                           </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| -------------- | </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">-------------------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">-------------------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">-------------------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">----------- |\\n| `model`        | The name or identifier of the specific AI model you want to use (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`\"gpt-3.5-turbo\"` or `\"gpt-4\"`).                                                                                   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| `temperature`  | Controls the randomness of the model\\'s output. A higher value (e.g., 1.0) makes responses </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused.                          </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| `timeout`      | The maximum time (in seconds) to wait for a response from the model before canceling the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">request. Ensures the request doesn‚Äôt hang indefinitely.                                                            </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| `max_tokens`   | Limits the total number of tokens (words and punctuation) in the response. This controls how </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">long the output can be.                                                                                            </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| `stop`         | Specifies stop sequences that indicate when the model should stop generating tokens. For </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">example, you might use specific strings to signal the end of a response.                                           </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| `max_retries`  | The maximum number of attempts the system will make to resend a request if it fails due to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">issues like network timeouts or rate limits.                                                                       </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| `api_key`      | The API key required for authenticating with the model provider. This is usually issued when </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you sign up for access to the model.                                                                               </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| `base_url`     | The URL of the API endpoint where requests are sent. This is typically provided by the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model\\'s provider and is necessary for directing your requests.                                                    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| `rate_limiter` | An optional </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[BaseRateLimiter](https://python.langchain.com/api_reference/core/rate_limiters/langchain_core.rate_limiters.BaseRa</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">teLimiter.html#langchain_core.rate_limiters.BaseRateLimiter) to space out requests to avoid exceeding rate limits. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">See [rate-limiting](https://python.langchain.com/docs/concepts/chat_models/#rate-limiting) below for more details. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n\\nSome important things to note:\\n\\n*   Standard parameters only apply to model providers that expose parameters</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with the intended functionality. For example, some providers do not expose a configuration for maximum output </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tokens, so max\\\\_tokens can\\'t be supported on these.\\n*   Standard parameters are currently only enforced on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">integrations that have their own integration packages (e.g. `langchain-openai`, `langchain-anthropic`, etc.), </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">they\\'re not enforced on models in `langchain-community`.\\n\\nChat models also accept other parameters that are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specific to that integration. To find all the parameters supported by a Chat model head to the their respective </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[API reference](https://python.langchain.com/api_reference/) for that model.\\n\\nChat models can call </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[tools](https://python.langchain.com/docs/concepts/tools/) to perform tasks such as fetching data from a database, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">making API requests, or running custom code. Please see the [tool </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">calling](https://python.langchain.com/docs/concepts/tool_calling/) guide for more information.\\n\\nStructured </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">outputs[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#structured-outputs \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Structured </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">outputs\")\\n--------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">-----------------------------\\n\\nChat models can be requested to respond in a particular format (e.g., JSON or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">matching a particular schema). This feature is extremely useful for information extraction tasks. Please read more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">about the technique in the [structured outputs](https://python.langchain.com/docs/concepts/structured_outputs/) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">guide.\\n\\nMultimodality[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#multimodality \"Direct link </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Multimodality\")\\n--------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--------------------\\n\\nLarge Language Models (LLMs) are not limited to processing text. They can also be used to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">process other types of data, such as images, audio, and video. This is known as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[multimodality](https://python.langchain.com/docs/concepts/multimodality/).\\n\\nCurrently, only some LLMs support </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">details.\\n\\nContext window[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#context-window \"Direct </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">link to Context </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">window\")\\n---------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">----------------\\n\\nA chat model\\'s context window refers to the maximum size of the input sequence the model can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">developers must keep in mind when working with chat models.\\n\\nIf the input exceeds the context window, the model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">may not be able to process the entire input and could raise an error. In conversational applications, this is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">especially important because the context window determines how much information the model can \"remember\" throughout</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">without exceeding the limit. For more details on handling memory in conversations, refer to the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\nThe size of the input is measured in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[tokens](https://python.langchain.com/docs/concepts/tokens/) which are the unit of processing that the model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">uses.\\n\\nAdvanced topics[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#advanced-topics \"Direct </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">link to Advanced </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">topics\")\\n---------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">-------------------\\n\\n### </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Rate-limiting[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#rate-limiting \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Rate-limiting\")\\n\\nMany chat model providers impose a limit on the number of requests that can be made in a given </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">time period.\\n\\nIf you hit a rate limit, you will typically receive a rate limit error response from the provider, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and will need to wait before making more requests.\\n\\nYou have a few options to deal with rate limits:\\n\\n1.  Try </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">provided during initialization. This parameter is used to control the rate at which requests are made to the model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">evaluate their performance. Please see the [how to handle rate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limits](https://python.langchain.com/docs/how_to/chat_model_rate_limiting/) for more information on how to use this</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">feature.\\n2.  Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[standard parameters](https://python.langchain.com/docs/concepts/chat_models/#standard-parameters) section for more</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information.\\n3.  Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">another chat model that is not rate-limited.\\n\\n### </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Caching[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#caching \"Direct link to Caching\")\\n\\nChat </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model APIs can be slow, so a natural question is whether to cache the results of previous conversations. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">In practice, caching chat model responses is a complex problem and should be approached with caution.\\n\\nThe reason</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">caching the **exact** inputs into the model. For example, how likely do you think that multiple conversations start</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with the exact same message? What about the exact same three messages?\\n\\nAn alternative approach is to use </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">This can be effective in some situations, but not in others.\\n\\nA semantic cache introduces a dependency on another</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model on the critical path of your application (e.g., the semantic cache may rely on an [embedding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model](https://python.langchain.com/docs/concepts/embedding_models/) to convert text to a vector representation), </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and it\\'s not guaranteed to capture the meaning of the input accurately.\\n\\nHowever, there might be situations </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">response times.\\n\\nPlease see the [how to cache chat model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">responses](https://python.langchain.com/docs/how_to/chat_model_caching/) guide for more details.\\n\\n*   How-to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">guides on using chat models: [how-to guides](https://python.langchain.com/docs/how_to/#chat-models).\\n*   List of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">supported chat models: [chat model integrations](https://python.langchain.com/docs/integrations/chat/).\\n\\n### </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Conceptual guides[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#conceptual-guides \"Direct link to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Conceptual guides\")\\n\\n*   [Messages](https://python.langchain.com/docs/concepts/messages/)\\n*   [Tool </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">calling](https://python.langchain.com/docs/concepts/tool_calling/)\\n*   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[Multimodality](https://python.langchain.com/docs/concepts/multimodality/)\\n*   [Structured </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">outputs](https://python.langchain.com/docs/concepts/structured_outputs/)'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'data/langchain2.md'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Title: Vector stores | ü¶úÔ∏èüîó LangChain\\n\\nURL Source: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://python.langchain.com/docs/concepts/vectorstores/\\n\\nMarkdown Content:\\nNote\\n\\nThis conceptual overview </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">focuses on text-based indexing and retrieval for simplicity. However, embedding models can be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[multi-modal](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings) and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vector stores can be used to store and retrieve a variety of data types beyond </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">text.\\n\\nOverview[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#overview \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Overview\")\\n-------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">-\\n\\nVector stores are specialized data stores that enable indexing and retrieving information based on vector </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">representations.\\n\\nThese vectors, called </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[embeddings](https://python.langchain.com/docs/concepts/embedding_models/), capture the semantic meaning of data </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that has been embedded.\\n\\nVector stores are frequently used to search over unstructured data, such as text, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">matches.\\n\\n![Image 5: Vector </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">stores](https://python.langchain.com/assets/images/vectorstores-2540b4bc355b966c99b0f02cfdddb273.png)\\n\\nIntegratio</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ns[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#integrations \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Integrations\")\\n---------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">-----------------\\n\\nLangChain has a large number of vectorstore integrations, allowing users to easily switch </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between different vectorstore implementations.\\n\\nPlease see the [full list of LangChain vectorstore </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">integrations](https://python.langchain.com/docs/integrations/vectorstores/).\\n\\nInterface[\\u200b](https://python.la</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ngchain.com/docs/concepts/vectorstores/#interface \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Interface\")\\n------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">-----\\n\\nLangChain provides a standard interface for working with vector stores, allowing users to easily switch </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between different vectorstore implementations.\\n\\nThe interface consists of basic methods for writing, deleting and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">searching for documents in the vector store.\\n\\nThe key methods are:\\n\\n*   `add_documents`: Add a list of texts to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the vector store.\\n*   `delete_documents`: Delete a list of documents from the vector store.\\n*   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`similarity_search`: Search for similar documents to a given </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">query.\\n\\nInitialization[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#initialization \"Direct </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Initialization\")\\n-------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">-------------------------\\n\\nMost vectors in LangChain accept an embedding model as an argument when initializing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the vector store.\\n\\nWe will use LangChain\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_m</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">emory.InMemoryVectorStore.html) implementation to illustrate the API.\\n\\n```\\nfrom langchain_core.vectorstores </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">import InMemoryVectorStore# Initialize with an embedding modelvector_store = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">InMemoryVectorStore(embedding=SomeEmbeddingModel())\\n```\\n\\nAdding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documents[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#adding-documents \"Direct link to Adding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documents\")\\n------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--------------------------\\n\\nTo add documents, use the `add_documents` method.\\n\\nThis API works with a list of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">objects. `Document` objects all have `page_content` and `metadata` attributes, making them a universal way to store</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">unstructured text and associated metadata.\\n\\n```\\nfrom langchain_core.documents import Documentdocument_1 = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Document(    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">metadata={\"source\": \"tweet\"},)document_2 = Document(    page_content=\"The weather forecast for tomorrow is cloudy </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and overcast, with a high of 62 degrees.\",    metadata={\"source\": \"news\"},)documents = [document_1, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">document_2]vector_store.add_documents(documents=documents)\\n```\\n\\nYou should usually provide IDs for the documents</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you add to the vector store, so that instead of adding the same document multiple times, you can update the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">existing document.\\n\\n```\\nvector_store.add_documents(documents=documents, ids=[\"doc1\", </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"doc2\"])\\n```\\n\\nDelete[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#delete \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Delete\")\\n--------------------------------------------------------------------------------------------------\\n\\nTo </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">delete documents, use the `delete_documents` method which takes a list of document IDs to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">delete.\\n\\n```\\nvector_store.delete_documents(ids=[\"doc1\"])\\n```\\n\\nSearch[\\u200b](https://python.langchain.com/doc</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">s/concepts/vectorstores/#search \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Search\")\\n--------------------------------------------------------------------------------------------------\\n\\nVec</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tor stores embed and store the documents that added. If we pass in a query, the vectorstore will embed the query, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">perform a similarity search over the embedded documents, and return the most similar ones. This captures two </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">important concepts: first, there needs to be a way to measure the similarity between the query and _any_ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[embedded](https://python.langchain.com/docs/concepts/embedding_models/) document. Second, there needs to be an </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">algorithm to efficiently perform this similarity search across _all_ embedded documents.\\n\\n### Similarity </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">metrics[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#similarity-metrics \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Similarity metrics\")\\n\\nA critical advantage of embeddings vectors is they can be compared using many simple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mathematical operations:\\n\\n*   **Cosine Similarity**: Measures the cosine of the angle between two vectors.\\n*   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">**Euclidean Distance**: Measures the straight-line distance between two points.\\n*   **Dot Product**: Measures the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">projection of one vector onto another.\\n\\nThe choice of similarity metric can sometimes be selected when </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">initializing the vectorstore. Please refer to the documentation of the specific vectorstore you are using to see </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">what similarity metrics are supported.\\n\\nFurther reading\\n\\n*   See [this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documentation](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity) from</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Google on similarity metrics to consider with embeddings.\\n*   See Pinecone\\'s [blog </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">post](https://www.pinecone.io/learn/vector-similarity/) on similarity metrics.\\n*   See OpenAI\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[FAQ](https://platform.openai.com/docs/guides/embeddings/faq) on what similarity metric to use with OpenAI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">embeddings.\\n\\n### Similarity </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">search[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#similarity-search \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Similarity search\")\\n\\nGiven a similarity metric to measure the distance between the embedded query and any </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">embedded document, we need an algorithm to efficiently search over _all_ the embedded documents to find the most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">similar ones. There are various ways to do this. As an example, many vectorstores implement [HNSW (Hierarchical </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Navigable Small World)](https://www.pinecone.io/learn/series/faiss/hnsw/), a graph-based index structure that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">allows for efficient similarity search. Regardless of the search algorithm used under the hood, the LangChain </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vectorstore interface has a `similarity_search` method for all integrations. This will take the search query, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">create an embedding, find similar documents, and return them as a list of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html).</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\n\\n```\\nquery = \"my query\"docs = vectorstore.similarity_search(query)\\n```\\n\\nMany vectorstores support search </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parameters to be passed with the `similarity_search` method. See the documentation for the specific vectorstore you</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are using to see what parameters are supported. As an example </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[Pinecone](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.Pinecon</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">eVectorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.similarity_search) several parameters that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are important general concepts: Many vectorstores support [the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`k`](https://python.langchain.com/docs/integrations/vectorstores/pinecone/#query-directly), which controls the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">number of Documents to return, and `filter`, which allows for filtering documents by metadata.\\n\\n*   `query (str) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">‚Äì Text to look up documents similar to.`\\n*   `k (int) ‚Äì Number of Documents to return. Defaults to 4.`\\n*   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`filter (dict | None) ‚Äì Dictionary of argument(s) to filter on metadata`\\n\\nFurther reading\\n\\n*   See the [how-to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">guide](https://python.langchain.com/docs/how_to/vectorstores/) for more details on how to use the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`similarity_search` method.\\n*   See the [integrations </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">page](https://python.langchain.com/docs/integrations/vectorstores/) for more details on arguments that can be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">passed in to the `similarity_search` method for specific vectorstores.\\n\\n### Metadata </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">filtering[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#metadata-filtering \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Metadata filtering\")\\n\\nWhile vectorstore implement a search algorithm to efficiently search over _all_ the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">embedded documents to find the most similar ones, many also support filtering on metadata. This allows structured </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">filters to reduce the size of the similarity search space. These two concepts work well together:\\n\\n1.  **Semantic</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">search**: Query the unstructured data directly, often using via embedding or keyword similarity.\\n2.  **Metadata </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">search**: Apply structured query to the metadata, filering specific documents.\\n\\nVector store support for metadata</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">filtering is typically dependent on the underlying vector store implementation.\\n\\nHere is example usage with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[Pinecone](https://python.langchain.com/docs/integrations/vectorstores/pinecone/#query-directly), showing that we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">filter for all documents that have the metadata key `source` with value </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`tweet`.\\n\\n```\\nvectorstore.similarity_search(    \"LangChain provides abstractions to make working with LLMs </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">easy\",    k=2,    filter={\"source\": \"tweet\"},)\\n```\\n\\nAdvanced search and retrieval </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">techniques[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#advanced-search-and-retrieval-technique</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">s \"Direct link to Advanced search and retrieval </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">techniques\")\\n-----------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">---------------------------------------------------------------------------------------------------\\n\\nWhile </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can be employed to improve search quality and diversity. For example, [maximal marginal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">diverse set of results. As a second example, some [vector </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">stores](https://python.langchain.com/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">similarity search, which marries the benefits of both approaches. At the moment, there is no unified way to perform</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with `similarity_search`. See this [how-to guide on hybrid </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">search](https://python.langchain.com/docs/how_to/hybrid/) for more details.\\n\\n| Name                              </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">| When to use                                           | Description                                              </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">|\\n| </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">-------------------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">-------------------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">--------- | ----------------------------------------------------- | </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">-------------------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">------------------------- |\\n| [Hybrid </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">search](https://python.langchain.com/docs/integrations/retrievers/pinecone_hybrid_search/)                         </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">| When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">marrying the benefits of both approaches. [Paper](https://arxiv.org/abs/2210.11934). |\\n| [Maximal Marginal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Relevance </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(MMR)](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVec</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">torStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.max_marginal_relevance_search) | When needing to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">diversify search results.             | MMR attempts to diversify the results of a search to avoid returning </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">similar and redundant documents.                                        |'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'data/langchain.md'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Title: Chat models | ü¶úÔ∏èüîó LangChain\\n\\nURL Source: \u001b[0m\n",
       "\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/\\n\\nMarkdown \u001b[0m\n",
       "\u001b[32mContent:\\nOverview\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#overview \"Direct link to \u001b[0m\n",
       "\u001b[32mOverview\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n-------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m\\n\\nLarge Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are advanced machine learning models that excel in a wide range of \u001b[0m\n",
       "\u001b[32mlanguage-related tasks such as text generation, translation, summarization, question answering, and more, without \u001b[0m\n",
       "\u001b[32mneeding task-specific fine tuning for every scenario.\\n\\nModern LLMs are typically accessed through a chat model \u001b[0m\n",
       "\u001b[32minterface that takes a list of \u001b[0m\u001b[32m[\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as input and \u001b[0m\n",
       "\u001b[32mreturns a \u001b[0m\u001b[32m[\u001b[0m\u001b[32mmessage\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output.\\n\\nThe newest generation of \u001b[0m\n",
       "\u001b[32mchat models offer additional capabilities:\\n\\n*   \u001b[0m\u001b[32m[\u001b[0m\u001b[32mTool \u001b[0m\n",
       "\u001b[32mcalling\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/tool_calling/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: Many popular chat models offer a native \u001b[0m\u001b[32m[\u001b[0m\u001b[32mtool \u001b[0m\n",
       "\u001b[32mcalling\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/tool_calling/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m API. This API allows developers to build rich \u001b[0m\n",
       "\u001b[32mapplications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be \u001b[0m\n",
       "\u001b[32mused to extract structured information from unstructured data and perform various other tasks.\\n*   \u001b[0m\u001b[32m[\u001b[0m\u001b[32mStructured \u001b[0m\n",
       "\u001b[32moutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/structured_outputs/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: A technique to make a chat model respond \u001b[0m\n",
       "\u001b[32min a structured format, such as JSON that matches a given schema.\\n*   \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mMultimodality\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/multimodality/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: The ability to work with data other \u001b[0m\n",
       "\u001b[32mthan text; for example, images, audio, and \u001b[0m\n",
       "\u001b[32mvideo.\\n\\nFeatures\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#features \"Direct link to \u001b[0m\n",
       "\u001b[32mFeatures\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n-------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m\\n\\nLangChain provides a consistent interface for working with chat models from different providers while offering \u001b[0m\n",
       "\u001b[32madditional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.\\n\\n*  \u001b[0m\n",
       "\u001b[32mIntegrations with many chat model providers \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, \u001b[0m\n",
       "\u001b[32mAmazon Bedrock, Hugging Face, Cohere, Groq\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Please see \u001b[0m\u001b[32m[\u001b[0m\u001b[32mchat model \u001b[0m\n",
       "\u001b[32mintegrations\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/integrations/chat/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for an up-to-date list of supported models.\\n* \u001b[0m\n",
       "\u001b[32mUse either LangChain\\'s \u001b[0m\u001b[32m[\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m format or OpenAI \u001b[0m\n",
       "\u001b[32mformat.\\n*   Standard \u001b[0m\u001b[32m[\u001b[0m\u001b[32mtool calling API\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/tool_calling/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: standard \u001b[0m\n",
       "\u001b[32minterface for binding tools to models, accessing tool call requests made by models, and sending tool results back \u001b[0m\n",
       "\u001b[32mto the model.\\n*   Standard API for \u001b[0m\u001b[32m[\u001b[0m\u001b[32mstructuring \u001b[0m\n",
       "\u001b[32moutputs\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/structured_outputs/#structured-output-method\u001b[0m\u001b[32m)\u001b[0m\u001b[32m via the \u001b[0m\n",
       "\u001b[32m`with_structured_output` method.\\n*   Provides support for \u001b[0m\u001b[32m[\u001b[0m\u001b[32masync \u001b[0m\n",
       "\u001b[32mprogramming\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/async/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\u001b[32m[\u001b[0m\u001b[32mefficient \u001b[0m\n",
       "\u001b[32mbatching\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\u001b[32m[\u001b[0m\u001b[32ma rich \u001b[0m\n",
       "\u001b[32mstreaming API\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/streaming/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n*   Integration with \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mLangSmith\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://docs.smith.langchain.com/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for monitoring and debugging production-grade applications based on \u001b[0m\n",
       "\u001b[32mLLMs.\\n*   Additional features like standardized \u001b[0m\u001b[32m[\u001b[0m\u001b[32mtoken \u001b[0m\n",
       "\u001b[32musage\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/#aimessage\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\u001b[32m[\u001b[0m\u001b[32mrate \u001b[0m\n",
       "\u001b[32mlimiting\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#rate-limiting\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mcaching\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#caching\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and \u001b[0m\n",
       "\u001b[32mmore.\\n\\nIntegrations\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#integrations \"Direct link to \u001b[0m\n",
       "\u001b[32mIntegrations\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n---------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m----------------\\n\\nLangChain has many chat model integrations that allow you to use a wide variety of models from \u001b[0m\n",
       "\u001b[32mdifferent providers.\\n\\nThese integrations are one of two types:\\n\\n1.  **Official models**: These are models that \u001b[0m\n",
       "\u001b[32mare officially supported by LangChain and/or model provider. You can find these models in the \u001b[0m\n",
       "\u001b[32m`langchain-\u001b[0m\u001b[32m<\u001b[0m\u001b[32mprovider\u001b[0m\u001b[32m>\u001b[0m\u001b[32m` packages.\\n2.  **Community models**: There are models that are mostly contributed and \u001b[0m\n",
       "\u001b[32msupported by the community. You can find these models in the `langchain-community` package.\\n\\nLangChain chat \u001b[0m\n",
       "\u001b[32mmodels are named with a convention that prefixes \"Chat\" to their class names \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., `ChatOllama`, `ChatAnthropic`, \u001b[0m\n",
       "\u001b[32m`ChatOpenAI`, etc.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nPlease review the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mchat model \u001b[0m\n",
       "\u001b[32mintegrations\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/integrations/chat/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for a list of supported \u001b[0m\n",
       "\u001b[32mmodels.\\n\\nnote\\n\\nModels that do **not** include the prefix \"Chat\" in their name or include \"LLM\" as a suffix in \u001b[0m\n",
       "\u001b[32mtheir name typically refer to older models that do not follow the chat model interface and instead use an interface\u001b[0m\n",
       "\u001b[32mthat takes a string as input and returns a string as \u001b[0m\n",
       "\u001b[32moutput.\\n\\nInterface\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#interface \"Direct link to \u001b[0m\n",
       "\u001b[32mInterface\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m----\\n\\nLangChain chat models implement the \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mBaseChatModel\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat\u001b[0m\n",
       "\u001b[32m_models.BaseChatModel.html\u001b[0m\u001b[32m)\u001b[0m\u001b[32m interface. Because `BaseChatModel` also implements the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mRunnable \u001b[0m\n",
       "\u001b[32mInterface\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/runnables/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, chat models support a \u001b[0m\u001b[32m[\u001b[0m\u001b[32mstandard streaming \u001b[0m\n",
       "\u001b[32minterface\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/streaming/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\u001b[32m[\u001b[0m\u001b[32masync \u001b[0m\n",
       "\u001b[32mprogramming\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/async/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, optimized \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mbatching\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and more. \u001b[0m\n",
       "\u001b[32mPlease see the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mRunnable Interface\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/runnables/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for more \u001b[0m\n",
       "\u001b[32mdetails.\\n\\nMany of the key methods of chat models operate on \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as input and return messages as output.\\n\\nChat \u001b[0m\n",
       "\u001b[32mmodels offer a standard set of parameters that can be used to configure the model. These parameters are typically \u001b[0m\n",
       "\u001b[32mused to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in \u001b[0m\n",
       "\u001b[32mthe response, and the maximum time to wait for a response. Please see the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mstandard \u001b[0m\n",
       "\u001b[32mparameters\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#standard-parameters\u001b[0m\u001b[32m)\u001b[0m\u001b[32m section for more \u001b[0m\n",
       "\u001b[32mdetails.\\n\\nnote\\n\\nIn documentation, we will often use the terms \"LLM\" and \"Chat Model\" interchangeably. This is \u001b[0m\n",
       "\u001b[32mbecause most modern LLMs are exposed to users via a chat model interface.\\n\\nHowever, LangChain also has \u001b[0m\n",
       "\u001b[32mimplementations of older LLMs that do not follow the chat model interface and instead use an interface that takes a\u001b[0m\n",
       "\u001b[32mstring as input and returns a string as output. These models are typically named without the \"Chat\" prefix \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32m`Ollama`, `Anthropic`, `OpenAI`, etc.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. These models implement the \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mBaseLLM\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.BaseL\u001b[0m\n",
       "\u001b[32mLM.html#langchain_core.language_models.llms.BaseLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m interface and may be named with the \"LLM\" suffix \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32m`OllamaLLM`, `AnthropicLLM`, `OpenAILLM`, etc.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Generally, users should not use these models.\\n\\n### Key \u001b[0m\n",
       "\u001b[32mmethods\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#key-methods \"Direct link to Key \u001b[0m\n",
       "\u001b[32mmethods\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nThe key methods of a chat model are:\\n\\n1.  **invoke**: The primary method for interacting with a chat\u001b[0m\n",
       "\u001b[32mmodel. It takes a list of \u001b[0m\u001b[32m[\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as input and returns a \u001b[0m\n",
       "\u001b[32mlist of messages as output.\\n2.  **stream**: A method that allows you to stream the output of a chat model as it is\u001b[0m\n",
       "\u001b[32mgenerated.\\n3.  **batch**: A method that allows you to batch multiple requests to a chat model together for more \u001b[0m\n",
       "\u001b[32mefficient processing.\\n4.  **bind\\\\_tools**: A method that allows you to bind a tool to a chat model for use in the\u001b[0m\n",
       "\u001b[32mmodel\\'s execution context.\\n5.  **with\\\\_structured\\\\_output**: A wrapper around the `invoke` method for models \u001b[0m\n",
       "\u001b[32mthat natively support \u001b[0m\u001b[32m[\u001b[0m\u001b[32mstructured output\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/structured_outputs/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nOther\u001b[0m\n",
       "\u001b[32mimportant methods can be found in the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mBaseChatModel API \u001b[0m\n",
       "\u001b[32mReference\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_mode\u001b[0m\n",
       "\u001b[32mls.BaseChatModel.html\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n### Inputs and \u001b[0m\n",
       "\u001b[32moutputs\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#inputs-and-outputs \"Direct link to Inputs \u001b[0m\n",
       "\u001b[32mand outputs\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nModern LLMs are typically accessed through a chat model interface that takes \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as input and returns \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output. Messages are typically associated with \u001b[0m\n",
       "\u001b[32ma role \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \"system\", \"human\", \"assistant\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and one or more content blocks that contain text or potentially \u001b[0m\n",
       "\u001b[32mmultimodal data \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., images, audio, video\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nLangChain supports two message formats to interact with chat \u001b[0m\n",
       "\u001b[32mmodels:\\n\\n1.  **LangChain Message Format**: LangChain\\'s own message format, which is used by default and is used \u001b[0m\n",
       "\u001b[32minternally by LangChain.\\n2.  **OpenAI\\'s Message Format**: OpenAI\\'s message format.\\n\\n### Standard \u001b[0m\n",
       "\u001b[32mparameters\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#standard-parameters \"Direct link to \u001b[0m\n",
       "\u001b[32mStandard parameters\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nMany chat models have standardized parameters that can be used to configure the \u001b[0m\n",
       "\u001b[32mmodel:\\n\\n| Parameter      | Description                                                                           \u001b[0m\n",
       "\u001b[32m|\\n| -------------- | \u001b[0m\n",
       "\u001b[32m-------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m-------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m-------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m----------- |\\n| `model`        | The name or identifier of the specific AI model you want to use \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32m`\"gpt-3.5-turbo\"` or `\"gpt-4\"`\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.                                                                                   \u001b[0m\n",
       "\u001b[32m|\\n| `temperature`  | Controls the randomness of the model\\'s output. A higher value \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., 1.0\u001b[0m\u001b[32m)\u001b[0m\u001b[32m makes responses \u001b[0m\n",
       "\u001b[32mmore creative, while a lower value \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., 0.0\u001b[0m\u001b[32m)\u001b[0m\u001b[32m makes them more deterministic and focused.                          \u001b[0m\n",
       "\u001b[32m|\\n| `timeout`      | The maximum time \u001b[0m\u001b[32m(\u001b[0m\u001b[32min seconds\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to wait for a response from the model before canceling the \u001b[0m\n",
       "\u001b[32mrequest. Ensures the request doesn‚Äôt hang indefinitely.                                                            \u001b[0m\n",
       "\u001b[32m|\\n| `max_tokens`   | Limits the total number of tokens \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwords and punctuation\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in the response. This controls how \u001b[0m\n",
       "\u001b[32mlong the output can be.                                                                                            \u001b[0m\n",
       "\u001b[32m|\\n| `stop`         | Specifies stop sequences that indicate when the model should stop generating tokens. For \u001b[0m\n",
       "\u001b[32mexample, you might use specific strings to signal the end of a response.                                           \u001b[0m\n",
       "\u001b[32m|\\n| `max_retries`  | The maximum number of attempts the system will make to resend a request if it fails due to \u001b[0m\n",
       "\u001b[32missues like network timeouts or rate limits.                                                                       \u001b[0m\n",
       "\u001b[32m|\\n| `api_key`      | The API key required for authenticating with the model provider. This is usually issued when \u001b[0m\n",
       "\u001b[32myou sign up for access to the model.                                                                               \u001b[0m\n",
       "\u001b[32m|\\n| `base_url`     | The URL of the API endpoint where requests are sent. This is typically provided by the \u001b[0m\n",
       "\u001b[32mmodel\\'s provider and is necessary for directing your requests.                                                    \u001b[0m\n",
       "\u001b[32m|\\n| `rate_limiter` | An optional \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mBaseRateLimiter\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/api_reference/core/rate_limiters/langchain_core.rate_limiters.BaseRa\u001b[0m\n",
       "\u001b[32mteLimiter.html#langchain_core.rate_limiters.BaseRateLimiter\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to space out requests to avoid exceeding rate limits. \u001b[0m\n",
       "\u001b[32mSee \u001b[0m\u001b[32m[\u001b[0m\u001b[32mrate-limiting\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#rate-limiting\u001b[0m\u001b[32m)\u001b[0m\u001b[32m below for more details. \u001b[0m\n",
       "\u001b[32m|\\n\\nSome important things to note:\\n\\n*   Standard parameters only apply to model providers that expose parameters\u001b[0m\n",
       "\u001b[32mwith the intended functionality. For example, some providers do not expose a configuration for maximum output \u001b[0m\n",
       "\u001b[32mtokens, so max\\\\_tokens can\\'t be supported on these.\\n*   Standard parameters are currently only enforced on \u001b[0m\n",
       "\u001b[32mintegrations that have their own integration packages \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g. `langchain-openai`, `langchain-anthropic`, etc.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mthey\\'re not enforced on models in `langchain-community`.\\n\\nChat models also accept other parameters that are \u001b[0m\n",
       "\u001b[32mspecific to that integration. To find all the parameters supported by a Chat model head to the their respective \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mAPI reference\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/api_reference/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for that model.\\n\\nChat models can call \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mtools\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/tools/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to perform tasks such as fetching data from a database, \u001b[0m\n",
       "\u001b[32mmaking API requests, or running custom code. Please see the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mtool \u001b[0m\n",
       "\u001b[32mcalling\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/tool_calling/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m guide for more information.\\n\\nStructured \u001b[0m\n",
       "\u001b[32moutputs\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#structured-outputs \"Direct link to \u001b[0m\n",
       "\u001b[32mStructured \u001b[0m\n",
       "\u001b[32moutputs\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n--------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m-----------------------------\\n\\nChat models can be requested to respond in a particular format \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., JSON or \u001b[0m\n",
       "\u001b[32mmatching a particular schema\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This feature is extremely useful for information extraction tasks. Please read more \u001b[0m\n",
       "\u001b[32mabout the technique in the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mstructured outputs\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/structured_outputs/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mguide.\\n\\nMultimodality\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#multimodality \"Direct link \u001b[0m\n",
       "\u001b[32mto \u001b[0m\n",
       "\u001b[32mMultimodality\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n--------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m--------------------\\n\\nLarge Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are not limited to processing text. They can also be used to \u001b[0m\n",
       "\u001b[32mprocess other types of data, such as images, audio, and video. This is known as \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mmultimodality\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/multimodality/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nCurrently, only some LLMs support \u001b[0m\n",
       "\u001b[32mmultimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for \u001b[0m\n",
       "\u001b[32mdetails.\\n\\nContext window\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#context-window \"Direct \u001b[0m\n",
       "\u001b[32mlink to Context \u001b[0m\n",
       "\u001b[32mwindow\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n---------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m----------------\\n\\nA chat model\\'s context window refers to the maximum size of the input sequence the model can \u001b[0m\n",
       "\u001b[32mprocess at one time. While the context windows of modern LLMs are quite large, they still present a limitation that\u001b[0m\n",
       "\u001b[32mdevelopers must keep in mind when working with chat models.\\n\\nIf the input exceeds the context window, the model \u001b[0m\n",
       "\u001b[32mmay not be able to process the entire input and could raise an error. In conversational applications, this is \u001b[0m\n",
       "\u001b[32mespecially important because the context window determines how much information the model can \"remember\" throughout\u001b[0m\n",
       "\u001b[32ma conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue\u001b[0m\n",
       "\u001b[32mwithout exceeding the limit. For more details on handling memory in conversations, refer to the \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mmemory\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://langchain-ai.github.io/langgraph/concepts/memory/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nThe size of the input is measured in \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mtokens\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/tokens/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m which are the unit of processing that the model \u001b[0m\n",
       "\u001b[32muses.\\n\\nAdvanced topics\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#advanced-topics \"Direct \u001b[0m\n",
       "\u001b[32mlink to Advanced \u001b[0m\n",
       "\u001b[32mtopics\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n---------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m-------------------\\n\\n### \u001b[0m\n",
       "\u001b[32mRate-limiting\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#rate-limiting \"Direct link to \u001b[0m\n",
       "\u001b[32mRate-limiting\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nMany chat model providers impose a limit on the number of requests that can be made in a given \u001b[0m\n",
       "\u001b[32mtime period.\\n\\nIf you hit a rate limit, you will typically receive a rate limit error response from the provider, \u001b[0m\n",
       "\u001b[32mand will need to wait before making more requests.\\n\\nYou have a few options to deal with rate limits:\\n\\n1.  Try \u001b[0m\n",
       "\u001b[32mto avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be \u001b[0m\n",
       "\u001b[32mprovided during initialization. This parameter is used to control the rate at which requests are made to the model \u001b[0m\n",
       "\u001b[32mprovider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to \u001b[0m\n",
       "\u001b[32mevaluate their performance. Please see the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mhow to handle rate \u001b[0m\n",
       "\u001b[32mlimits\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/how_to/chat_model_rate_limiting/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for more information on how to use this\u001b[0m\n",
       "\u001b[32mfeature.\\n2.  Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain \u001b[0m\n",
       "\u001b[32mamount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate \u001b[0m\n",
       "\u001b[32mlimit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mstandard parameters\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#standard-parameters\u001b[0m\u001b[32m)\u001b[0m\u001b[32m section for more\u001b[0m\n",
       "\u001b[32minformation.\\n3.  Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to \u001b[0m\n",
       "\u001b[32manother chat model that is not rate-limited.\\n\\n### \u001b[0m\n",
       "\u001b[32mCaching\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#caching \"Direct link to Caching\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nChat \u001b[0m\n",
       "\u001b[32mmodel APIs can be slow, so a natural question is whether to cache the results of previous conversations. \u001b[0m\n",
       "\u001b[32mTheoretically, caching can help improve performance by reducing the number of requests made to the model provider. \u001b[0m\n",
       "\u001b[32mIn practice, caching chat model responses is a complex problem and should be approached with caution.\\n\\nThe reason\u001b[0m\n",
       "\u001b[32mis that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on \u001b[0m\n",
       "\u001b[32mcaching the **exact** inputs into the model. For example, how likely do you think that multiple conversations start\u001b[0m\n",
       "\u001b[32mwith the exact same message? What about the exact same three messages?\\n\\nAn alternative approach is to use \u001b[0m\n",
       "\u001b[32msemantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. \u001b[0m\n",
       "\u001b[32mThis can be effective in some situations, but not in others.\\n\\nA semantic cache introduces a dependency on another\u001b[0m\n",
       "\u001b[32mmodel on the critical path of your application \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., the semantic cache may rely on an \u001b[0m\u001b[32m[\u001b[0m\u001b[32membedding \u001b[0m\n",
       "\u001b[32mmodel\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/embedding_models/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to convert text to a vector representation\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mand it\\'s not guaranteed to capture the meaning of the input accurately.\\n\\nHowever, there might be situations \u001b[0m\n",
       "\u001b[32mwhere caching chat model responses is beneficial. For example, if you have a chat model that is used to answer \u001b[0m\n",
       "\u001b[32mfrequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve \u001b[0m\n",
       "\u001b[32mresponse times.\\n\\nPlease see the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mhow to cache chat model \u001b[0m\n",
       "\u001b[32mresponses\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/how_to/chat_model_caching/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m guide for more details.\\n\\n*   How-to \u001b[0m\n",
       "\u001b[32mguides on using chat models: \u001b[0m\u001b[32m[\u001b[0m\u001b[32mhow-to guides\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/how_to/#chat-models\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n*   List of \u001b[0m\n",
       "\u001b[32msupported chat models: \u001b[0m\u001b[32m[\u001b[0m\u001b[32mchat model integrations\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/integrations/chat/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n### \u001b[0m\n",
       "\u001b[32mConceptual guides\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#conceptual-guides \"Direct link to\u001b[0m\n",
       "\u001b[32mConceptual guides\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n*   \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMessages\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n*   \u001b[0m\u001b[32m[\u001b[0m\u001b[32mTool \u001b[0m\n",
       "\u001b[32mcalling\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/tool_calling/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n*   \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mMultimodality\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/multimodality/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n*   \u001b[0m\u001b[32m[\u001b[0m\u001b[32mStructured \u001b[0m\n",
       "\u001b[32moutputs\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/structured_outputs/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'data/langchain2.md'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'Title: Vector stores | ü¶úÔ∏èüîó LangChain\\n\\nURL Source: \u001b[0m\n",
       "\u001b[32mhttps://python.langchain.com/docs/concepts/vectorstores/\\n\\nMarkdown Content:\\nNote\\n\\nThis conceptual overview \u001b[0m\n",
       "\u001b[32mfocuses on text-based indexing and retrieval for simplicity. However, embedding models can be \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mmulti-modal\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and \u001b[0m\n",
       "\u001b[32mvector stores can be used to store and retrieve a variety of data types beyond \u001b[0m\n",
       "\u001b[32mtext.\\n\\nOverview\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/vectorstores/#overview \"Direct link to \u001b[0m\n",
       "\u001b[32mOverview\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n-------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m-\\n\\nVector stores are specialized data stores that enable indexing and retrieving information based on vector \u001b[0m\n",
       "\u001b[32mrepresentations.\\n\\nThese vectors, called \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32membeddings\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/embedding_models/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, capture the semantic meaning of data \u001b[0m\n",
       "\u001b[32mthat has been embedded.\\n\\nVector stores are frequently used to search over unstructured data, such as text, \u001b[0m\n",
       "\u001b[32mimages, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword \u001b[0m\n",
       "\u001b[32mmatches.\\n\\n!\u001b[0m\u001b[32m[\u001b[0m\u001b[32mImage 5: Vector \u001b[0m\n",
       "\u001b[32mstores\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/assets/images/vectorstores-2540b4bc355b966c99b0f02cfdddb273.png\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nIntegratio\u001b[0m\n",
       "\u001b[32mns\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/vectorstores/#integrations \"Direct link to \u001b[0m\n",
       "\u001b[32mIntegrations\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n---------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m-----------------\\n\\nLangChain has a large number of vectorstore integrations, allowing users to easily switch \u001b[0m\n",
       "\u001b[32mbetween different vectorstore implementations.\\n\\nPlease see the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mfull list of LangChain vectorstore \u001b[0m\n",
       "\u001b[32mintegrations\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/integrations/vectorstores/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nInterface\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.la\u001b[0m\n",
       "\u001b[32mngchain.com/docs/concepts/vectorstores/#interface \"Direct link to \u001b[0m\n",
       "\u001b[32mInterface\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m-----\\n\\nLangChain provides a standard interface for working with vector stores, allowing users to easily switch \u001b[0m\n",
       "\u001b[32mbetween different vectorstore implementations.\\n\\nThe interface consists of basic methods for writing, deleting and\u001b[0m\n",
       "\u001b[32msearching for documents in the vector store.\\n\\nThe key methods are:\\n\\n*   `add_documents`: Add a list of texts to\u001b[0m\n",
       "\u001b[32mthe vector store.\\n*   `delete_documents`: Delete a list of documents from the vector store.\\n*   \u001b[0m\n",
       "\u001b[32m`similarity_search`: Search for similar documents to a given \u001b[0m\n",
       "\u001b[32mquery.\\n\\nInitialization\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/vectorstores/#initialization \"Direct \u001b[0m\n",
       "\u001b[32mlink to \u001b[0m\n",
       "\u001b[32mInitialization\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n-------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m-------------------------\\n\\nMost vectors in LangChain accept an embedding model as an argument when initializing \u001b[0m\n",
       "\u001b[32mthe vector store.\\n\\nWe will use LangChain\\'s \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mInMemoryVectorStore\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_m\u001b[0m\n",
       "\u001b[32memory.InMemoryVectorStore.html\u001b[0m\u001b[32m)\u001b[0m\u001b[32m implementation to illustrate the API.\\n\\n```\\nfrom langchain_core.vectorstores \u001b[0m\n",
       "\u001b[32mimport InMemoryVectorStore# Initialize with an embedding modelvector_store = \u001b[0m\n",
       "\u001b[32mInMemoryVectorStore\u001b[0m\u001b[32m(\u001b[0m\u001b[32membedding\u001b[0m\u001b[32m=\u001b[0m\u001b[32mSomeEmbeddingModel\u001b[0m\u001b[32m(\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n```\\n\\nAdding \u001b[0m\n",
       "\u001b[32mdocuments\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/vectorstores/#adding-documents \"Direct link to Adding \u001b[0m\n",
       "\u001b[32mdocuments\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m--------------------------\\n\\nTo add documents, use the `add_documents` method.\\n\\nThis API works with a list of \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mDocument\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mobjects. `Document` objects all have `page_content` and `metadata` attributes, making them a universal way to store\u001b[0m\n",
       "\u001b[32munstructured text and associated metadata.\\n\\n```\\nfrom langchain_core.documents import Documentdocument_1 = \u001b[0m\n",
       "\u001b[32mDocument\u001b[0m\u001b[32m(\u001b[0m\u001b[32m    \u001b[0m\u001b[32mpage_content\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"I\u001b[0m\u001b[32m had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",    \u001b[0m\n",
       "\u001b[32mmetadata\u001b[0m\u001b[32m=\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"source\": \"tweet\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\u001b[0m\u001b[32m)\u001b[0m\u001b[32mdocument_2 = Document\u001b[0m\u001b[32m(\u001b[0m\u001b[32m    \u001b[0m\u001b[32mpage_content\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"The\u001b[0m\u001b[32m weather forecast for tomorrow is cloudy \u001b[0m\n",
       "\u001b[32mand overcast, with a high of 62 degrees.\",    \u001b[0m\u001b[32mmetadata\u001b[0m\u001b[32m=\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"source\": \"news\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\u001b[0m\u001b[32m)\u001b[0m\u001b[32mdocuments = \u001b[0m\u001b[32m[\u001b[0m\u001b[32mdocument_1, \u001b[0m\n",
       "\u001b[32mdocument_2\u001b[0m\u001b[32m]\u001b[0m\u001b[32mvector_store.add_documents\u001b[0m\u001b[32m(\u001b[0m\u001b[32mdocuments\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdocuments\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n```\\n\\nYou should usually provide IDs for the documents\u001b[0m\n",
       "\u001b[32myou add to the vector store, so that instead of adding the same document multiple times, you can update the \u001b[0m\n",
       "\u001b[32mexisting document.\\n\\n```\\nvector_store.add_documents\u001b[0m\u001b[32m(\u001b[0m\u001b[32mdocuments\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdocuments\u001b[0m\u001b[32m, \u001b[0m\u001b[32mids\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"doc1\", \u001b[0m\n",
       "\u001b[32m\"doc2\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n```\\n\\nDelete\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/vectorstores/#delete \"Direct link to \u001b[0m\n",
       "\u001b[32mDelete\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n--------------------------------------------------------------------------------------------------\\n\\nTo \u001b[0m\n",
       "\u001b[32mdelete documents, use the `delete_documents` method which takes a list of document IDs to \u001b[0m\n",
       "\u001b[32mdelete.\\n\\n```\\nvector_store.delete_documents\u001b[0m\u001b[32m(\u001b[0m\u001b[32mids\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"doc1\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n```\\n\\nSearch\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/doc\u001b[0m\n",
       "\u001b[32ms/concepts/vectorstores/#search \"Direct link to \u001b[0m\n",
       "\u001b[32mSearch\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n--------------------------------------------------------------------------------------------------\\n\\nVec\u001b[0m\n",
       "\u001b[32mtor stores embed and store the documents that added. If we pass in a query, the vectorstore will embed the query, \u001b[0m\n",
       "\u001b[32mperform a similarity search over the embedded documents, and return the most similar ones. This captures two \u001b[0m\n",
       "\u001b[32mimportant concepts: first, there needs to be a way to measure the similarity between the query and _any_ \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32membedded\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/embedding_models/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m document. Second, there needs to be an \u001b[0m\n",
       "\u001b[32malgorithm to efficiently perform this similarity search across _all_ embedded documents.\\n\\n### Similarity \u001b[0m\n",
       "\u001b[32mmetrics\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/vectorstores/#similarity-metrics \"Direct link to \u001b[0m\n",
       "\u001b[32mSimilarity metrics\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nA critical advantage of embeddings vectors is they can be compared using many simple \u001b[0m\n",
       "\u001b[32mmathematical operations:\\n\\n*   **Cosine Similarity**: Measures the cosine of the angle between two vectors.\\n*   \u001b[0m\n",
       "\u001b[32m**Euclidean Distance**: Measures the straight-line distance between two points.\\n*   **Dot Product**: Measures the \u001b[0m\n",
       "\u001b[32mprojection of one vector onto another.\\n\\nThe choice of similarity metric can sometimes be selected when \u001b[0m\n",
       "\u001b[32minitializing the vectorstore. Please refer to the documentation of the specific vectorstore you are using to see \u001b[0m\n",
       "\u001b[32mwhat similarity metrics are supported.\\n\\nFurther reading\\n\\n*   See \u001b[0m\u001b[32m[\u001b[0m\u001b[32mthis \u001b[0m\n",
       "\u001b[32mdocumentation\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity\u001b[0m\u001b[32m)\u001b[0m\u001b[32m from\u001b[0m\n",
       "\u001b[32mGoogle on similarity metrics to consider with embeddings.\\n*   See Pinecone\\'s \u001b[0m\u001b[32m[\u001b[0m\u001b[32mblog \u001b[0m\n",
       "\u001b[32mpost\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://www.pinecone.io/learn/vector-similarity/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m on similarity metrics.\\n*   See OpenAI\\'s \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mFAQ\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://platform.openai.com/docs/guides/embeddings/faq\u001b[0m\u001b[32m)\u001b[0m\u001b[32m on what similarity metric to use with OpenAI \u001b[0m\n",
       "\u001b[32membeddings.\\n\\n### Similarity \u001b[0m\n",
       "\u001b[32msearch\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/vectorstores/#similarity-search \"Direct link to \u001b[0m\n",
       "\u001b[32mSimilarity search\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nGiven a similarity metric to measure the distance between the embedded query and any \u001b[0m\n",
       "\u001b[32membedded document, we need an algorithm to efficiently search over _all_ the embedded documents to find the most \u001b[0m\n",
       "\u001b[32msimilar ones. There are various ways to do this. As an example, many vectorstores implement \u001b[0m\u001b[32m[\u001b[0m\u001b[32mHNSW \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHierarchical \u001b[0m\n",
       "\u001b[32mNavigable Small World\u001b[0m\u001b[32m)\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://www.pinecone.io/learn/series/faiss/hnsw/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, a graph-based index structure that \u001b[0m\n",
       "\u001b[32mallows for efficient similarity search. Regardless of the search algorithm used under the hood, the LangChain \u001b[0m\n",
       "\u001b[32mvectorstore interface has a `similarity_search` method for all integrations. This will take the search query, \u001b[0m\n",
       "\u001b[32mcreate an embedding, find similar documents, and return them as a list of \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mDocuments\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\u001b[0m\n",
       "\u001b[32m\\n\\n```\\nquery = \"my query\"docs = vectorstore.similarity_search\u001b[0m\u001b[32m(\u001b[0m\u001b[32mquery\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n```\\n\\nMany vectorstores support search \u001b[0m\n",
       "\u001b[32mparameters to be passed with the `similarity_search` method. See the documentation for the specific vectorstore you\u001b[0m\n",
       "\u001b[32mare using to see what parameters are supported. As an example \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mPinecone\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.Pinecon\u001b[0m\n",
       "\u001b[32meVectorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.similarity_search\u001b[0m\u001b[32m)\u001b[0m\u001b[32m several parameters that \u001b[0m\n",
       "\u001b[32mare important general concepts: Many vectorstores support \u001b[0m\u001b[32m[\u001b[0m\u001b[32mthe \u001b[0m\n",
       "\u001b[32m`k`\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/integrations/vectorstores/pinecone/#query-directly\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, which controls the \u001b[0m\n",
       "\u001b[32mnumber of Documents to return, and `filter`, which allows for filtering documents by metadata.\\n\\n*   `query \u001b[0m\u001b[32m(\u001b[0m\u001b[32mstr\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m‚Äì Text to look up documents similar to.`\\n*   `k \u001b[0m\u001b[32m(\u001b[0m\u001b[32mint\u001b[0m\u001b[32m)\u001b[0m\u001b[32m ‚Äì Number of Documents to return. Defaults to 4.`\\n*   \u001b[0m\n",
       "\u001b[32m`filter \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdict | None\u001b[0m\u001b[32m)\u001b[0m\u001b[32m ‚Äì Dictionary of argument\u001b[0m\u001b[32m(\u001b[0m\u001b[32ms\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to filter on metadata`\\n\\nFurther reading\\n\\n*   See the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mhow-to \u001b[0m\n",
       "\u001b[32mguide\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/how_to/vectorstores/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for more details on how to use the \u001b[0m\n",
       "\u001b[32m`similarity_search` method.\\n*   See the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mintegrations \u001b[0m\n",
       "\u001b[32mpage\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/integrations/vectorstores/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for more details on arguments that can be \u001b[0m\n",
       "\u001b[32mpassed in to the `similarity_search` method for specific vectorstores.\\n\\n### Metadata \u001b[0m\n",
       "\u001b[32mfiltering\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/vectorstores/#metadata-filtering \"Direct link to \u001b[0m\n",
       "\u001b[32mMetadata filtering\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nWhile vectorstore implement a search algorithm to efficiently search over _all_ the \u001b[0m\n",
       "\u001b[32membedded documents to find the most similar ones, many also support filtering on metadata. This allows structured \u001b[0m\n",
       "\u001b[32mfilters to reduce the size of the similarity search space. These two concepts work well together:\\n\\n1.  **Semantic\u001b[0m\n",
       "\u001b[32msearch**: Query the unstructured data directly, often using via embedding or keyword similarity.\\n2.  **Metadata \u001b[0m\n",
       "\u001b[32msearch**: Apply structured query to the metadata, filering specific documents.\\n\\nVector store support for metadata\u001b[0m\n",
       "\u001b[32mfiltering is typically dependent on the underlying vector store implementation.\\n\\nHere is example usage with \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mPinecone\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/integrations/vectorstores/pinecone/#query-directly\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, showing that we \u001b[0m\n",
       "\u001b[32mfilter for all documents that have the metadata key `source` with value \u001b[0m\n",
       "\u001b[32m`tweet`.\\n\\n```\\nvectorstore.similarity_search\u001b[0m\u001b[32m(\u001b[0m\u001b[32m    \"LangChain provides abstractions to make working with LLMs \u001b[0m\n",
       "\u001b[32measy\",    \u001b[0m\u001b[32mk\u001b[0m\u001b[32m=\u001b[0m\u001b[32m2\u001b[0m\u001b[32m,    \u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"source\": \"tweet\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m,\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n```\\n\\nAdvanced search and retrieval \u001b[0m\n",
       "\u001b[32mtechniques\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/vectorstores/#advanced-search-and-retrieval-technique\u001b[0m\n",
       "\u001b[32ms \"Direct link to Advanced search and retrieval \u001b[0m\n",
       "\u001b[32mtechniques\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n-----------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m---------------------------------------------------------------------------------------------------\\n\\nWhile \u001b[0m\n",
       "\u001b[32malgorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques \u001b[0m\n",
       "\u001b[32mcan be employed to improve search quality and diversity. For example, \u001b[0m\u001b[32m[\u001b[0m\u001b[32mmaximal marginal \u001b[0m\n",
       "\u001b[32mrelevance\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is a re-ranking \u001b[0m\n",
       "\u001b[32malgorithm used to diversify search results, which is applied after the initial similarity search to ensure a more \u001b[0m\n",
       "\u001b[32mdiverse set of results. As a second example, some \u001b[0m\u001b[32m[\u001b[0m\u001b[32mvector \u001b[0m\n",
       "\u001b[32mstores\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/integrations/retrievers/pinecone_hybrid_search/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m offer built-in \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mhybrid-search\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://docs.pinecone.io/guides/data/understanding-hybrid-search\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to combine keyword and semantic \u001b[0m\n",
       "\u001b[32msimilarity search, which marries the benefits of both approaches. At the moment, there is no unified way to perform\u001b[0m\n",
       "\u001b[32mhybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in \u001b[0m\n",
       "\u001b[32mwith `similarity_search`. See this \u001b[0m\u001b[32m[\u001b[0m\u001b[32mhow-to guide on hybrid \u001b[0m\n",
       "\u001b[32msearch\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/how_to/hybrid/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for more details.\\n\\n| Name                              \u001b[0m\n",
       "\u001b[32m| When to use                                           | Description                                              \u001b[0m\n",
       "\u001b[32m|\\n| \u001b[0m\n",
       "\u001b[32m-------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m-------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m--------- | ----------------------------------------------------- | \u001b[0m\n",
       "\u001b[32m-------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m------------------------- |\\n| \u001b[0m\u001b[32m[\u001b[0m\u001b[32mHybrid \u001b[0m\n",
       "\u001b[32msearch\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/integrations/retrievers/pinecone_hybrid_search/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m                         \u001b[0m\n",
       "\u001b[32m| When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, \u001b[0m\n",
       "\u001b[32mmarrying the benefits of both approaches. \u001b[0m\u001b[32m[\u001b[0m\u001b[32mPaper\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://arxiv.org/abs/2210.11934\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. |\\n| \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMaximal Marginal \u001b[0m\n",
       "\u001b[32mRelevance \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mMMR\u001b[0m\u001b[32m)\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVec\u001b[0m\n",
       "\u001b[32mtorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.max_marginal_relevance_search\u001b[0m\u001b[32m)\u001b[0m\u001b[32m | When needing to \u001b[0m\n",
       "\u001b[32mdiversify search results.             | MMR attempts to diversify the results of a search to avoid returning \u001b[0m\n",
       "\u001b[32msimilar and redundant documents.                                        |'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaders = [\n",
    "    TextLoader(\"data/langchain.md\"),\n",
    "    TextLoader(\"data/langchain2.md\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "full_docs = docs\n",
    "print(full_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c5a541-1359-4a91-a783-e1b27f13545b",
   "metadata": {},
   "source": [
    " ## Step 2: Split Documents into Large Chunks\n",
    " Using `RecursiveCharacterTextSplitter`, we split documents into smaller chunks\n",
    " for more efficient processing and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9dde2ed4-89d2-4cca-abf5-8ab6b8ecf59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c573e-f697-4d0c-babd-1a644ab89f55",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 3: Initialize the Vectorstore\n",
    " The vectorstore indexes the document chunks for similarity search. We use the `Chroma`\n",
    " library with embeddings from Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5a12c03f-c7da-41eb-8b8c-7ffd93c5855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\",\n",
    "    embedding_function=OllamaEmbeddings(model='snowflake-arctic-embed:33m')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6755b9eb-0f5b-4b82-94a2-ae4da717b9f5",
   "metadata": {},
   "source": [
    " ## Step 4: Create Smaller Chunks\n",
    " For fine-grained retrieval, we split the documents further into smaller chunks\n",
    " and associate metadata for linking with original documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "67c4eff4-9e45-455e-848a-8d838b922b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# Initialize storage for parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Create the retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Split into smaller chunks\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "sub_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id\n",
    "    sub_docs.extend(_sub_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d9fb9-824a-41fe-b4b0-f75e49d7b02b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 5: Add Documents to Vectorstore\n",
    " Add the smaller chunks and their metadata to the vectorstore for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9b680ec2-0672-43ec-8608-2bd7de1fd2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MultiVectorRetriever</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">vectorstore</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">langchain_chroma.vectorstores.Chroma</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x310473310</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">byte_store</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;langchain_core.stores.InMemoryByteStore object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x31045e910</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">docstore</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;langchain.storage.encoder_backed.EncoderBackedStore object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x31045d010</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">search_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mMultiVectorRetriever\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mvectorstore\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mlangchain_chroma.vectorstores.Chroma\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x310473310\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mbyte_store\u001b[0m\u001b[39m=<langchain_core.stores.InMemoryByteStore object at \u001b[0m\u001b[1;36m0x31045e910\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mdocstore\u001b[0m\u001b[39m=<langchain.storage.encoder_backed.EncoderBackedStore object at \u001b[0m\u001b[1;36m0x31045d010\u001b[0m\u001b[1m>\u001b[0m,\n",
       "    \u001b[33msearch_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.vectorstore.add_documents(sub_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22bb2d2-e1b6-4ee7-aea6-b384ee283c54",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 6: Perform a Similarity Search\n",
    " Use the retriever to find documents similar to the query \"LangChain\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "59881f72-e3db-4c88-8c5a-56ccae8ffc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'97524e7b-902d-4069-a3ce-06f873e0491f'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'doc_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'a5154e19-f37d-4514-aaba-68933f778c71'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'data/langchain.md'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'LangChain provides a consistent interface for working with chat models from different </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">providers while offering additional features for monitoring, debugging, and optimizing the performance of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">applications that use LLMs.'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fc6cac54-4b72-40f9-a65b-262a30072a65'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'doc_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'7a172ae0-0341-4830-9524-9bb45208da8f'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'data/langchain.md'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'LangChain provides a consistent interface for working with chat models from different </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">providers while offering additional features for monitoring, debugging, and optimizing the performance of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">applications that use LLMs.'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'97524e7b-902d-4069-a3ce-06f873e0491f'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'doc_id'\u001b[0m: \u001b[32m'a5154e19-f37d-4514-aaba-68933f778c71'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'data/langchain.md'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'LangChain provides a consistent interface for working with chat models from different \u001b[0m\n",
       "\u001b[32mproviders while offering additional features for monitoring, debugging, and optimizing the performance of \u001b[0m\n",
       "\u001b[32mapplications that use LLMs.'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'fc6cac54-4b72-40f9-a65b-262a30072a65'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'doc_id'\u001b[0m: \u001b[32m'7a172ae0-0341-4830-9524-9bb45208da8f'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'data/langchain.md'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'LangChain provides a consistent interface for working with chat models from different \u001b[0m\n",
       "\u001b[32mproviders while offering additional features for monitoring, debugging, and optimizing the performance of \u001b[0m\n",
       "\u001b[32mapplications that use LLMs.'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(retriever.vectorstore.similarity_search(\"LangChain\")[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31458ef6-090b-421d-ae81-0779ea1e2f5e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 7: Multi-Modal Retrieval\n",
    " Modify the search type to use Maximal Marginal Relevance (MMR) for diverse results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "69c34d97-635b-403c-9ce3-77316430cd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.retrievers.multi_vector import SearchType\n",
    "\n",
    "retriever.search_type = SearchType.mmr\n",
    "print(retriever.invoke(\"LangChain\")[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3370c4-a402-4a08-932a-7debe40afc32",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 8: Summarize Documents\n",
    " Associate summaries with documents using a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "17895d90-4ada-4f78-a4e4-5dfb93025c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"The document discusses LangChain, a platform for developing large language models (LLMs) that can be used as a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">chat model interface. Here's a summary of the key points:\\n\\n**Overview**\\n\\n* Large Language Models excel in tasks</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">like text generation, translation, summarization, and question answering without fine-tuning.\\n* Modern LLMs are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">accessed through a chat model interface, taking input messages and returning output </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">messages.\\n\\n**Capabilities**\\n\\n* **Tool calling**: Many popular chat models have native API for tool calling, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">allowing developers to interact with external services, APIs, and databases.\\n* **Structured outputs**: A technique</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to return responses in structured formats like JSON that match a given schema.\\n* **Multimodality**: The ability to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">work with data other than text, such as images, audio, and video.\\n\\nThese features enable the development of rich </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">applications that can interact with external services and perform various tasks.\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'The document provides an overview of LangChain, a framework for working with chat models from various </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">providers. It highlights the following features:\\n\\n- Integration with multiple chat model providers (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Anthropic, OpenAI, Ollama) and their respective formats.\\n- Support for messages in either the LangChain or OpenAI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">format.\\n- A standard tool calling API to bind tools to models and access their requests and results.\\n- Standard </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">API for structuring outputs using the `with_structured_output` method.\\n- Efficient batching capabilities.\\n- A </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rich streaming API.\\n\\nAdditionally, the document mentions integration with LangSmith for monitoring and debugging </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">production-grade applications based on LLMs. It also covers various features such as token usage, rate limiting, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">caching, and more.'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m\"The document discusses LangChain, a platform for developing large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that can be used as a\u001b[0m\n",
       "\u001b[32mchat model interface. Here's a summary of the key points:\\n\\n**Overview**\\n\\n* Large Language Models excel in tasks\u001b[0m\n",
       "\u001b[32mlike text generation, translation, summarization, and question answering without fine-tuning.\\n* Modern LLMs are \u001b[0m\n",
       "\u001b[32maccessed through a chat model interface, taking input messages and returning output \u001b[0m\n",
       "\u001b[32mmessages.\\n\\n**Capabilities**\\n\\n* **Tool calling**: Many popular chat models have native API for tool calling, \u001b[0m\n",
       "\u001b[32mallowing developers to interact with external services, APIs, and databases.\\n* **Structured outputs**: A technique\u001b[0m\n",
       "\u001b[32mto return responses in structured formats like JSON that match a given schema.\\n* **Multimodality**: The ability to\u001b[0m\n",
       "\u001b[32mwork with data other than text, such as images, audio, and video.\\n\\nThese features enable the development of rich \u001b[0m\n",
       "\u001b[32mapplications that can interact with external services and perform various tasks.\"\u001b[0m,\n",
       "    \u001b[32m'The document provides an overview of LangChain, a framework for working with chat models from various \u001b[0m\n",
       "\u001b[32mproviders. It highlights the following features:\\n\\n- Integration with multiple chat model providers \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32mAnthropic, OpenAI, Ollama\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and their respective formats.\\n- Support for messages in either the LangChain or OpenAI \u001b[0m\n",
       "\u001b[32mformat.\\n- A standard tool calling API to bind tools to models and access their requests and results.\\n- Standard \u001b[0m\n",
       "\u001b[32mAPI for structuring outputs using the `with_structured_output` method.\\n- Efficient batching capabilities.\\n- A \u001b[0m\n",
       "\u001b[32mrich streaming API.\\n\\nAdditionally, the document mentions integration with LangSmith for monitoring and debugging \u001b[0m\n",
       "\u001b[32mproduction-grade applications based on LLMs. It also covers various features such as token usage, rate limiting, \u001b[0m\n",
       "\u001b[32mcaching, and more.'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOllama(model='llama3.2:1b')\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Summarize documents in parallel\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})\n",
    "print(summaries[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09103003-a352-48ad-8e4e-77144b004366",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 9: Add Summaries to Vectorstore\n",
    " Store the summaries in the vectorstore for enhanced retrieval capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "af8b5863-4267-48ee-9396-cc0968111b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713fe068-a00d-42ad-9394-d3c0f4ccca03",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 10: Retrieve Summaries\n",
    " Perform a similarity search on the summaries to get the most relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "52de173c-6bb5-478c-b7f5-41373c930703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sub Docs:\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'810f2212-0166-4293-a9d3-ddf1834072d8'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'doc_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'a5154e19-f37d-4514-aaba-68933f778c71'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The document describes LangChain's features as follows:\\n\\n- Provides a consistent interface for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interacting with various chat models.\\n- Offers additional features such as monitoring, debugging, and optimization</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for improving application performance with Large Language Models (LLMs).\"</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Sub Docs:\n",
       "\n",
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'810f2212-0166-4293-a9d3-ddf1834072d8'\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'doc_id'\u001b[0m: \u001b[32m'a5154e19-f37d-4514-aaba-68933f778c71'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m document describes LangChain's features as follows:\\n\\n- Provides a consistent interface for \u001b[0m\n",
       "\u001b[32minteracting with various chat models.\\n- Offers additional features such as monitoring, debugging, and optimization\u001b[0m\n",
       "\u001b[32mfor improving application performance with Large Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\"\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrieved Docs:\n",
       "\n",
       "<span style=\"font-weight: bold\">[]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrieved Docs:\n",
       "\n",
       "\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub_docs = retriever.vectorstore.similarity_search(\"LangChain\")\n",
    "print(\"Sub Docs:\\n\",sub_docs[0])\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"LangChain\")\n",
    "print(\"Retrieved Docs:\\n\",retrieved_docs[0:2])\n",
    "# len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the retrieved documents and print the content of each\n",
    "for doc in retrieved_docs:\n",
    "\tprint(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
