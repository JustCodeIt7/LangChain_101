{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b5643f",
   "metadata": {},
   "source": [
    "# Multi Query Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:41.680097Z",
     "start_time": "2025-01-13T13:16:40.631343Z"
    }
   },
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loaders = [\n",
    "    TextLoader(\"data/langchain.md\"),\n",
    "    TextLoader(\"data/langchain2.md\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OllamaEmbeddings(model='snowflake-arctic-embed:33m')\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "eaf63df3",
   "metadata": {},
   "source": [
    "## Smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "id": "1f4e1796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:41.752737Z",
     "start_time": "2025-01-13T13:16:41.688627Z"
    }
   },
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "32c889bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:41.788943Z",
     "start_time": "2025-01-13T13:16:41.786006Z"
    }
   },
   "source": [
    "# The splitter to use to create smaller chunks\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "sub_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id\n",
    "    sub_docs.extend(_sub_docs)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "f055aadc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:43.161038Z",
     "start_time": "2025-01-13T13:16:41.792898Z"
    }
   },
   "source": [
    "retriever.vectorstore.add_documents(sub_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "08d006fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:43.177271Z",
     "start_time": "2025-01-13T13:16:43.165708Z"
    }
   },
   "source": [
    "retriever.vectorstore.similarity_search(\"LangChain\")[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': '1cbe455e-758c-4998-9aab-9476345b2909', 'source': 'data/langchain.md'}, page_content='LangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:43.196381Z",
     "start_time": "2025-01-13T13:16:43.186222Z"
    }
   },
   "source": [
    "retriever.vectorstore.similarity_search(\"LangChain\")[0]"
   ],
   "id": "e42cc30d2b97dc7c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': '1cbe455e-758c-4998-9aab-9476345b2909', 'source': 'data/langchain.md'}, page_content='LangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:43.220752Z",
     "start_time": "2025-01-13T13:16:43.209619Z"
    }
   },
   "source": [
    "retriever.vectorstore.similarity_search(\"LangChain\")[0]"
   ],
   "id": "f1bfe1c2aa3745b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': '1cbe455e-758c-4998-9aab-9476345b2909', 'source': 'data/langchain.md'}, page_content='LangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "9f9ffdf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:43.303601Z",
     "start_time": "2025-01-13T13:16:43.237421Z"
    }
   },
   "source": [
    "len(retriever.invoke(\"LangChain\")[0].page_content)\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9488"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "49b5a197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:43.333936Z",
     "start_time": "2025-01-13T13:16:43.308213Z"
    }
   },
   "source": [
    "from langchain.retrievers.multi_vector import SearchType\n",
    "\n",
    "retriever.search_type = SearchType.mmr\n",
    "\n",
    "len(retriever.invoke(\"LangChain\")[0].page_content)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9488"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "b1d3114f",
   "metadata": {},
   "source": [
    "## Associating summaries with a document for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "id": "12106958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:43.365701Z",
     "start_time": "2025-01-13T13:16:43.338544Z"
    }
   },
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm = ChatOllama(model='llama3.2:1b')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "187acba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:43.372919Z",
     "start_time": "2025-01-13T13:16:43.370445Z"
    }
   },
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chain = (\n",
    "        {\"doc\": lambda x: x.page_content}\n",
    "        | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "298bbc51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:43.379498Z",
     "start_time": "2025-01-13T13:16:43.377525Z"
    }
   },
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chain = (\n",
    "        {\"doc\": lambda x: x.page_content}\n",
    "        | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "bd5a29b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:55.872040Z",
     "start_time": "2025-01-13T13:16:43.383866Z"
    }
   },
   "source": [
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "b9c6c3a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:56.707336Z",
     "start_time": "2025-01-13T13:16:55.885536Z"
    }
   },
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "edbdfdb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:56.714169Z",
     "start_time": "2025-01-13T13:16:56.712784Z"
    }
   },
   "source": [
    "# # We can also add the original chunks to the vectorstore if we so want\n",
    "# for i, doc in enumerate(docs):\n",
    "#     doc.metadata[id_key] = doc_ids[i]\n",
    "# retriever.vectorstore.add_documents(docs)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "e4c7340d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:57.206463Z",
     "start_time": "2025-01-13T13:16:56.719543Z"
    }
   },
   "source": [
    "sub_docs = retriever.vectorstore.similarity_search(\"LangChain\")\n",
    "\n",
    "print(sub_docs[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='The document provides an overview of LangChain, a platform for developing and deploying language models (LLMs) without requiring task-specific fine-tuning. It explains how modern LLMs are accessed through a chat model interface, which takes messages as input and returns responses as output.\n",
      "\n",
      "Key features of LangChain include:\n",
      "\n",
      "*   A chat model interface that can handle multiple requests concurrently\n",
      "*   Support for [structured output](https://python.langchain.com/docs/concepts/structured_outputs/) for models that natively support this format\n",
      "*   Standard parameters for configuring the model, including temperature, maximum token count, and response waiting time\n",
      "\n",
      "The document also provides information on standard methods in LangChain, such as:\n",
      "\n",
      "*   `invoke`: The primary method for interacting with a chat model\n",
      "*   `stream`: A method that allows streaming the output of a chat model\n",
      "*   `batch`: A method for batching multiple requests together for more efficient processing\n",
      "*   `bind_tools`: A method for binding a tool to a chat model\n",
      "*   `with_structured_output`: A wrapper around the `invoke` method for models that natively support structured output\n",
      "\n",
      "Additionally, the document explains how LangChain supports two message formats: LangChain's own message format and OpenAI's Message Format. It also provides information on standard parameters that can be used to configure the model.\n",
      "\n",
      "LangChain is designed to provide a flexible and efficient way to develop and deploy LLMs without requiring extensive knowledge of machine learning or programming languages. The platform aims to make it easier for users to build and integrate language models into their applications, while still providing a robust set of tools and features for fine-tuning and optimizing these models.\n",
      "\n",
      "The key takeaways from the document are:\n",
      "\n",
      "*   LangChain is a platform for developing and deploying LLMs without requiring task-specific fine-tuning\n",
      "*   The chat model interface allows users to interact with LLMs by sending messages, which can be processed concurrently or in batches\n",
      "*   Standard parameters can be used to configure the model, including temperature, maximum token count, and response waiting time\n",
      "*   LangChain supports two message formats: LangChain's own message format and OpenAI's Message Format' metadata={'doc_id': 'b799c2d8-ebca-4f9b-ac08-6129d415de04'}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "e53180f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:16:57.627704Z",
     "start_time": "2025-01-13T13:16:57.211492Z"
    }
   },
   "source": [
    "retrieved_docs = retriever.invoke(\"LangChain\")\n",
    "\n",
    "len(retrieved_docs[0].page_content)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9488"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
