{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e4270c-086b-4720-a802-9f08832c285c",
   "metadata": {},
   "source": [
    " # Multi Query Retrievers\n",
    " This tutorial demonstrates how to build and use multi-query retrievers with LangChain.\n",
    " We'll start by loading documents, splitting them into chunks, and using embeddings\n",
    " to enable similarity search and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e42ff1-c67e-46e4-a291-606c008f2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from rich import print\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5672f51-1ca8-4f60-8e07-f9948a7ea3c0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 1: Load Documents\n",
    " Here, we load the documents using `TextLoader`. You can replace the file paths\n",
    " with your own document paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12301cf8-9fbe-4f1c-802d-2d5f407a03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    TextLoader(\"data/langchain.md\"),\n",
    "    TextLoader(\"data/langchain2.md\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c5a541-1359-4a91-a783-e1b27f13545b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 2: Split Documents into Chunks\n",
    " Using `RecursiveCharacterTextSplitter`, we split documents into smaller chunks\n",
    " for more efficient processing and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dde2ed4-89d2-4cca-abf5-8ab6b8ecf59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\n",
    "docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c573e-f697-4d0c-babd-1a644ab89f55",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 3: Initialize the Vectorstore\n",
    " The vectorstore indexes the document chunks for similarity search. We use the `Chroma`\n",
    " library with embeddings from Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a12c03f-c7da-41eb-8b8c-7ffd93c5855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\",\n",
    "    embedding_function=OllamaEmbeddings(model='snowflake-arctic-embed:33m')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6755b9eb-0f5b-4b82-94a2-ae4da717b9f5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 4: Create Smaller Chunks\n",
    " For fine-grained retrieval, we split the documents further into smaller chunks\n",
    " and associate metadata for linking with original documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c4eff4-9e45-455e-848a-8d838b922b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# Initialize storage for parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Create the retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Split into smaller chunks\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "sub_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id\n",
    "    sub_docs.extend(_sub_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d9fb9-824a-41fe-b4b0-f75e49d7b02b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 5: Add Documents to Vectorstore\n",
    " Add the smaller chunks and their metadata to the vectorstore for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b680ec2-0672-43ec-8608-2bd7de1fd2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MultiVectorRetriever</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">vectorstore</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">langchain_chroma.vectorstores.Chroma</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x12c5c8290</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">byte_store</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;langchain_core.stores.InMemoryByteStore object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x12c3f9bd0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">docstore</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;langchain.storage.encoder_backed.EncoderBackedStore object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x12c3860d0</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">search_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mMultiVectorRetriever\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mvectorstore\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mlangchain_chroma.vectorstores.Chroma\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x12c5c8290\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mbyte_store\u001b[0m\u001b[39m=<langchain_core.stores.InMemoryByteStore object at \u001b[0m\u001b[1;36m0x12c3f9bd0\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mdocstore\u001b[0m\u001b[39m=<langchain.storage.encoder_backed.EncoderBackedStore object at \u001b[0m\u001b[1;36m0x12c3860d0\u001b[0m\u001b[1m>\u001b[0m,\n",
       "    \u001b[33msearch_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.vectorstore.add_documents(sub_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22bb2d2-e1b6-4ee7-aea6-b384ee283c54",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 6: Perform a Similarity Search\n",
    " Use the retriever to find documents similar to the query \"LangChain\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59881f72-e3db-4c88-8c5a-56ccae8ffc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'defc5659-e538-472e-9aa5-abe17938a388'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'doc_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'9c4e0d26-1836-4056-8643-6ea0f58bf49b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'data/langchain.md'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'LangChain provides a consistent interface for working with chat models from different providers </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">while offering additional features for monitoring, debugging, and optimizing the performance of applications that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">use LLMs.'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'defc5659-e538-472e-9aa5-abe17938a388'\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'doc_id'\u001b[0m: \u001b[32m'9c4e0d26-1836-4056-8643-6ea0f58bf49b'\u001b[0m, \u001b[32m'source'\u001b[0m: \u001b[32m'data/langchain.md'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'LangChain provides a consistent interface for working with chat models from different providers \u001b[0m\n",
       "\u001b[32mwhile offering additional features for monitoring, debugging, and optimizing the performance of applications that \u001b[0m\n",
       "\u001b[32muse LLMs.'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(retriever.vectorstore.similarity_search(\"LangChain\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31458ef6-090b-421d-ae81-0779ea1e2f5e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 7: Multi-Modal Retrieval\n",
    " Modify the search type to use Maximal Marginal Relevance (MMR) for diverse results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69c34d97-635b-403c-9ce3-77316430cd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/langchain.md'}, page_content='Title: Chat models | ü¶úÔ∏èüîó LangChain\\n\\nURL Source: https://python.langchain.com/docs/concepts/chat_models/\\n\\nMarkdown Content:\\nOverview[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#overview \"Direct link to Overview\")\\n-------------------------------------------------------------------------------------------------------\\n\\nLarge Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.\\n\\nModern LLMs are typically accessed through a chat model interface that takes a list of [messages](https://python.langchain.com/docs/concepts/messages/) as input and returns a [message](https://python.langchain.com/docs/concepts/messages/) as output.\\n\\nThe newest generation of chat models offer additional capabilities:\\n\\n*   [Tool calling](https://python.langchain.com/docs/concepts/tool_calling/): Many popular chat models offer a native [tool calling](https://python.langchain.com/docs/concepts/tool_calling/) API. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.\\n*   [Structured output](https://python.langchain.com/docs/concepts/structured_outputs/): A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n*   [Multimodality](https://python.langchain.com/docs/concepts/multimodality/): The ability to work with data other than text; for example, images, audio, and video.\\n\\nFeatures[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#features \"Direct link to Features\")\\n-------------------------------------------------------------------------------------------------------\\n\\nLangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.\\n\\n*   Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](https://python.langchain.com/docs/integrations/chat/) for an up-to-date list of supported models.\\n*   Use either LangChain\\'s [messages](https://python.langchain.com/docs/concepts/messages/) format or OpenAI format.\\n*   Standard [tool calling API](https://python.langchain.com/docs/concepts/tool_calling/): standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model.\\n*   Standard API for [structuring outputs](https://python.langchain.com/docs/concepts/structured_outputs/#structured-output-method) via the `with_structured_output` method.\\n*   Provides support for [async programming](https://python.langchain.com/docs/concepts/async/), [efficient batching](https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch), [a rich streaming API](https://python.langchain.com/docs/concepts/streaming/).\\n*   Integration with [LangSmith](https://docs.smith.langchain.com/) for monitoring and debugging production-grade applications based on LLMs.\\n*   Additional features like standardized [token usage](https://python.langchain.com/docs/concepts/messages/#aimessage), [rate limiting](https://python.langchain.com/docs/concepts/chat_models/#rate-limiting), [caching](https://python.langchain.com/docs/concepts/chat_models/#caching) and more.\\n\\nIntegrations[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#integrations \"Direct link to Integrations\")\\n-------------------------------------------------------------------------------------------------------------------\\n\\nLangChain has many chat model integrations that allow you to use a wide variety of models from different providers.\\n\\nThese integrations are one of two types:\\n\\n1.  **Official models**: These are models that are officially supported by LangChain and/or model provider. You can find these models in the `langchain-<provider>` packages.\\n2.  **Community models**: There are models that are mostly contributed and supported by the community. You can find these models in the `langchain-community` package.\\n\\nLangChain chat models are named with a convention that prefixes \"Chat\" to their class names (e.g., `ChatOllama`, `ChatAnthropic`, `ChatOpenAI`, etc.).\\n\\nPlease review the [chat model integrations](https://python.langchain.com/docs/integrations/chat/) for a list of supported models.\\n\\nnote\\n\\nModels that do **not** include the prefix \"Chat\" in their name or include \"LLM\" as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.\\n\\nInterface[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#interface \"Direct link to Interface\")\\n----------------------------------------------------------------------------------------------------------\\n\\nLangChain chat models implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface. Because `BaseChatModel` also implements the [Runnable Interface](https://python.langchain.com/docs/concepts/runnables/), chat models support a [standard streaming interface](https://python.langchain.com/docs/concepts/streaming/), [async programming](https://python.langchain.com/docs/concepts/async/), optimized [batching](https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch), and more. Please see the [Runnable Interface](https://python.langchain.com/docs/concepts/runnables/) for more details.\\n\\nMany of the key methods of chat models operate on [messages](https://python.langchain.com/docs/concepts/messages/) as input and return messages as output.\\n\\nChat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the [standard parameters](https://python.langchain.com/docs/concepts/chat_models/#standard-parameters) section for more details.\\n\\nnote\\n\\nIn documentation, we will often use the terms \"LLM\" and \"Chat Model\" interchangeably. This is because most modern LLMs are exposed to users via a chat model interface.\\n\\nHowever, LangChain also has implementations of older LLMs that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. These models are typically named without the \"Chat\" prefix (e.g., `Ollama`, `Anthropic`, `OpenAI`, etc.). These models implement the [BaseLLM](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.BaseLLM.html#langchain_core.language_models.llms.BaseLLM) interface and may be named with the \"LLM\" suffix (e.g., `OllamaLLM`, `AnthropicLLM`, `OpenAILLM`, etc.). Generally, users should not use these models.\\n\\n### Key methods[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#key-methods \"Direct link to Key methods\")\\n\\nThe key methods of a chat model are:\\n\\n1.  **invoke**: The primary method for interacting with a chat model. It takes a list of [messages](https://python.langchain.com/docs/concepts/messages/) as input and returns a list of messages as output.\\n2.  **stream**: A method that allows you to stream the output of a chat model as it is generated.\\n3.  **batch**: A method that allows you to batch multiple requests to a chat model together for more efficient processing.\\n4.  **bind\\\\_tools**: A method that allows you to bind a tool to a chat model for use in the model\\'s execution context.\\n5.  **with\\\\_structured\\\\_output**: A wrapper around the `invoke` method for models that natively support [structured output](https://python.langchain.com/docs/concepts/structured_outputs/).\\n\\nOther important methods can be found in the [BaseChatModel API Reference](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html).\\n\\n### Inputs and outputs[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#inputs-and-outputs \"Direct link to Inputs and outputs\")\\n\\nModern LLMs are typically accessed through a chat model interface that takes [messages](https://python.langchain.com/docs/concepts/messages/) as input and returns [messages](https://python.langchain.com/docs/concepts/messages/) as output. Messages are typically associated with a role (e.g., \"system\", \"human\", \"assistant\") and one or more content blocks that contain text or potentially multimodal data (e.g., images, audio, video).\\n\\nLangChain supports two message formats to interact with chat models:\\n\\n1.  **LangChain Message Format**: LangChain\\'s own message format, which is used by default and is used internally by LangChain.\\n2.  **OpenAI\\'s Message Format**: OpenAI\\'s message format.\\n\\n### Standard parameters[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#standard-parameters \"Direct link to Standard parameters\")\\n\\nMany chat models have standardized parameters that can be used to configure the model:'),\n",
       " Document(metadata={'source': 'data/langchain2.md'}, page_content='Title: Vector stores | ü¶úÔ∏èüîó LangChain\\n\\nURL Source: https://python.langchain.com/docs/concepts/vectorstores/\\n\\nMarkdown Content:\\nNote\\n\\nThis conceptual overview focuses on text-based indexing and retrieval for simplicity. However, embedding models can be [multi-modal](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings) and vector stores can be used to store and retrieve a variety of data types beyond text.\\n\\nOverview[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#overview \"Direct link to Overview\")\\n--------------------------------------------------------------------------------------------------------\\n\\nVector stores are specialized data stores that enable indexing and retrieving information based on vector representations.\\n\\nThese vectors, called [embeddings](https://python.langchain.com/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.\\n\\nVector stores are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches.\\n\\n![Image 5: Vector stores](https://python.langchain.com/assets/images/vectorstores-2540b4bc355b966c99b0f02cfdddb273.png)\\n\\nIntegrations[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#integrations \"Direct link to Integrations\")\\n--------------------------------------------------------------------------------------------------------------------\\n\\nLangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations.\\n\\nPlease see the [full list of LangChain vectorstore integrations](https://python.langchain.com/docs/integrations/vectorstores/).\\n\\nInterface[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#interface \"Direct link to Interface\")\\n-----------------------------------------------------------------------------------------------------------\\n\\nLangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations.\\n\\nThe interface consists of basic methods for writing, deleting and searching for documents in the vector store.\\n\\nThe key methods are:\\n\\n*   `add_documents`: Add a list of texts to the vector store.\\n*   `delete_documents`: Delete a list of documents from the vector store.\\n*   `similarity_search`: Search for similar documents to a given query.\\n\\nInitialization[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#initialization \"Direct link to Initialization\")\\n--------------------------------------------------------------------------------------------------------------------------\\n\\nMost vectors in LangChain accept an embedding model as an argument when initializing the vector store.\\n\\nWe will use LangChain\\'s [InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) implementation to illustrate the API.\\n\\n```\\nfrom langchain_core.vectorstores import InMemoryVectorStore# Initialize with an embedding modelvector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())\\n```\\n\\nAdding documents[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#adding-documents \"Direct link to Adding documents\")\\n--------------------------------------------------------------------------------------------------------------------------------\\n\\nTo add documents, use the `add_documents` method.\\n\\nThis API works with a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects. `Document` objects all have `page_content` and `metadata` attributes, making them a universal way to store unstructured text and associated metadata.\\n\\n```\\nfrom langchain_core.documents import Documentdocument_1 = Document(    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",    metadata={\"source\": \"tweet\"},)document_2 = Document(    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",    metadata={\"source\": \"news\"},)documents = [document_1, document_2]vector_store.add_documents(documents=documents)\\n```\\n\\nYou should usually provide IDs for the documents you add to the vector store, so that instead of adding the same document multiple times, you can update the existing document.\\n\\n```\\nvector_store.add_documents(documents=documents, ids=[\"doc1\", \"doc2\"])\\n```\\n\\nDelete[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#delete \"Direct link to Delete\")\\n--------------------------------------------------------------------------------------------------\\n\\nTo delete documents, use the `delete_documents` method which takes a list of document IDs to delete.\\n\\n```\\nvector_store.delete_documents(ids=[\"doc1\"])\\n```\\n\\nSearch[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#search \"Direct link to Search\")\\n--------------------------------------------------------------------------------------------------\\n\\nVector stores embed and store the documents that added. If we pass in a query, the vectorstore will embed the query, perform a similarity search over the embedded documents, and return the most similar ones. This captures two important concepts: first, there needs to be a way to measure the similarity between the query and _any_ [embedded](https://python.langchain.com/docs/concepts/embedding_models/) document. Second, there needs to be an algorithm to efficiently perform this similarity search across _all_ embedded documents.\\n\\n### Similarity metrics[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#similarity-metrics \"Direct link to Similarity metrics\")\\n\\nA critical advantage of embeddings vectors is they can be compared using many simple mathematical operations:\\n\\n*   **Cosine Similarity**: Measures the cosine of the angle between two vectors.\\n*   **Euclidean Distance**: Measures the straight-line distance between two points.\\n*   **Dot Product**: Measures the projection of one vector onto another.\\n\\nThe choice of similarity metric can sometimes be selected when initializing the vectorstore. Please refer to the documentation of the specific vectorstore you are using to see what similarity metrics are supported.\\n\\nFurther reading\\n\\n*   See [this documentation](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity) from Google on similarity metrics to consider with embeddings.\\n*   See Pinecone\\'s [blog post](https://www.pinecone.io/learn/vector-similarity/) on similarity metrics.\\n*   See OpenAI\\'s [FAQ](https://platform.openai.com/docs/guides/embeddings/faq) on what similarity metric to use with OpenAI embeddings.\\n\\n### Similarity search[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#similarity-search \"Direct link to Similarity search\")\\n\\nGiven a similarity metric to measure the distance between the embedded query and any embedded document, we need an algorithm to efficiently search over _all_ the embedded documents to find the most similar ones. There are various ways to do this. As an example, many vectorstores implement [HNSW (Hierarchical Navigable Small World)](https://www.pinecone.io/learn/series/faiss/hnsw/), a graph-based index structure that allows for efficient similarity search. Regardless of the search algorithm used under the hood, the LangChain vectorstore interface has a `similarity_search` method for all integrations. This will take the search query, create an embedding, find similar documents, and return them as a list of [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html).\\n\\n```\\nquery = \"my query\"docs = vectorstore.similarity_search(query)\\n```\\n\\nMany vectorstores support search parameters to be passed with the `similarity_search` method. See the documentation for the specific vectorstore you are using to see what parameters are supported. As an example [Pinecone](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.similarity_search) several parameters that are important general concepts: Many vectorstores support [the `k`](https://python.langchain.com/docs/integrations/vectorstores/pinecone/#query-directly), which controls the number of Documents to return, and `filter`, which allows for filtering documents by metadata.\\n\\n*   `query (str) ‚Äì Text to look up documents similar to.`\\n*   `k (int) ‚Äì Number of Documents to return. Defaults to 4.`\\n*   `filter (dict | None) ‚Äì Dictionary of argument(s) to filter on metadata`\\n\\nFurther reading\\n\\n*   See the [how-to guide](https://python.langchain.com/docs/how_to/vectorstores/) for more details on how to use the `similarity_search` method.\\n*   See the [integrations page](https://python.langchain.com/docs/integrations/vectorstores/) for more details on arguments that can be passed in to the `similarity_search` method for specific vectorstores.\\n\\n### Metadata filtering[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#metadata-filtering \"Direct link to Metadata filtering\")\\n\\nWhile vectorstore implement a search algorithm to efficiently search over _all_ the embedded documents to find the most similar ones, many also support filtering on metadata. This allows structured filters to reduce the size of the similarity search space. These two concepts work well together:\\n\\n1.  **Semantic search**: Query the unstructured data directly, often using via embedding or keyword similarity.\\n2.  **Metadata search**: Apply structured query to the metadata, filering specific documents.\\n\\nVector store support for metadata filtering is typically dependent on the underlying vector store implementation.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.multi_vector import SearchType\n",
    "\n",
    "retriever.search_type = SearchType.mmr\n",
    "retriever.invoke(\"LangChain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3370c4-a402-4a08-932a-7debe40afc32",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 8: Summarize Documents\n",
    " Associate summaries with documents using a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17895d90-4ada-4f78-a4e4-5dfb93025c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"The document provides an overview of LangChain, a platform for interacting with large language models (LLMs). </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Here's a summary:\\n\\n**Overview**\\n\\nLangChain is a platform for working with LLMs through a chat model interface. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">It allows users to interact with chat models by sending input messages and receiving output messages.\\n\\n**Key </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Features**\\n\\n1. **Standard Streaming Interface**: LangChain supports standard streaming interfaces, which allow </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for efficient processing of multiple requests in batches.\\n2. **Async Programming**: Chat models support async </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">programming, making it easy to write concurrent code.\\n3. **Optimized Batching**: Many chat models have optimized </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">batching capabilities, allowing for more efficient use of resources.\\n\\n**Inputs and Outputs**\\n\\n1. **Input </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Messages**: Modern LLMs are typically accessed through a chat model interface that takes input messages as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">input.\\n2. **Output Messages**: The output messages are sent back to the user in response to their input </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">messages.\\n\\n**Standard Parameters**\\n\\nSome chat models have standardized parameters, including:\\n\\n1. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">**Temperature**: Many chat models allow users to control the temperature of their outputs.\\n2. **Maximum Number of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Tokens**: Some chat models allow users to specify a maximum number of tokens for their responses.\\n3. **Maximum </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Time to Wait**: Some chat models allow users to specify a maximum time to wait for a response.\\n\\n**Key </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Methods**\\n\\n1. **Invoke**: The primary method for interacting with a chat model, which takes input messages and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">returns output messages.\\n2. **Stream**: A method that allows for streaming the output of a chat model as it is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generated.\\n3. **Batch**: A method that allows for batching multiple requests together for more efficient </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">processing.\\n4. **Bind Tools**: A method that allows users to bind tools to their chat models.\\n5. **With </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Structured Output**: A wrapper around the invoke method for models that natively support structured </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output.\\n\\nOverall, LangChain provides a flexible and efficient way to interact with LLMs through a chat model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interface.\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"The document provides an overview of how to configure and use multiple AI models in a chat application. Here's</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a summary of the key points:\\n\\n**Standard Parameters**\\n\\n*   `model`: The name or identifier of the specific AI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model you want to use.\\n*   `temperature`: Controls the randomness of the model's output, with higher values making</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">responses more creative and lower values focused on deterministic answers.\\n*   `timeout`: Sets the maximum time </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(in seconds) to wait for a response from the model before canceling the request.\\n*   `max_tokens`: Limits the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">total number of tokens in the response, controlling how long the output can be.\\n*   `stop`: Specifies stop </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequences that indicate when the model should stop generating tokens.\\n*   `max_retries`: Sets the maximum number </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of attempts to resend a request if it fails due to issues like network timeouts or rate limits.\\n*   `api_key`: The</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">API key required for authenticating with the model provider.\\n*   `base_url`: The URL of the API endpoint where </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">requests are sent.\\n*   `rate_limiter`: An optional parameter that controls the rate at which requests are made to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the model provider.\\n\\n**Important Notes**\\n\\n*   Standard parameters only apply to model providers that expose </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">these parameters.\\n*   Standard parameters are currently not enforced on integrations with models in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`langchain-community`.\\n*   Other parameters specific to each integration should be found in their respective API </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">references.\\n\\n**Other Parameters and Features**\\n\\n*   Chat models can call tools to perform tasks such as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fetching data from a database or running custom code.\\n*   Structured outputs are available, allowing for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information extraction tasks.\\n*   Multimodality allows chat models to process other types of data, such as images,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">audio, and video.\\n*   Context windows refer to the maximum size of the input sequence that the model can process </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at one time.\\n\\n**Rate-Limiting and Caching**\\n\\n*   Chat models have rate-limiting features that control the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">number of requests made in a given time period.\\n*   Advanced topics include how to handle rate limits, including </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">spacing out requests, recovering from rate limit errors, and using caching.\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"The document discusses the use of caching in a chat model. It highlights two approaches to caching: \\n\\n1.  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">**Exact Input Caching**: This approach caches the exact inputs into the model, which is unlikely after multiple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interactions in a conversation.\\n2.  **Semantic Caching**: This approach caches responses based on the meaning of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the input rather than the exact input itself.\\n\\nThe document notes that semantic caching can be effective in some </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">situations but not others, and it's not guaranteed to capture the meaning of the input accurately. However, it </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">suggests that there might be scenarios where caching chat model responses is beneficial, such as when the same </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">questions are asked frequently.\\n\\nAdditionally, the document provides information on how to cache chat model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">responses using a guide, which includes:\\n\\n*   How-to guides for using chat models\\n*   A list of supported chat </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models\\n\\nIt also mentions some conceptual guides that discuss messages, tool calling, multimodality, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">structured outputs.\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"The document provides an overview of vector stores in LangChain, a Python library that allows you to build </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">scalable and efficient machine learning models. Here's a summary of the key points:\\n\\n**What are Vector </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Stores?**\\n\\nVector stores are specialized data stores that enable indexing and retrieval information based on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vector representations. These vectors, called embeddings, capture the semantic meaning of data that has been </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">embedded.\\n\\n**Overview**\\n\\n*   Vector stores are used to search over unstructured data, such as text, images, and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches.\\n*   The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LangChain library provides a Python interface for building vector stores and performing similarity </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">searches.\\n\\n**Similarity Search**\\n\\nThe document discusses the process of similarity searching in vector stores. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">This involves:\\n\\n1.  Querying the unstructured data directly using embedding or keyword similarity.\\n2.  Applying </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">structured query to the metadata, filtering specific documents.\\n\\n**Metadata Filtering**\\n\\nVector store support </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for metadata filtering is typically dependent on the underlying vector store implementation. Some common ways to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">filter metadata include:\\n\\n*   **Semantic search**: Querying unstructured data directly using embedding or keyword</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">similarity.\\n*   **Metadata search**: Applying structured query to the metadata, filtering specific </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documents.\\n\\n**K and Filter Parameters**\\n\\nThe document provides information about the `k` parameter in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`similarity_search` method, which controls the number of Documents to return. Additionally, it mentions that you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can pass filters to the `filter` argument to further refine your search results.\\n\\nOverall, vector stores are a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">powerful tool for building scalable and efficient machine learning models. By providing a Python interface for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">building vector stores and performing similarity searches, LangChain makes it easy to integrate these capabilities </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">into your existing workflow.\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'The document discusses vector store support for metadata filtering in LangChain, which is typically dependent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on the underlying vector store implementation. It provides examples using Pinecone as an example of how to filter </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">metadata with specific keywords.\\n\\nAdditionally, it highlights advanced search and retrieval techniques that can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">be employed to improve search quality and diversity, such as maximal marginal relevance (MMR) algorithm, a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">re-ranking technique used to diversify search results after the initial similarity search. It also mentions hybrid </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">search capabilities offered by some vector stores like Pinecone, which combine keyword-based and semantic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">similarity search.\\n\\nThe document provides guidelines on when to use each of these techniques, including:\\n\\n- </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Hybrid search: When combining keyword-based and semantic similarity.\\n- Maximal Marginal Relevance (MMR): When </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">needing to diversify search results.'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m\"The document provides an overview of LangChain, a platform for interacting with large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. \u001b[0m\n",
       "\u001b[32mHere's a summary:\\n\\n**Overview**\\n\\nLangChain is a platform for working with LLMs through a chat model interface. \u001b[0m\n",
       "\u001b[32mIt allows users to interact with chat models by sending input messages and receiving output messages.\\n\\n**Key \u001b[0m\n",
       "\u001b[32mFeatures**\\n\\n1. **Standard Streaming Interface**: LangChain supports standard streaming interfaces, which allow \u001b[0m\n",
       "\u001b[32mfor efficient processing of multiple requests in batches.\\n2. **Async Programming**: Chat models support async \u001b[0m\n",
       "\u001b[32mprogramming, making it easy to write concurrent code.\\n3. **Optimized Batching**: Many chat models have optimized \u001b[0m\n",
       "\u001b[32mbatching capabilities, allowing for more efficient use of resources.\\n\\n**Inputs and Outputs**\\n\\n1. **Input \u001b[0m\n",
       "\u001b[32mMessages**: Modern LLMs are typically accessed through a chat model interface that takes input messages as \u001b[0m\n",
       "\u001b[32minput.\\n2. **Output Messages**: The output messages are sent back to the user in response to their input \u001b[0m\n",
       "\u001b[32mmessages.\\n\\n**Standard Parameters**\\n\\nSome chat models have standardized parameters, including:\\n\\n1. \u001b[0m\n",
       "\u001b[32m**Temperature**: Many chat models allow users to control the temperature of their outputs.\\n2. **Maximum Number of \u001b[0m\n",
       "\u001b[32mTokens**: Some chat models allow users to specify a maximum number of tokens for their responses.\\n3. **Maximum \u001b[0m\n",
       "\u001b[32mTime to Wait**: Some chat models allow users to specify a maximum time to wait for a response.\\n\\n**Key \u001b[0m\n",
       "\u001b[32mMethods**\\n\\n1. **Invoke**: The primary method for interacting with a chat model, which takes input messages and \u001b[0m\n",
       "\u001b[32mreturns output messages.\\n2. **Stream**: A method that allows for streaming the output of a chat model as it is \u001b[0m\n",
       "\u001b[32mgenerated.\\n3. **Batch**: A method that allows for batching multiple requests together for more efficient \u001b[0m\n",
       "\u001b[32mprocessing.\\n4. **Bind Tools**: A method that allows users to bind tools to their chat models.\\n5. **With \u001b[0m\n",
       "\u001b[32mStructured Output**: A wrapper around the invoke method for models that natively support structured \u001b[0m\n",
       "\u001b[32moutput.\\n\\nOverall, LangChain provides a flexible and efficient way to interact with LLMs through a chat model \u001b[0m\n",
       "\u001b[32minterface.\"\u001b[0m,\n",
       "    \u001b[32m\"The document provides an overview of how to configure and use multiple AI models in a chat application. Here's\u001b[0m\n",
       "\u001b[32ma summary of the key points:\\n\\n**Standard Parameters**\\n\\n*   `model`: The name or identifier of the specific AI \u001b[0m\n",
       "\u001b[32mmodel you want to use.\\n*   `temperature`: Controls the randomness of the model's output, with higher values making\u001b[0m\n",
       "\u001b[32mresponses more creative and lower values focused on deterministic answers.\\n*   `timeout`: Sets the maximum time \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32min seconds\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to wait for a response from the model before canceling the request.\\n*   `max_tokens`: Limits the \u001b[0m\n",
       "\u001b[32mtotal number of tokens in the response, controlling how long the output can be.\\n*   `stop`: Specifies stop \u001b[0m\n",
       "\u001b[32msequences that indicate when the model should stop generating tokens.\\n*   `max_retries`: Sets the maximum number \u001b[0m\n",
       "\u001b[32mof attempts to resend a request if it fails due to issues like network timeouts or rate limits.\\n*   `api_key`: The\u001b[0m\n",
       "\u001b[32mAPI key required for authenticating with the model provider.\\n*   `base_url`: The URL of the API endpoint where \u001b[0m\n",
       "\u001b[32mrequests are sent.\\n*   `rate_limiter`: An optional parameter that controls the rate at which requests are made to \u001b[0m\n",
       "\u001b[32mthe model provider.\\n\\n**Important Notes**\\n\\n*   Standard parameters only apply to model providers that expose \u001b[0m\n",
       "\u001b[32mthese parameters.\\n*   Standard parameters are currently not enforced on integrations with models in \u001b[0m\n",
       "\u001b[32m`langchain-community`.\\n*   Other parameters specific to each integration should be found in their respective API \u001b[0m\n",
       "\u001b[32mreferences.\\n\\n**Other Parameters and Features**\\n\\n*   Chat models can call tools to perform tasks such as \u001b[0m\n",
       "\u001b[32mfetching data from a database or running custom code.\\n*   Structured outputs are available, allowing for \u001b[0m\n",
       "\u001b[32minformation extraction tasks.\\n*   Multimodality allows chat models to process other types of data, such as images,\u001b[0m\n",
       "\u001b[32maudio, and video.\\n*   Context windows refer to the maximum size of the input sequence that the model can process \u001b[0m\n",
       "\u001b[32mat one time.\\n\\n**Rate-Limiting and Caching**\\n\\n*   Chat models have rate-limiting features that control the \u001b[0m\n",
       "\u001b[32mnumber of requests made in a given time period.\\n*   Advanced topics include how to handle rate limits, including \u001b[0m\n",
       "\u001b[32mspacing out requests, recovering from rate limit errors, and using caching.\"\u001b[0m,\n",
       "    \u001b[32m\"The document discusses the use of caching in a chat model. It highlights two approaches to caching: \\n\\n1.  \u001b[0m\n",
       "\u001b[32m**Exact Input Caching**: This approach caches the exact inputs into the model, which is unlikely after multiple \u001b[0m\n",
       "\u001b[32minteractions in a conversation.\\n2.  **Semantic Caching**: This approach caches responses based on the meaning of \u001b[0m\n",
       "\u001b[32mthe input rather than the exact input itself.\\n\\nThe document notes that semantic caching can be effective in some \u001b[0m\n",
       "\u001b[32msituations but not others, and it's not guaranteed to capture the meaning of the input accurately. However, it \u001b[0m\n",
       "\u001b[32msuggests that there might be scenarios where caching chat model responses is beneficial, such as when the same \u001b[0m\n",
       "\u001b[32mquestions are asked frequently.\\n\\nAdditionally, the document provides information on how to cache chat model \u001b[0m\n",
       "\u001b[32mresponses using a guide, which includes:\\n\\n*   How-to guides for using chat models\\n*   A list of supported chat \u001b[0m\n",
       "\u001b[32mmodels\\n\\nIt also mentions some conceptual guides that discuss messages, tool calling, multimodality, and \u001b[0m\n",
       "\u001b[32mstructured outputs.\"\u001b[0m,\n",
       "    \u001b[32m\"The document provides an overview of vector stores in LangChain, a Python library that allows you to build \u001b[0m\n",
       "\u001b[32mscalable and efficient machine learning models. Here's a summary of the key points:\\n\\n**What are Vector \u001b[0m\n",
       "\u001b[32mStores?**\\n\\nVector stores are specialized data stores that enable indexing and retrieval information based on \u001b[0m\n",
       "\u001b[32mvector representations. These vectors, called embeddings, capture the semantic meaning of data that has been \u001b[0m\n",
       "\u001b[32membedded.\\n\\n**Overview**\\n\\n*   Vector stores are used to search over unstructured data, such as text, images, and\u001b[0m\n",
       "\u001b[32maudio, to retrieve relevant information based on semantic similarity rather than exact keyword matches.\\n*   The \u001b[0m\n",
       "\u001b[32mLangChain library provides a Python interface for building vector stores and performing similarity \u001b[0m\n",
       "\u001b[32msearches.\\n\\n**Similarity Search**\\n\\nThe document discusses the process of similarity searching in vector stores. \u001b[0m\n",
       "\u001b[32mThis involves:\\n\\n1.  Querying the unstructured data directly using embedding or keyword similarity.\\n2.  Applying \u001b[0m\n",
       "\u001b[32mstructured query to the metadata, filtering specific documents.\\n\\n**Metadata Filtering**\\n\\nVector store support \u001b[0m\n",
       "\u001b[32mfor metadata filtering is typically dependent on the underlying vector store implementation. Some common ways to \u001b[0m\n",
       "\u001b[32mfilter metadata include:\\n\\n*   **Semantic search**: Querying unstructured data directly using embedding or keyword\u001b[0m\n",
       "\u001b[32msimilarity.\\n*   **Metadata search**: Applying structured query to the metadata, filtering specific \u001b[0m\n",
       "\u001b[32mdocuments.\\n\\n**K and Filter Parameters**\\n\\nThe document provides information about the `k` parameter in the \u001b[0m\n",
       "\u001b[32m`similarity_search` method, which controls the number of Documents to return. Additionally, it mentions that you \u001b[0m\n",
       "\u001b[32mcan pass filters to the `filter` argument to further refine your search results.\\n\\nOverall, vector stores are a \u001b[0m\n",
       "\u001b[32mpowerful tool for building scalable and efficient machine learning models. By providing a Python interface for \u001b[0m\n",
       "\u001b[32mbuilding vector stores and performing similarity searches, LangChain makes it easy to integrate these capabilities \u001b[0m\n",
       "\u001b[32minto your existing workflow.\"\u001b[0m,\n",
       "    \u001b[32m'The document discusses vector store support for metadata filtering in LangChain, which is typically dependent \u001b[0m\n",
       "\u001b[32mon the underlying vector store implementation. It provides examples using Pinecone as an example of how to filter \u001b[0m\n",
       "\u001b[32mmetadata with specific keywords.\\n\\nAdditionally, it highlights advanced search and retrieval techniques that can \u001b[0m\n",
       "\u001b[32mbe employed to improve search quality and diversity, such as maximal marginal relevance \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMMR\u001b[0m\u001b[32m)\u001b[0m\u001b[32m algorithm, a \u001b[0m\n",
       "\u001b[32mre-ranking technique used to diversify search results after the initial similarity search. It also mentions hybrid \u001b[0m\n",
       "\u001b[32msearch capabilities offered by some vector stores like Pinecone, which combine keyword-based and semantic \u001b[0m\n",
       "\u001b[32msimilarity search.\\n\\nThe document provides guidelines on when to use each of these techniques, including:\\n\\n- \u001b[0m\n",
       "\u001b[32mHybrid search: When combining keyword-based and semantic similarity.\\n- Maximal Marginal Relevance \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMMR\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: When \u001b[0m\n",
       "\u001b[32mneeding to diversify search results.'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOllama(model='llama3.2:1b')\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Summarize documents in parallel\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09103003-a352-48ad-8e4e-77144b004366",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 9: Add Summaries to Vectorstore\n",
    " Store the summaries in the vectorstore for enhanced retrieval capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af8b5863-4267-48ee-9396-cc0968111b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713fe068-a00d-42ad-9394-d3c0f4ccca03",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Step 10: Retrieve Summaries\n",
    " Perform a similarity search on the summaries to get the most relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52de173c-6bb5-478c-b7f5-41373c930703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sub Docs:\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'c7bcae43-1331-4508-8117-3363375e2125'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'doc_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'9c4e0d26-1836-4056-8643-6ea0f58bf49b'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The document provides an overview of LangChain, a platform for interacting with large language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models (LLMs). Here's a summary:\\n\\n**Overview**\\n\\nLangChain is a platform for working with LLMs through a chat </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model interface. It allows users to interact with chat models by sending input messages and receiving output </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">messages.\\n\\n**Key Features**\\n\\n1. **Standard Streaming Interface**: LangChain supports standard streaming </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interfaces, which allow for efficient processing of multiple requests in batches.\\n2. **Async Programming**: Chat </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models support async programming, making it easy to write concurrent code.\\n3. **Optimized Batching**: Many chat </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models have optimized batching capabilities, allowing for more efficient use of resources.\\n\\n**Inputs and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Outputs**\\n\\n1. **Input Messages**: Modern LLMs are typically accessed through a chat model interface that takes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">input messages as input.\\n2. **Output Messages**: The output messages are sent back to the user in response to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">their input messages.\\n\\n**Standard Parameters**\\n\\nSome chat models have standardized parameters, including:\\n\\n1.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">**Temperature**: Many chat models allow users to control the temperature of their outputs.\\n2. **Maximum Number of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Tokens**: Some chat models allow users to specify a maximum number of tokens for their responses.\\n3. **Maximum </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Time to Wait**: Some chat models allow users to specify a maximum time to wait for a response.\\n\\n**Key </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Methods**\\n\\n1. **Invoke**: The primary method for interacting with a chat model, which takes input messages and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">returns output messages.\\n2. **Stream**: A method that allows for streaming the output of a chat model as it is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generated.\\n3. **Batch**: A method that allows for batching multiple requests together for more efficient </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">processing.\\n4. **Bind Tools**: A method that allows users to bind tools to their chat models.\\n5. **With </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Structured Output**: A wrapper around the invoke method for models that natively support structured </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output.\\n\\nOverall, LangChain provides a flexible and efficient way to interact with LLMs through a chat model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interface.\"</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Sub Docs:\n",
       "\n",
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'c7bcae43-1331-4508-8117-3363375e2125'\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'doc_id'\u001b[0m: \u001b[32m'9c4e0d26-1836-4056-8643-6ea0f58bf49b'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m document provides an overview of LangChain, a platform for interacting with large language \u001b[0m\n",
       "\u001b[32mmodels \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Here's a summary:\\n\\n**Overview**\\n\\nLangChain is a platform for working with LLMs through a chat \u001b[0m\n",
       "\u001b[32mmodel interface. It allows users to interact with chat models by sending input messages and receiving output \u001b[0m\n",
       "\u001b[32mmessages.\\n\\n**Key Features**\\n\\n1. **Standard Streaming Interface**: LangChain supports standard streaming \u001b[0m\n",
       "\u001b[32minterfaces, which allow for efficient processing of multiple requests in batches.\\n2. **Async Programming**: Chat \u001b[0m\n",
       "\u001b[32mmodels support async programming, making it easy to write concurrent code.\\n3. **Optimized Batching**: Many chat \u001b[0m\n",
       "\u001b[32mmodels have optimized batching capabilities, allowing for more efficient use of resources.\\n\\n**Inputs and \u001b[0m\n",
       "\u001b[32mOutputs**\\n\\n1. **Input Messages**: Modern LLMs are typically accessed through a chat model interface that takes \u001b[0m\n",
       "\u001b[32minput messages as input.\\n2. **Output Messages**: The output messages are sent back to the user in response to \u001b[0m\n",
       "\u001b[32mtheir input messages.\\n\\n**Standard Parameters**\\n\\nSome chat models have standardized parameters, including:\\n\\n1.\u001b[0m\n",
       "\u001b[32m**Temperature**: Many chat models allow users to control the temperature of their outputs.\\n2. **Maximum Number of \u001b[0m\n",
       "\u001b[32mTokens**: Some chat models allow users to specify a maximum number of tokens for their responses.\\n3. **Maximum \u001b[0m\n",
       "\u001b[32mTime to Wait**: Some chat models allow users to specify a maximum time to wait for a response.\\n\\n**Key \u001b[0m\n",
       "\u001b[32mMethods**\\n\\n1. **Invoke**: The primary method for interacting with a chat model, which takes input messages and \u001b[0m\n",
       "\u001b[32mreturns output messages.\\n2. **Stream**: A method that allows for streaming the output of a chat model as it is \u001b[0m\n",
       "\u001b[32mgenerated.\\n3. **Batch**: A method that allows for batching multiple requests together for more efficient \u001b[0m\n",
       "\u001b[32mprocessing.\\n4. **Bind Tools**: A method that allows users to bind tools to their chat models.\\n5. **With \u001b[0m\n",
       "\u001b[32mStructured Output**: A wrapper around the invoke method for models that natively support structured \u001b[0m\n",
       "\u001b[32moutput.\\n\\nOverall, LangChain provides a flexible and efficient way to interact with LLMs through a chat model \u001b[0m\n",
       "\u001b[32minterface.\"\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrieved Docs:\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'data/langchain.md'</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Title: Chat models | ü¶úÔ∏èüîó LangChain\\n\\nURL Source: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://python.langchain.com/docs/concepts/chat_models/\\n\\nMarkdown </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Content:\\nOverview[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#overview \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Overview\")\\n-------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\n\\nLarge Language Models (LLMs) are advanced machine learning models that excel in a wide range of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language-related tasks such as text generation, translation, summarization, question answering, and more, without </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">needing task-specific fine tuning for every scenario.\\n\\nModern LLMs are typically accessed through a chat model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interface that takes a list of [messages](https://python.langchain.com/docs/concepts/messages/) as input and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">returns a [message](https://python.langchain.com/docs/concepts/messages/) as output.\\n\\nThe newest generation of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">chat models offer additional capabilities:\\n\\n*   [Tool </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">calling](https://python.langchain.com/docs/concepts/tool_calling/): Many popular chat models offer a native [tool </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">calling](https://python.langchain.com/docs/concepts/tool_calling/) API. This API allows developers to build rich </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">used to extract structured information from unstructured data and perform various other tasks.\\n*   [Structured </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output](https://python.langchain.com/docs/concepts/structured_outputs/): A technique to make a chat model respond </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in a structured format, such as JSON that matches a given schema.\\n*   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[Multimodality](https://python.langchain.com/docs/concepts/multimodality/): The ability to work with data other </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">than text; for example, images, audio, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">video.\\n\\nFeatures[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#features \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Features\")\\n-------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\n\\nLangChain provides a consistent interface for working with chat models from different providers while offering </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.\\n\\n*  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">integrations](https://python.langchain.com/docs/integrations/chat/) for an up-to-date list of supported models.\\n* </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Use either LangChain\\'s [messages](https://python.langchain.com/docs/concepts/messages/) format or OpenAI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">format.\\n*   Standard [tool calling API](https://python.langchain.com/docs/concepts/tool_calling/): standard </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interface for binding tools to models, accessing tool call requests made by models, and sending tool results back </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to the model.\\n*   Standard API for [structuring </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">outputs](https://python.langchain.com/docs/concepts/structured_outputs/#structured-output-method) via the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`with_structured_output` method.\\n*   Provides support for [async </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">programming](https://python.langchain.com/docs/concepts/async/), [efficient </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">batching](https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch), [a rich </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">streaming API](https://python.langchain.com/docs/concepts/streaming/).\\n*   Integration with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[LangSmith](https://docs.smith.langchain.com/) for monitoring and debugging production-grade applications based on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLMs.\\n*   Additional features like standardized [token </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">usage](https://python.langchain.com/docs/concepts/messages/#aimessage), [rate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limiting](https://python.langchain.com/docs/concepts/chat_models/#rate-limiting), </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[caching](https://python.langchain.com/docs/concepts/chat_models/#caching) and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">more.\\n\\nIntegrations[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#integrations \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Integrations\")\\n---------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">----------------\\n\\nLangChain has many chat model integrations that allow you to use a wide variety of models from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">different providers.\\n\\nThese integrations are one of two types:\\n\\n1.  **Official models**: These are models that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are officially supported by LangChain and/or model provider. You can find these models in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`langchain-&lt;provider&gt;` packages.\\n2.  **Community models**: There are models that are mostly contributed and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">supported by the community. You can find these models in the `langchain-community` package.\\n\\nLangChain chat </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models are named with a convention that prefixes \"Chat\" to their class names (e.g., `ChatOllama`, `ChatAnthropic`, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`ChatOpenAI`, etc.).\\n\\nPlease review the [chat model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">integrations](https://python.langchain.com/docs/integrations/chat/) for a list of supported </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models.\\n\\nnote\\n\\nModels that do **not** include the prefix \"Chat\" in their name or include \"LLM\" as a suffix in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">their name typically refer to older models that do not follow the chat model interface and instead use an interface</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that takes a string as input and returns a string as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">output.\\n\\nInterface[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#interface \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Interface\")\\n------------------------------------------------------------------------------------------------------</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">----\\n\\nLangChain chat models implement the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">_models.BaseChatModel.html) interface. Because `BaseChatModel` also implements the [Runnable </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Interface](https://python.langchain.com/docs/concepts/runnables/), chat models support a [standard streaming </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interface](https://python.langchain.com/docs/concepts/streaming/), [async </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">programming](https://python.langchain.com/docs/concepts/async/), optimized </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[batching](https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch), and more. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Please see the [Runnable Interface](https://python.langchain.com/docs/concepts/runnables/) for more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">details.\\n\\nMany of the key methods of chat models operate on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[messages](https://python.langchain.com/docs/concepts/messages/) as input and return messages as output.\\n\\nChat </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models offer a standard set of parameters that can be used to configure the model. These parameters are typically </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the response, and the maximum time to wait for a response. Please see the [standard </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parameters](https://python.langchain.com/docs/concepts/chat_models/#standard-parameters) section for more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">details.\\n\\nnote\\n\\nIn documentation, we will often use the terms \"LLM\" and \"Chat Model\" interchangeably. This is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">because most modern LLMs are exposed to users via a chat model interface.\\n\\nHowever, LangChain also has </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">implementations of older LLMs that do not follow the chat model interface and instead use an interface that takes a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">string as input and returns a string as output. These models are typically named without the \"Chat\" prefix (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`Ollama`, `Anthropic`, `OpenAI`, etc.). These models implement the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[BaseLLM](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.BaseL</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LM.html#langchain_core.language_models.llms.BaseLLM) interface and may be named with the \"LLM\" suffix (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">`OllamaLLM`, `AnthropicLLM`, `OpenAILLM`, etc.). Generally, users should not use these models.\\n\\n### Key </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">methods[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#key-methods \"Direct link to Key </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">methods\")\\n\\nThe key methods of a chat model are:\\n\\n1.  **invoke**: The primary method for interacting with a chat</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model. It takes a list of [messages](https://python.langchain.com/docs/concepts/messages/) as input and returns a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">list of messages as output.\\n2.  **stream**: A method that allows you to stream the output of a chat model as it is</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generated.\\n3.  **batch**: A method that allows you to batch multiple requests to a chat model together for more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">efficient processing.\\n4.  **bind\\\\_tools**: A method that allows you to bind a tool to a chat model for use in the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model\\'s execution context.\\n5.  **with\\\\_structured\\\\_output**: A wrapper around the `invoke` method for models </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that natively support [structured output](https://python.langchain.com/docs/concepts/structured_outputs/).\\n\\nOther</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">important methods can be found in the [BaseChatModel API </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Reference](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_mode</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ls.BaseChatModel.html).\\n\\n### Inputs and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">outputs[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#inputs-and-outputs \"Direct link to Inputs </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and outputs\")\\n\\nModern LLMs are typically accessed through a chat model interface that takes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[messages](https://python.langchain.com/docs/concepts/messages/) as input and returns </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[messages](https://python.langchain.com/docs/concepts/messages/) as output. Messages are typically associated with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a role (e.g., \"system\", \"human\", \"assistant\") and one or more content blocks that contain text or potentially </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multimodal data (e.g., images, audio, video).\\n\\nLangChain supports two message formats to interact with chat </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models:\\n\\n1.  **LangChain Message Format**: LangChain\\'s own message format, which is used by default and is used </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">internally by LangChain.\\n2.  **OpenAI\\'s Message Format**: OpenAI\\'s message format.\\n\\n### Standard </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parameters[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#standard-parameters \"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Standard parameters\")\\n\\nMany chat models have standardized parameters that can be used to configure the model:'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrieved Docs:\n",
       "\n",
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'data/langchain.md'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'Title: Chat models | ü¶úÔ∏èüîó LangChain\\n\\nURL Source: \u001b[0m\n",
       "\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/\\n\\nMarkdown \u001b[0m\n",
       "\u001b[32mContent:\\nOverview\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#overview \"Direct link to \u001b[0m\n",
       "\u001b[32mOverview\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n-------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m\\n\\nLarge Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are advanced machine learning models that excel in a wide range of \u001b[0m\n",
       "\u001b[32mlanguage-related tasks such as text generation, translation, summarization, question answering, and more, without \u001b[0m\n",
       "\u001b[32mneeding task-specific fine tuning for every scenario.\\n\\nModern LLMs are typically accessed through a chat model \u001b[0m\n",
       "\u001b[32minterface that takes a list of \u001b[0m\u001b[32m[\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as input and \u001b[0m\n",
       "\u001b[32mreturns a \u001b[0m\u001b[32m[\u001b[0m\u001b[32mmessage\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output.\\n\\nThe newest generation of \u001b[0m\n",
       "\u001b[32mchat models offer additional capabilities:\\n\\n*   \u001b[0m\u001b[32m[\u001b[0m\u001b[32mTool \u001b[0m\n",
       "\u001b[32mcalling\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/tool_calling/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: Many popular chat models offer a native \u001b[0m\u001b[32m[\u001b[0m\u001b[32mtool \u001b[0m\n",
       "\u001b[32mcalling\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/tool_calling/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m API. This API allows developers to build rich \u001b[0m\n",
       "\u001b[32mapplications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be \u001b[0m\n",
       "\u001b[32mused to extract structured information from unstructured data and perform various other tasks.\\n*   \u001b[0m\u001b[32m[\u001b[0m\u001b[32mStructured \u001b[0m\n",
       "\u001b[32moutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/structured_outputs/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: A technique to make a chat model respond \u001b[0m\n",
       "\u001b[32min a structured format, such as JSON that matches a given schema.\\n*   \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mMultimodality\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/multimodality/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: The ability to work with data other \u001b[0m\n",
       "\u001b[32mthan text; for example, images, audio, and \u001b[0m\n",
       "\u001b[32mvideo.\\n\\nFeatures\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#features \"Direct link to \u001b[0m\n",
       "\u001b[32mFeatures\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n-------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m\\n\\nLangChain provides a consistent interface for working with chat models from different providers while offering \u001b[0m\n",
       "\u001b[32madditional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.\\n\\n*  \u001b[0m\n",
       "\u001b[32mIntegrations with many chat model providers \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, \u001b[0m\n",
       "\u001b[32mAmazon Bedrock, Hugging Face, Cohere, Groq\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Please see \u001b[0m\u001b[32m[\u001b[0m\u001b[32mchat model \u001b[0m\n",
       "\u001b[32mintegrations\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/integrations/chat/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for an up-to-date list of supported models.\\n* \u001b[0m\n",
       "\u001b[32mUse either LangChain\\'s \u001b[0m\u001b[32m[\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m format or OpenAI \u001b[0m\n",
       "\u001b[32mformat.\\n*   Standard \u001b[0m\u001b[32m[\u001b[0m\u001b[32mtool calling API\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/tool_calling/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: standard \u001b[0m\n",
       "\u001b[32minterface for binding tools to models, accessing tool call requests made by models, and sending tool results back \u001b[0m\n",
       "\u001b[32mto the model.\\n*   Standard API for \u001b[0m\u001b[32m[\u001b[0m\u001b[32mstructuring \u001b[0m\n",
       "\u001b[32moutputs\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/structured_outputs/#structured-output-method\u001b[0m\u001b[32m)\u001b[0m\u001b[32m via the \u001b[0m\n",
       "\u001b[32m`with_structured_output` method.\\n*   Provides support for \u001b[0m\u001b[32m[\u001b[0m\u001b[32masync \u001b[0m\n",
       "\u001b[32mprogramming\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/async/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\u001b[32m[\u001b[0m\u001b[32mefficient \u001b[0m\n",
       "\u001b[32mbatching\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\u001b[32m[\u001b[0m\u001b[32ma rich \u001b[0m\n",
       "\u001b[32mstreaming API\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/streaming/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n*   Integration with \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mLangSmith\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://docs.smith.langchain.com/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for monitoring and debugging production-grade applications based on \u001b[0m\n",
       "\u001b[32mLLMs.\\n*   Additional features like standardized \u001b[0m\u001b[32m[\u001b[0m\u001b[32mtoken \u001b[0m\n",
       "\u001b[32musage\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/#aimessage\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\u001b[32m[\u001b[0m\u001b[32mrate \u001b[0m\n",
       "\u001b[32mlimiting\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#rate-limiting\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mcaching\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#caching\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and \u001b[0m\n",
       "\u001b[32mmore.\\n\\nIntegrations\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#integrations \"Direct link to \u001b[0m\n",
       "\u001b[32mIntegrations\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n---------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m----------------\\n\\nLangChain has many chat model integrations that allow you to use a wide variety of models from \u001b[0m\n",
       "\u001b[32mdifferent providers.\\n\\nThese integrations are one of two types:\\n\\n1.  **Official models**: These are models that \u001b[0m\n",
       "\u001b[32mare officially supported by LangChain and/or model provider. You can find these models in the \u001b[0m\n",
       "\u001b[32m`langchain-\u001b[0m\u001b[32m<\u001b[0m\u001b[32mprovider\u001b[0m\u001b[32m>\u001b[0m\u001b[32m` packages.\\n2.  **Community models**: There are models that are mostly contributed and \u001b[0m\n",
       "\u001b[32msupported by the community. You can find these models in the `langchain-community` package.\\n\\nLangChain chat \u001b[0m\n",
       "\u001b[32mmodels are named with a convention that prefixes \"Chat\" to their class names \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., `ChatOllama`, `ChatAnthropic`, \u001b[0m\n",
       "\u001b[32m`ChatOpenAI`, etc.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nPlease review the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mchat model \u001b[0m\n",
       "\u001b[32mintegrations\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/integrations/chat/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for a list of supported \u001b[0m\n",
       "\u001b[32mmodels.\\n\\nnote\\n\\nModels that do **not** include the prefix \"Chat\" in their name or include \"LLM\" as a suffix in \u001b[0m\n",
       "\u001b[32mtheir name typically refer to older models that do not follow the chat model interface and instead use an interface\u001b[0m\n",
       "\u001b[32mthat takes a string as input and returns a string as \u001b[0m\n",
       "\u001b[32moutput.\\n\\nInterface\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#interface \"Direct link to \u001b[0m\n",
       "\u001b[32mInterface\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n------------------------------------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[32m----\\n\\nLangChain chat models implement the \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mBaseChatModel\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat\u001b[0m\n",
       "\u001b[32m_models.BaseChatModel.html\u001b[0m\u001b[32m)\u001b[0m\u001b[32m interface. Because `BaseChatModel` also implements the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mRunnable \u001b[0m\n",
       "\u001b[32mInterface\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/runnables/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, chat models support a \u001b[0m\u001b[32m[\u001b[0m\u001b[32mstandard streaming \u001b[0m\n",
       "\u001b[32minterface\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/streaming/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, \u001b[0m\u001b[32m[\u001b[0m\u001b[32masync \u001b[0m\n",
       "\u001b[32mprogramming\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/async/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, optimized \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mbatching\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and more. \u001b[0m\n",
       "\u001b[32mPlease see the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mRunnable Interface\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/runnables/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for more \u001b[0m\n",
       "\u001b[32mdetails.\\n\\nMany of the key methods of chat models operate on \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as input and return messages as output.\\n\\nChat \u001b[0m\n",
       "\u001b[32mmodels offer a standard set of parameters that can be used to configure the model. These parameters are typically \u001b[0m\n",
       "\u001b[32mused to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in \u001b[0m\n",
       "\u001b[32mthe response, and the maximum time to wait for a response. Please see the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mstandard \u001b[0m\n",
       "\u001b[32mparameters\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#standard-parameters\u001b[0m\u001b[32m)\u001b[0m\u001b[32m section for more \u001b[0m\n",
       "\u001b[32mdetails.\\n\\nnote\\n\\nIn documentation, we will often use the terms \"LLM\" and \"Chat Model\" interchangeably. This is \u001b[0m\n",
       "\u001b[32mbecause most modern LLMs are exposed to users via a chat model interface.\\n\\nHowever, LangChain also has \u001b[0m\n",
       "\u001b[32mimplementations of older LLMs that do not follow the chat model interface and instead use an interface that takes a\u001b[0m\n",
       "\u001b[32mstring as input and returns a string as output. These models are typically named without the \"Chat\" prefix \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32m`Ollama`, `Anthropic`, `OpenAI`, etc.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. These models implement the \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mBaseLLM\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.BaseL\u001b[0m\n",
       "\u001b[32mLM.html#langchain_core.language_models.llms.BaseLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m interface and may be named with the \"LLM\" suffix \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \u001b[0m\n",
       "\u001b[32m`OllamaLLM`, `AnthropicLLM`, `OpenAILLM`, etc.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Generally, users should not use these models.\\n\\n### Key \u001b[0m\n",
       "\u001b[32mmethods\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#key-methods \"Direct link to Key \u001b[0m\n",
       "\u001b[32mmethods\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nThe key methods of a chat model are:\\n\\n1.  **invoke**: The primary method for interacting with a chat\u001b[0m\n",
       "\u001b[32mmodel. It takes a list of \u001b[0m\u001b[32m[\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as input and returns a \u001b[0m\n",
       "\u001b[32mlist of messages as output.\\n2.  **stream**: A method that allows you to stream the output of a chat model as it is\u001b[0m\n",
       "\u001b[32mgenerated.\\n3.  **batch**: A method that allows you to batch multiple requests to a chat model together for more \u001b[0m\n",
       "\u001b[32mefficient processing.\\n4.  **bind\\\\_tools**: A method that allows you to bind a tool to a chat model for use in the\u001b[0m\n",
       "\u001b[32mmodel\\'s execution context.\\n5.  **with\\\\_structured\\\\_output**: A wrapper around the `invoke` method for models \u001b[0m\n",
       "\u001b[32mthat natively support \u001b[0m\u001b[32m[\u001b[0m\u001b[32mstructured output\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/structured_outputs/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nOther\u001b[0m\n",
       "\u001b[32mimportant methods can be found in the \u001b[0m\u001b[32m[\u001b[0m\u001b[32mBaseChatModel API \u001b[0m\n",
       "\u001b[32mReference\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_mode\u001b[0m\n",
       "\u001b[32mls.BaseChatModel.html\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n### Inputs and \u001b[0m\n",
       "\u001b[32moutputs\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#inputs-and-outputs \"Direct link to Inputs \u001b[0m\n",
       "\u001b[32mand outputs\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nModern LLMs are typically accessed through a chat model interface that takes \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as input and returns \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32mmessages\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as output. Messages are typically associated with \u001b[0m\n",
       "\u001b[32ma role \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., \"system\", \"human\", \"assistant\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and one or more content blocks that contain text or potentially \u001b[0m\n",
       "\u001b[32mmultimodal data \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., images, audio, video\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nLangChain supports two message formats to interact with chat \u001b[0m\n",
       "\u001b[32mmodels:\\n\\n1.  **LangChain Message Format**: LangChain\\'s own message format, which is used by default and is used \u001b[0m\n",
       "\u001b[32minternally by LangChain.\\n2.  **OpenAI\\'s Message Format**: OpenAI\\'s message format.\\n\\n### Standard \u001b[0m\n",
       "\u001b[32mparameters\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\u200b\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://python.langchain.com/docs/concepts/chat_models/#standard-parameters \"Direct link to \u001b[0m\n",
       "\u001b[32mStandard parameters\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nMany chat models have standardized parameters that can be used to configure the model:'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub_docs = retriever.vectorstore.similarity_search(\"LangChain\")\n",
    "print(\"Sub Docs:\\n\",sub_docs[0])\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"LangChain\")\n",
    "print(\"Retrieved Docs:\\n\",retrieved_docs[0])\n",
    "# len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Title: Chat models | ü¶úÔ∏èüîó LangChain\n",
       "\n",
       "URL Source: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/chat_models/</span>\n",
       "\n",
       "Markdown Content:\n",
       "Overview<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/chat_models/#overview</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to Overview\"</span><span style=\"font-weight: bold\">)</span>\n",
       "-------------------------------------------------------------------------------------------------------\n",
       "\n",
       "Large Language Models <span style=\"font-weight: bold\">(</span>LLMs<span style=\"font-weight: bold\">)</span> are advanced machine learning models that excel in a wide range of language-related \n",
       "tasks such as text generation, translation, summarization, question answering, and more, without needing \n",
       "task-specific fine tuning for every scenario.\n",
       "\n",
       "Modern LLMs are typically accessed through a chat model interface that takes a list of \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/messages/)</span> as input and returns a \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/messages/)</span> as output.\n",
       "\n",
       "The newest generation of chat models offer additional capabilities:\n",
       "\n",
       "*   <span style=\"font-weight: bold\">[</span>Tool calling<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/tool_calling/):</span> Many popular chat models offer a \n",
       "native <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/tool_calling/)</span> API. This API allows developers to build rich \n",
       "applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be \n",
       "used to extract structured information from unstructured data and perform various other tasks.\n",
       "*   <span style=\"font-weight: bold\">[</span>Structured output<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/structured_outputs/):</span> A technique to make a chat\n",
       "model respond in a structured format, such as JSON that matches a given schema.\n",
       "*   <span style=\"font-weight: bold\">[</span>Multimodality<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/multimodality/):</span> The ability to work with data other\n",
       "than text; for example, images, audio, and video.\n",
       "\n",
       "Features<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/chat_models/#features</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to Features\"</span><span style=\"font-weight: bold\">)</span>\n",
       "-------------------------------------------------------------------------------------------------------\n",
       "\n",
       "LangChain provides a consistent interface for working with chat models from different providers while offering \n",
       "additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.\n",
       "\n",
       "*   Integrations with many chat model providers <span style=\"font-weight: bold\">(</span>e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, \n",
       "Amazon Bedrock, Hugging Face, Cohere, Groq<span style=\"font-weight: bold\">)</span>. Please see <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/integrations/chat/)</span> for \n",
       "an up-to-date list of supported models.\n",
       "*   Use either LangChain's <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/messages/)</span> format or OpenAI format.\n",
       "*   Standard <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/tool_calling/):</span> standard interface for binding tools to \n",
       "models, accessing tool call requests made by models, and sending tool results back to the model.\n",
       "*   Standard API for <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/structured_outputs/#structured-output-method)</span> via \n",
       "the `with_structured_output` method.\n",
       "*   Provides support for <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/async/),</span> \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch),</span> \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/streaming/).</span>\n",
       "*   Integration with <span style=\"font-weight: bold\">[</span>LangSmith<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.smith.langchain.com/)</span> for monitoring and debugging production-grade \n",
       "applications based on LLMs.\n",
       "*   Additional features like standardized <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/messages/#aimessage),</span> \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/chat_models/#rate-limiting),</span> \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/chat_models/#caching)</span> and more.\n",
       "\n",
       "Integrations<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/chat_models/#integrations</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to Integrations\"</span><span style=\"font-weight: bold\">)</span>\n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "\n",
       "LangChain has many chat model integrations that allow you to use a wide variety of models from different providers.\n",
       "\n",
       "These integrations are one of two types:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.  **Official models**: These are models that are officially supported by LangChain and/or model provider. You can\n",
       "find these models in the `langchain-<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">provider</span><span style=\"font-weight: bold\">&gt;</span>` packages.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>.  **Community models**: There are models that are mostly contributed and supported by the community. You can find\n",
       "these models in the `langchain-community` package.\n",
       "\n",
       "LangChain chat models are named with a convention that prefixes <span style=\"color: #008000; text-decoration-color: #008000\">\"Chat\"</span> to their class names <span style=\"font-weight: bold\">(</span>e.g., `ChatOllama`, \n",
       "`ChatAnthropic`, `ChatOpenAI`, etc.<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "Please review the <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/integrations/chat/)</span> for a list of supported models.\n",
       "\n",
       "note\n",
       "\n",
       "Models that do **not** include the prefix <span style=\"color: #008000; text-decoration-color: #008000\">\"Chat\"</span> in their name or include <span style=\"color: #008000; text-decoration-color: #008000\">\"LLM\"</span> as a suffix in their name typically\n",
       "refer to older models that do not follow the chat model interface and instead use an interface that takes a string \n",
       "as input and returns a string as output.\n",
       "\n",
       "Interface<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/chat_models/#interface</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to Interface\"</span><span style=\"font-weight: bold\">)</span>\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "\n",
       "LangChain chat models implement the \n",
       "<span style=\"font-weight: bold\">[</span>BaseChatModel<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">_models.BaseChatModel.html)</span> interface. Because `BaseChatModel` also implements the <span style=\"font-weight: bold\">[</span>Runnable \n",
       "Interface<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/runnables/),</span> chat models support a \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/streaming/),</span> <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/async/),</span> \n",
       "optimized <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch),</span> and more. \n",
       "Please see the <span style=\"font-weight: bold\">[</span>Runnable Interface<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/runnables/)</span> for more details.\n",
       "\n",
       "Many of the key methods of chat models operate on <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/messages/)</span> as input \n",
       "and return messages as output.\n",
       "\n",
       "Chat models offer a standard set of parameters that can be used to configure the model. These parameters are \n",
       "typically used to control the behavior of the model, such as the temperature of the output, the maximum number of \n",
       "tokens in the response, and the maximum time to wait for a response. Please see the \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/chat_models/#standard-parameters)</span> section for more details.\n",
       "\n",
       "note\n",
       "\n",
       "In documentation, we will often use the terms <span style=\"color: #008000; text-decoration-color: #008000\">\"LLM\"</span> and <span style=\"color: #008000; text-decoration-color: #008000\">\"Chat Model\"</span> interchangeably. This is because most modern \n",
       "LLMs are exposed to users via a chat model interface.\n",
       "\n",
       "However, LangChain also has implementations of older LLMs that do not follow the chat model interface and instead \n",
       "use an interface that takes a string as input and returns a string as output. These models are typically named \n",
       "without the <span style=\"color: #008000; text-decoration-color: #008000\">\"Chat\"</span> prefix <span style=\"font-weight: bold\">(</span>e.g., `Ollama`, `Anthropic`, `OpenAI`, etc.<span style=\"font-weight: bold\">)</span>. These models implement the \n",
       "<span style=\"font-weight: bold\">[</span>BaseLLM<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.BaseL</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">LM.html#langchain_core.language_models.llms.BaseLLM)</span> interface and may be named with the <span style=\"color: #008000; text-decoration-color: #008000\">\"LLM\"</span> suffix <span style=\"font-weight: bold\">(</span>e.g., \n",
       "`OllamaLLM`, `AnthropicLLM`, `OpenAILLM`, etc.<span style=\"font-weight: bold\">)</span>. Generally, users should not use these models.\n",
       "\n",
       "### Key methods<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/chat_models/#key-methods</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to Key methods\"</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "The key methods of a chat model are:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.  **invoke**: The primary method for interacting with a chat model. It takes a list of \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/messages/)</span> as input and returns a list of messages as output.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>.  **stream**: A method that allows you to stream the output of a chat model as it is generated.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>.  **batch**: A method that allows you to batch multiple requests to a chat model together for more efficient \n",
       "processing.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>.  **bind\\_tools**: A method that allows you to bind a tool to a chat model for use in the model's execution \n",
       "context.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>.  **with\\_structured\\_output**: A wrapper around the `invoke` method for models that natively support \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/structured_outputs/).</span>\n",
       "\n",
       "Other important methods can be found in the <span style=\"font-weight: bold\">[</span>BaseChatModel API \n",
       "Reference<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_mode</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">ls.BaseChatModel.html).</span>\n",
       "\n",
       "### Inputs and outputs<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/chat_models/#inputs-and-outputs</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Inputs and outputs\"</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "Modern LLMs are typically accessed through a chat model interface that takes \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/messages/)</span> as input and returns \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/messages/)</span> as output. Messages are typically associated with a role \n",
       "<span style=\"font-weight: bold\">(</span>e.g., <span style=\"color: #008000; text-decoration-color: #008000\">\"system\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"human\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"assistant\"</span><span style=\"font-weight: bold\">)</span> and one or more content blocks that contain text or potentially multimodal \n",
       "data <span style=\"font-weight: bold\">(</span>e.g., images, audio, video<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "LangChain supports two message formats to interact with chat models:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.  **LangChain Message Format**: LangChain's own message format, which is used by default and is used internally \n",
       "by LangChain.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>.  **OpenAI's Message Format**: OpenAI's message format.\n",
       "\n",
       "### Standard parameters<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/chat_models/#standard-parameters</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to Standard parameters\"</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "Many chat models have standardized parameters that can be used to configure the model:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Title: Chat models | ü¶úÔ∏èüîó LangChain\n",
       "\n",
       "URL Source: \u001b[4;94mhttps://python.langchain.com/docs/concepts/chat_models/\u001b[0m\n",
       "\n",
       "Markdown Content:\n",
       "Overview\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/chat_models/#overview\u001b[0m \u001b[32m\"Direct link to Overview\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "-------------------------------------------------------------------------------------------------------\n",
       "\n",
       "Large Language Models \u001b[1m(\u001b[0mLLMs\u001b[1m)\u001b[0m are advanced machine learning models that excel in a wide range of language-related \n",
       "tasks such as text generation, translation, summarization, question answering, and more, without needing \n",
       "task-specific fine tuning for every scenario.\n",
       "\n",
       "Modern LLMs are typically accessed through a chat model interface that takes a list of \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[4;94m)\u001b[0m as input and returns a \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[4;94m)\u001b[0m as output.\n",
       "\n",
       "The newest generation of chat models offer additional capabilities:\n",
       "\n",
       "*   \u001b[1m[\u001b[0mTool calling\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/tool_calling/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m:\u001b[0m Many popular chat models offer a \n",
       "native \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/tool_calling/\u001b[0m\u001b[4;94m)\u001b[0m API. This API allows developers to build rich \n",
       "applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be \n",
       "used to extract structured information from unstructured data and perform various other tasks.\n",
       "*   \u001b[1m[\u001b[0mStructured output\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/structured_outputs/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m:\u001b[0m A technique to make a chat\n",
       "model respond in a structured format, such as JSON that matches a given schema.\n",
       "*   \u001b[1m[\u001b[0mMultimodality\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/multimodality/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m:\u001b[0m The ability to work with data other\n",
       "than text; for example, images, audio, and video.\n",
       "\n",
       "Features\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/chat_models/#features\u001b[0m \u001b[32m\"Direct link to Features\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "-------------------------------------------------------------------------------------------------------\n",
       "\n",
       "LangChain provides a consistent interface for working with chat models from different providers while offering \n",
       "additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.\n",
       "\n",
       "*   Integrations with many chat model providers \u001b[1m(\u001b[0me.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, \n",
       "Amazon Bedrock, Hugging Face, Cohere, Groq\u001b[1m)\u001b[0m. Please see \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/integrations/chat/\u001b[0m\u001b[4;94m)\u001b[0m for \n",
       "an up-to-date list of supported models.\n",
       "*   Use either LangChain's \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[4;94m)\u001b[0m format or OpenAI format.\n",
       "*   Standard \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/tool_calling/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m:\u001b[0m standard interface for binding tools to \n",
       "models, accessing tool call requests made by models, and sending tool results back to the model.\n",
       "*   Standard API for \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/structured_outputs/#structured-output-method\u001b[0m\u001b[4;94m)\u001b[0m via \n",
       "the `with_structured_output` method.\n",
       "*   Provides support for \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/async/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/streaming/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "*   Integration with \u001b[1m[\u001b[0mLangSmith\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://docs.smith.langchain.com/\u001b[0m\u001b[4;94m)\u001b[0m for monitoring and debugging production-grade \n",
       "applications based on LLMs.\n",
       "*   Additional features like standardized \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/messages/#aimessage\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/chat_models/#rate-limiting\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/chat_models/#caching\u001b[0m\u001b[4;94m)\u001b[0m and more.\n",
       "\n",
       "Integrations\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/chat_models/#integrations\u001b[0m \u001b[32m\"Direct link to Integrations\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "\n",
       "LangChain has many chat model integrations that allow you to use a wide variety of models from different providers.\n",
       "\n",
       "These integrations are one of two types:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m.  **Official models**: These are models that are officially supported by LangChain and/or model provider. You can\n",
       "find these models in the `langchain-\u001b[1m<\u001b[0m\u001b[1;95mprovider\u001b[0m\u001b[1m>\u001b[0m` packages.\n",
       "\u001b[1;36m2\u001b[0m.  **Community models**: There are models that are mostly contributed and supported by the community. You can find\n",
       "these models in the `langchain-community` package.\n",
       "\n",
       "LangChain chat models are named with a convention that prefixes \u001b[32m\"Chat\"\u001b[0m to their class names \u001b[1m(\u001b[0me.g., `ChatOllama`, \n",
       "`ChatAnthropic`, `ChatOpenAI`, etc.\u001b[1m)\u001b[0m.\n",
       "\n",
       "Please review the \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/integrations/chat/\u001b[0m\u001b[4;94m)\u001b[0m for a list of supported models.\n",
       "\n",
       "note\n",
       "\n",
       "Models that do **not** include the prefix \u001b[32m\"Chat\"\u001b[0m in their name or include \u001b[32m\"LLM\"\u001b[0m as a suffix in their name typically\n",
       "refer to older models that do not follow the chat model interface and instead use an interface that takes a string \n",
       "as input and returns a string as output.\n",
       "\n",
       "Interface\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/chat_models/#interface\u001b[0m \u001b[32m\"Direct link to Interface\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "\n",
       "LangChain chat models implement the \n",
       "\u001b[1m[\u001b[0mBaseChatModel\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat\u001b[0m\n",
       "\u001b[4;94m_models.BaseChatModel.html\u001b[0m\u001b[4;94m)\u001b[0m interface. Because `BaseChatModel` also implements the \u001b[1m[\u001b[0mRunnable \n",
       "Interface\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/runnables/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m chat models support a \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/streaming/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/async/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m \n",
       "optimized \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m and more. \n",
       "Please see the \u001b[1m[\u001b[0mRunnable Interface\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/runnables/\u001b[0m\u001b[4;94m)\u001b[0m for more details.\n",
       "\n",
       "Many of the key methods of chat models operate on \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[4;94m)\u001b[0m as input \n",
       "and return messages as output.\n",
       "\n",
       "Chat models offer a standard set of parameters that can be used to configure the model. These parameters are \n",
       "typically used to control the behavior of the model, such as the temperature of the output, the maximum number of \n",
       "tokens in the response, and the maximum time to wait for a response. Please see the \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/chat_models/#standard-parameters\u001b[0m\u001b[4;94m)\u001b[0m section for more details.\n",
       "\n",
       "note\n",
       "\n",
       "In documentation, we will often use the terms \u001b[32m\"LLM\"\u001b[0m and \u001b[32m\"Chat Model\"\u001b[0m interchangeably. This is because most modern \n",
       "LLMs are exposed to users via a chat model interface.\n",
       "\n",
       "However, LangChain also has implementations of older LLMs that do not follow the chat model interface and instead \n",
       "use an interface that takes a string as input and returns a string as output. These models are typically named \n",
       "without the \u001b[32m\"Chat\"\u001b[0m prefix \u001b[1m(\u001b[0me.g., `Ollama`, `Anthropic`, `OpenAI`, etc.\u001b[1m)\u001b[0m. These models implement the \n",
       "\u001b[1m[\u001b[0mBaseLLM\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.BaseL\u001b[0m\n",
       "\u001b[4;94mLM.html#langchain_core.language_models.llms.BaseLLM\u001b[0m\u001b[4;94m)\u001b[0m interface and may be named with the \u001b[32m\"LLM\"\u001b[0m suffix \u001b[1m(\u001b[0me.g., \n",
       "`OllamaLLM`, `AnthropicLLM`, `OpenAILLM`, etc.\u001b[1m)\u001b[0m. Generally, users should not use these models.\n",
       "\n",
       "### Key methods\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/chat_models/#key-methods\u001b[0m \u001b[32m\"Direct link to Key methods\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "The key methods of a chat model are:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m.  **invoke**: The primary method for interacting with a chat model. It takes a list of \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[4;94m)\u001b[0m as input and returns a list of messages as output.\n",
       "\u001b[1;36m2\u001b[0m.  **stream**: A method that allows you to stream the output of a chat model as it is generated.\n",
       "\u001b[1;36m3\u001b[0m.  **batch**: A method that allows you to batch multiple requests to a chat model together for more efficient \n",
       "processing.\n",
       "\u001b[1;36m4\u001b[0m.  **bind\\_tools**: A method that allows you to bind a tool to a chat model for use in the model's execution \n",
       "context.\n",
       "\u001b[1;36m5\u001b[0m.  **with\\_structured\\_output**: A wrapper around the `invoke` method for models that natively support \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/structured_outputs/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "Other important methods can be found in the \u001b[1m[\u001b[0mBaseChatModel API \n",
       "Reference\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_mode\u001b[0m\n",
       "\u001b[4;94mls.BaseChatModel.html\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "### Inputs and outputs\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/chat_models/#inputs-and-outputs\u001b[0m \u001b[32m\"Direct link to\u001b[0m\n",
       "\u001b[32mInputs and outputs\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "Modern LLMs are typically accessed through a chat model interface that takes \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[4;94m)\u001b[0m as input and returns \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[4;94m)\u001b[0m as output. Messages are typically associated with a role \n",
       "\u001b[1m(\u001b[0me.g., \u001b[32m\"system\"\u001b[0m, \u001b[32m\"human\"\u001b[0m, \u001b[32m\"assistant\"\u001b[0m\u001b[1m)\u001b[0m and one or more content blocks that contain text or potentially multimodal \n",
       "data \u001b[1m(\u001b[0me.g., images, audio, video\u001b[1m)\u001b[0m.\n",
       "\n",
       "LangChain supports two message formats to interact with chat models:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m.  **LangChain Message Format**: LangChain's own message format, which is used by default and is used internally \n",
       "by LangChain.\n",
       "\u001b[1;36m2\u001b[0m.  **OpenAI's Message Format**: OpenAI's message format.\n",
       "\n",
       "### Standard parameters\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/chat_models/#standard-parameters\u001b[0m \u001b[32m\"Direct link\u001b[0m\n",
       "\u001b[32mto Standard parameters\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "Many chat models have standardized parameters that can be used to configure the model:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Vector store support for metadata filtering is typically dependent on the underlying vector store implementation.\n",
       "\n",
       "Here is example usage with \n",
       "<span style=\"font-weight: bold\">[</span>Pinecone<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/integrations/vectorstores/pinecone/#query-directly),</span> showing that we \n",
       "filter for all documents that have the metadata key `source` with value `tweet`.\n",
       "\n",
       "```\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">vectorstore.similarity_search</span><span style=\"font-weight: bold\">(</span>    <span style=\"color: #008000; text-decoration-color: #008000\">\"LangChain provides abstractions to make working with LLMs easy\"</span>,    <span style=\"color: #808000; text-decoration-color: #808000\">k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">filter</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"source\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"tweet\"</span><span style=\"font-weight: bold\">}</span>,<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "Advanced search and retrieval \n",
       "techniques<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/vectorstores/#advanced-search-and-retrieval-techniques</span> \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to Advanced search and retrieval techniques\"</span><span style=\"font-weight: bold\">)</span>\n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "-------------------------------------------------------------------------------------\n",
       "\n",
       "While algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional \n",
       "techniques can be employed to improve search quality and diversity. For example, \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/)</span> is a re-ranking algorithm \n",
       "used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set\n",
       "of results. As a second example, some \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/integrations/retrievers/pinecone_hybrid_search/)</span> offer built-in \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.pinecone.io/guides/data/understanding-hybrid-search)</span> to combine keyword and semantic similarity \n",
       "search, which marries the benefits of both approaches. At the moment, there is no unified way to perform hybrid \n",
       "search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with \n",
       "`similarity_search`. See this <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/how_to/hybrid/)</span> for more details.\n",
       "\n",
       "| Name                                                                                                             \n",
       "| When to use                                           | Description                                              \n",
       "|\n",
       "| \n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "--------- | ----------------------------------------------------- | \n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "------------------------- |\n",
       "| <span style=\"font-weight: bold\">[</span>Hybrid search<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/integrations/retrievers/pinecone_hybrid_search/)</span>               \n",
       "| When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, \n",
       "marrying the benefits of both approaches. <span style=\"font-weight: bold\">[</span>Paper<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/abs/2210.11934).</span> |\n",
       "| <span style=\"font-weight: bold\">[</span>Maximal Marginal Relevance \n",
       "<span style=\"font-weight: bold\">(</span>MMR<span style=\"font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVec</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">torStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.max_marginal_relevance_search)</span> | When needing to \n",
       "diversify search results.             | MMR attempts to diversify the results of a search to avoid returning \n",
       "similar and redundant documents.                                        |\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Vector store support for metadata filtering is typically dependent on the underlying vector store implementation.\n",
       "\n",
       "Here is example usage with \n",
       "\u001b[1m[\u001b[0mPinecone\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/integrations/vectorstores/pinecone/#query-directly\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m showing that we \n",
       "filter for all documents that have the metadata key `source` with value `tweet`.\n",
       "\n",
       "```\n",
       "\u001b[1;35mvectorstore.similarity_search\u001b[0m\u001b[1m(\u001b[0m    \u001b[32m\"LangChain provides abstractions to make working with LLMs easy\"\u001b[0m,    \u001b[33mk\u001b[0m=\u001b[1;36m2\u001b[0m,    \n",
       "\u001b[33mfilter\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m\"source\"\u001b[0m: \u001b[32m\"tweet\"\u001b[0m\u001b[1m}\u001b[0m,\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "Advanced search and retrieval \n",
       "techniques\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/vectorstores/#advanced-search-and-retrieval-techniques\u001b[0m \n",
       "\u001b[32m\"Direct link to Advanced search and retrieval techniques\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "-------------------------------------------------------------------------------------\n",
       "\n",
       "While algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional \n",
       "techniques can be employed to improve search quality and diversity. For example, \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/\u001b[0m\u001b[4;94m)\u001b[0m is a re-ranking algorithm \n",
       "used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set\n",
       "of results. As a second example, some \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/integrations/retrievers/pinecone_hybrid_search/\u001b[0m\u001b[4;94m)\u001b[0m offer built-in \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://docs.pinecone.io/guides/data/understanding-hybrid-search\u001b[0m\u001b[4;94m)\u001b[0m to combine keyword and semantic similarity \n",
       "search, which marries the benefits of both approaches. At the moment, there is no unified way to perform hybrid \n",
       "search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with \n",
       "`similarity_search`. See this \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/how_to/hybrid/\u001b[0m\u001b[4;94m)\u001b[0m for more details.\n",
       "\n",
       "| Name                                                                                                             \n",
       "| When to use                                           | Description                                              \n",
       "|\n",
       "| \n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "--------- | ----------------------------------------------------- | \n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "------------------------- |\n",
       "| \u001b[1m[\u001b[0mHybrid search\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/integrations/retrievers/pinecone_hybrid_search/\u001b[0m\u001b[4;94m)\u001b[0m               \n",
       "| When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, \n",
       "marrying the benefits of both approaches. \u001b[1m[\u001b[0mPaper\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://arxiv.org/abs/2210.11934\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m |\n",
       "| \u001b[1m[\u001b[0mMaximal Marginal Relevance \n",
       "\u001b[1m(\u001b[0mMMR\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVec\u001b[0m\n",
       "\u001b[4;94mtorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.max_marginal_relevance_search\u001b[0m\u001b[4;94m)\u001b[0m | When needing to \n",
       "diversify search results.             | MMR attempts to diversify the results of a search to avoid returning \n",
       "similar and redundant documents.                                        |\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Title: Vector stores | ü¶úÔ∏èüîó LangChain\n",
       "\n",
       "URL Source: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/vectorstores/</span>\n",
       "\n",
       "Markdown Content:\n",
       "Note\n",
       "\n",
       "This conceptual overview focuses on text-based indexing and retrieval for simplicity. However, embedding models can\n",
       "be <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings)</span> and vector stores \n",
       "can be used to store and retrieve a variety of data types beyond text.\n",
       "\n",
       "Overview<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/vectorstores/#overview</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to Overview\"</span><span style=\"font-weight: bold\">)</span>\n",
       "--------------------------------------------------------------------------------------------------------\n",
       "\n",
       "Vector stores are specialized data stores that enable indexing and retrieving information based on vector \n",
       "representations.\n",
       "\n",
       "These vectors, called <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/embedding_models/),</span> capture the semantic meaning \n",
       "of data that has been embedded.\n",
       "\n",
       "Vector stores are frequently used to search over unstructured data, such as text, images, and audio, to retrieve \n",
       "relevant information based on semantic similarity rather than exact keyword matches.\n",
       "\n",
       "!<span style=\"font-weight: bold\">[</span>Image <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: Vector \n",
       "stores<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/assets/images/vectorstores-2540b4bc355b966c99b0f02cfdddb273.png)</span>\n",
       "\n",
       "Integrations<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/vectorstores/#integrations</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to Integrations\"</span><span style=\"font-weight: bold\">)</span>\n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "-\n",
       "\n",
       "LangChain has a large number of vectorstore integrations, allowing users to easily switch between different \n",
       "vectorstore implementations.\n",
       "\n",
       "Please see the <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/integrations/vectorstores/).</span>\n",
       "\n",
       "Interface<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/vectorstores/#interface</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to Interface\"</span><span style=\"font-weight: bold\">)</span>\n",
       "-----------------------------------------------------------------------------------------------------------\n",
       "\n",
       "LangChain provides a standard interface for working with vector stores, allowing users to easily switch between \n",
       "different vectorstore implementations.\n",
       "\n",
       "The interface consists of basic methods for writing, deleting and searching for documents in the vector store.\n",
       "\n",
       "The key methods are:\n",
       "\n",
       "*   `add_documents`: Add a list of texts to the vector store.\n",
       "*   `delete_documents`: Delete a list of documents from the vector store.\n",
       "*   `similarity_search`: Search for similar documents to a given query.\n",
       "\n",
       "Initialization<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/vectorstores/#initialization</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Initialization\"</span><span style=\"font-weight: bold\">)</span>\n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "-------\n",
       "\n",
       "Most vectors in LangChain accept an embedding model as an argument when initializing the vector store.\n",
       "\n",
       "We will use LangChain's \n",
       "<span style=\"font-weight: bold\">[</span>InMemoryVectorStore<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_m</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">emory.InMemoryVectorStore.html)</span> implementation to illustrate the API.\n",
       "\n",
       "```\n",
       "from langchain_core.vectorstores import InMemoryVectorStore# Initialize with an embedding modelvector_store = \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">InMemoryVectorStore</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SomeEmbeddingModel</span><span style=\"font-weight: bold\">())</span>\n",
       "```\n",
       "\n",
       "Adding documents<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/vectorstores/#adding-documents</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to Adding</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documents\"</span><span style=\"font-weight: bold\">)</span>\n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "-------------\n",
       "\n",
       "To add documents, use the `add_documents` method.\n",
       "\n",
       "This API works with a list of \n",
       "<span style=\"font-weight: bold\">[</span>Document<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)</span> \n",
       "objects. `Document` objects all have `page_content` and `metadata` attributes, making them a universal way to store\n",
       "unstructured text and associated metadata.\n",
       "\n",
       "```\n",
       "from langchain_core.documents import Documentdocument_1 = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"I had chocalate chip pancakes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and scrambled eggs for breakfast this morning.\"</span>,    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"source\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"tweet\"</span><span style=\"font-weight: bold\">}</span>,<span style=\"font-weight: bold\">)</span>document_2 = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\"</span>,    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"source\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"news\"</span><span style=\"font-weight: bold\">}</span>,<span style=\"font-weight: bold\">)</span>documents = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">vector_store.add_documents</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">documents</span>=<span style=\"color: #800080; text-decoration-color: #800080\">documents</span><span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "You should usually provide IDs for the documents you add to the vector store, so that instead of adding the same \n",
       "document multiple times, you can update the existing document.\n",
       "\n",
       "```\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">vector_store.add_documents</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">documents</span>=<span style=\"color: #800080; text-decoration-color: #800080\">documents</span>, <span style=\"color: #808000; text-decoration-color: #808000\">ids</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"doc1\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"doc2\"</span><span style=\"font-weight: bold\">])</span>\n",
       "```\n",
       "\n",
       "Delete<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/vectorstores/#delete</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to Delete\"</span><span style=\"font-weight: bold\">)</span>\n",
       "--------------------------------------------------------------------------------------------------\n",
       "\n",
       "To delete documents, use the `delete_documents` method which takes a list of document IDs to delete.\n",
       "\n",
       "```\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">vector_store.delete_documents</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">ids</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"doc1\"</span><span style=\"font-weight: bold\">])</span>\n",
       "```\n",
       "\n",
       "Search<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/vectorstores/#search</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to Search\"</span><span style=\"font-weight: bold\">)</span>\n",
       "--------------------------------------------------------------------------------------------------\n",
       "\n",
       "Vector stores embed and store the documents that added. If we pass in a query, the vectorstore will embed the \n",
       "query, perform a similarity search over the embedded documents, and return the most similar ones. This captures two\n",
       "important concepts: first, there needs to be a way to measure the similarity between the query and _any_ \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/embedding_models/)</span> document. Second, there needs to be an algorithm to \n",
       "efficiently perform this similarity search across _all_ embedded documents.\n",
       "\n",
       "### Similarity metrics<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/vectorstores/#similarity-metrics</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to Similarity metrics\"</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "A critical advantage of embeddings vectors is they can be compared using many simple mathematical operations:\n",
       "\n",
       "*   **Cosine Similarity**: Measures the cosine of the angle between two vectors.\n",
       "*   **Euclidean Distance**: Measures the straight-line distance between two points.\n",
       "*   **Dot Product**: Measures the projection of one vector onto another.\n",
       "\n",
       "The choice of similarity metric can sometimes be selected when initializing the vectorstore. Please refer to the \n",
       "documentation of the specific vectorstore you are using to see what similarity metrics are supported.\n",
       "\n",
       "Further reading\n",
       "\n",
       "*   See <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity)</span> from \n",
       "Google on similarity metrics to consider with embeddings.\n",
       "*   See Pinecone's <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.pinecone.io/learn/vector-similarity/)</span> on similarity metrics.\n",
       "*   See OpenAI's <span style=\"font-weight: bold\">[</span>FAQ<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://platform.openai.com/docs/guides/embeddings/faq)</span> on what similarity metric to use \n",
       "with OpenAI embeddings.\n",
       "\n",
       "### Similarity search<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/vectorstores/#similarity-search</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Similarity search\"</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "Given a similarity metric to measure the distance between the embedded query and any embedded document, we need an \n",
       "algorithm to efficiently search over _all_ the embedded documents to find the most similar ones. There are various \n",
       "ways to do this. As an example, many vectorstores implement <span style=\"font-weight: bold\">[</span>HNSW <span style=\"font-weight: bold\">(</span>Hierarchical Navigable Small \n",
       "World<span style=\"font-weight: bold\">)](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.pinecone.io/learn/series/faiss/hnsw/),</span> a graph-based index structure that allows for efficient \n",
       "similarity search. Regardless of the search algorithm used under the hood, the LangChain vectorstore interface has \n",
       "a `similarity_search` method for all integrations. This will take the search query, create an embedding, find \n",
       "similar documents, and return them as a list of \n",
       "<span style=\"font-weight: bold\">[</span>Documents<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html).</span>\n",
       "\n",
       "```\n",
       "query = <span style=\"color: #008000; text-decoration-color: #008000\">\"my query\"</span>docs = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">vectorstore.similarity_search</span><span style=\"font-weight: bold\">(</span>query<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "Many vectorstores support search parameters to be passed with the `similarity_search` method. See the documentation\n",
       "for the specific vectorstore you are using to see what parameters are supported. As an example \n",
       "<span style=\"font-weight: bold\">[</span>Pinecone<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.Pinecon</span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">eVectorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.similarity_search)</span> several parameters that \n",
       "are important general concepts: Many vectorstores support \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/integrations/vectorstores/pinecone/#query-directly),</span> which controls the number \n",
       "of Documents to return, and `filter`, which allows for filtering documents by metadata.\n",
       "\n",
       "*   `query <span style=\"font-weight: bold\">(</span>str<span style=\"font-weight: bold\">)</span> ‚Äì Text to look up documents similar to.`\n",
       "*   `k <span style=\"font-weight: bold\">(</span>int<span style=\"font-weight: bold\">)</span> ‚Äì Number of Documents to return. Defaults to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>.`\n",
       "*   `filter <span style=\"font-weight: bold\">(</span>dict | <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span> ‚Äì Dictionary of <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">argument</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span> to filter on metadata`\n",
       "\n",
       "Further reading\n",
       "\n",
       "*   See the <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/how_to/vectorstores/)</span> for more details on how to use the \n",
       "`similarity_search` method.\n",
       "*   See the <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/integrations/vectorstores/)</span> for more details on arguments that can \n",
       "be passed in to the `similarity_search` method for specific vectorstores.\n",
       "\n",
       "### Metadata filtering<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/vectorstores/#metadata-filtering</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to Metadata filtering\"</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "While vectorstore implement a search algorithm to efficiently search over _all_ the embedded documents to find the \n",
       "most similar ones, many also support filtering on metadata. This allows structured filters to reduce the size of \n",
       "the similarity search space. These two concepts work well together:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.  **Semantic search**: Query the unstructured data directly, often using via embedding or keyword similarity.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>.  **Metadata search**: Apply structured query to the metadata, filering specific documents.\n",
       "\n",
       "Vector store support for metadata filtering is typically dependent on the underlying vector store implementation.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Title: Vector stores | ü¶úÔ∏èüîó LangChain\n",
       "\n",
       "URL Source: \u001b[4;94mhttps://python.langchain.com/docs/concepts/vectorstores/\u001b[0m\n",
       "\n",
       "Markdown Content:\n",
       "Note\n",
       "\n",
       "This conceptual overview focuses on text-based indexing and retrieval for simplicity. However, embedding models can\n",
       "be \u001b[1m(\u001b[0m\u001b[4;94mhttps://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings\u001b[0m\u001b[4;94m)\u001b[0m and vector stores \n",
       "can be used to store and retrieve a variety of data types beyond text.\n",
       "\n",
       "Overview\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/vectorstores/#overview\u001b[0m \u001b[32m\"Direct link to Overview\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "--------------------------------------------------------------------------------------------------------\n",
       "\n",
       "Vector stores are specialized data stores that enable indexing and retrieving information based on vector \n",
       "representations.\n",
       "\n",
       "These vectors, called \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/embedding_models/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m capture the semantic meaning \n",
       "of data that has been embedded.\n",
       "\n",
       "Vector stores are frequently used to search over unstructured data, such as text, images, and audio, to retrieve \n",
       "relevant information based on semantic similarity rather than exact keyword matches.\n",
       "\n",
       "!\u001b[1m[\u001b[0mImage \u001b[1;36m5\u001b[0m: Vector \n",
       "stores\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/assets/images/vectorstores-2540b4bc355b966c99b0f02cfdddb273.png\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "\n",
       "Integrations\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/vectorstores/#integrations\u001b[0m \u001b[32m\"Direct link to Integrations\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "-\n",
       "\n",
       "LangChain has a large number of vectorstore integrations, allowing users to easily switch between different \n",
       "vectorstore implementations.\n",
       "\n",
       "Please see the \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/integrations/vectorstores/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "Interface\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/vectorstores/#interface\u001b[0m \u001b[32m\"Direct link to Interface\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "-----------------------------------------------------------------------------------------------------------\n",
       "\n",
       "LangChain provides a standard interface for working with vector stores, allowing users to easily switch between \n",
       "different vectorstore implementations.\n",
       "\n",
       "The interface consists of basic methods for writing, deleting and searching for documents in the vector store.\n",
       "\n",
       "The key methods are:\n",
       "\n",
       "*   `add_documents`: Add a list of texts to the vector store.\n",
       "*   `delete_documents`: Delete a list of documents from the vector store.\n",
       "*   `similarity_search`: Search for similar documents to a given query.\n",
       "\n",
       "Initialization\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/vectorstores/#initialization\u001b[0m \u001b[32m\"Direct link to \u001b[0m\n",
       "\u001b[32mInitialization\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "-------\n",
       "\n",
       "Most vectors in LangChain accept an embedding model as an argument when initializing the vector store.\n",
       "\n",
       "We will use LangChain's \n",
       "\u001b[1m[\u001b[0mInMemoryVectorStore\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_m\u001b[0m\n",
       "\u001b[4;94memory.InMemoryVectorStore.html\u001b[0m\u001b[4;94m)\u001b[0m implementation to illustrate the API.\n",
       "\n",
       "```\n",
       "from langchain_core.vectorstores import InMemoryVectorStore# Initialize with an embedding modelvector_store = \n",
       "\u001b[1;35mInMemoryVectorStore\u001b[0m\u001b[1m(\u001b[0m\u001b[33membedding\u001b[0m=\u001b[1;35mSomeEmbeddingModel\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "Adding documents\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/vectorstores/#adding-documents\u001b[0m \u001b[32m\"Direct link to Adding\u001b[0m\n",
       "\u001b[32mdocuments\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "-------------------------------------------------------------------------------------------------------------------\n",
       "-------------\n",
       "\n",
       "To add documents, use the `add_documents` method.\n",
       "\n",
       "This API works with a list of \n",
       "\u001b[1m[\u001b[0mDocument\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html\u001b[0m\u001b[4;94m)\u001b[0m \n",
       "objects. `Document` objects all have `page_content` and `metadata` attributes, making them a universal way to store\n",
       "unstructured text and associated metadata.\n",
       "\n",
       "```\n",
       "from langchain_core.documents import Documentdocument_1 = \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m    \u001b[33mpage_content\u001b[0m=\u001b[32m\"I\u001b[0m\u001b[32m had chocalate chip pancakes \u001b[0m\n",
       "\u001b[32mand scrambled eggs for breakfast this morning.\"\u001b[0m,    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m\"source\"\u001b[0m: \u001b[32m\"tweet\"\u001b[0m\u001b[1m}\u001b[0m,\u001b[1m)\u001b[0mdocument_2 = \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m    \n",
       "\u001b[33mpage_content\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\"\u001b[0m,    \n",
       "\u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m\"source\"\u001b[0m: \u001b[32m\"news\"\u001b[0m\u001b[1m}\u001b[0m,\u001b[1m)\u001b[0mdocuments = \u001b[1;35mvector_store.add_documents\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdocuments\u001b[0m=\u001b[35mdocuments\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "You should usually provide IDs for the documents you add to the vector store, so that instead of adding the same \n",
       "document multiple times, you can update the existing document.\n",
       "\n",
       "```\n",
       "\u001b[1;35mvector_store.add_documents\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdocuments\u001b[0m=\u001b[35mdocuments\u001b[0m, \u001b[33mids\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m\"doc1\"\u001b[0m, \u001b[32m\"doc2\"\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "Delete\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/vectorstores/#delete\u001b[0m \u001b[32m\"Direct link to Delete\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "--------------------------------------------------------------------------------------------------\n",
       "\n",
       "To delete documents, use the `delete_documents` method which takes a list of document IDs to delete.\n",
       "\n",
       "```\n",
       "\u001b[1;35mvector_store.delete_documents\u001b[0m\u001b[1m(\u001b[0m\u001b[33mids\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m\"doc1\"\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "Search\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/vectorstores/#search\u001b[0m \u001b[32m\"Direct link to Search\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "--------------------------------------------------------------------------------------------------\n",
       "\n",
       "Vector stores embed and store the documents that added. If we pass in a query, the vectorstore will embed the \n",
       "query, perform a similarity search over the embedded documents, and return the most similar ones. This captures two\n",
       "important concepts: first, there needs to be a way to measure the similarity between the query and _any_ \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/embedding_models/\u001b[0m\u001b[4;94m)\u001b[0m document. Second, there needs to be an algorithm to \n",
       "efficiently perform this similarity search across _all_ embedded documents.\n",
       "\n",
       "### Similarity metrics\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/vectorstores/#similarity-metrics\u001b[0m \u001b[32m\"Direct link \u001b[0m\n",
       "\u001b[32mto Similarity metrics\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "A critical advantage of embeddings vectors is they can be compared using many simple mathematical operations:\n",
       "\n",
       "*   **Cosine Similarity**: Measures the cosine of the angle between two vectors.\n",
       "*   **Euclidean Distance**: Measures the straight-line distance between two points.\n",
       "*   **Dot Product**: Measures the projection of one vector onto another.\n",
       "\n",
       "The choice of similarity metric can sometimes be selected when initializing the vectorstore. Please refer to the \n",
       "documentation of the specific vectorstore you are using to see what similarity metrics are supported.\n",
       "\n",
       "Further reading\n",
       "\n",
       "*   See \u001b[1m(\u001b[0m\u001b[4;94mhttps://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity\u001b[0m\u001b[4;94m)\u001b[0m from \n",
       "Google on similarity metrics to consider with embeddings.\n",
       "*   See Pinecone's \u001b[1m(\u001b[0m\u001b[4;94mhttps://www.pinecone.io/learn/vector-similarity/\u001b[0m\u001b[4;94m)\u001b[0m on similarity metrics.\n",
       "*   See OpenAI's \u001b[1m[\u001b[0mFAQ\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://platform.openai.com/docs/guides/embeddings/faq\u001b[0m\u001b[4;94m)\u001b[0m on what similarity metric to use \n",
       "with OpenAI embeddings.\n",
       "\n",
       "### Similarity search\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/vectorstores/#similarity-search\u001b[0m \u001b[32m\"Direct link to\u001b[0m\n",
       "\u001b[32mSimilarity search\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "Given a similarity metric to measure the distance between the embedded query and any embedded document, we need an \n",
       "algorithm to efficiently search over _all_ the embedded documents to find the most similar ones. There are various \n",
       "ways to do this. As an example, many vectorstores implement \u001b[1m[\u001b[0mHNSW \u001b[1m(\u001b[0mHierarchical Navigable Small \n",
       "World\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://www.pinecone.io/learn/series/faiss/hnsw/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m a graph-based index structure that allows for efficient \n",
       "similarity search. Regardless of the search algorithm used under the hood, the LangChain vectorstore interface has \n",
       "a `similarity_search` method for all integrations. This will take the search query, create an embedding, find \n",
       "similar documents, and return them as a list of \n",
       "\u001b[1m[\u001b[0mDocuments\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "```\n",
       "query = \u001b[32m\"my query\"\u001b[0mdocs = \u001b[1;35mvectorstore.similarity_search\u001b[0m\u001b[1m(\u001b[0mquery\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "Many vectorstores support search parameters to be passed with the `similarity_search` method. See the documentation\n",
       "for the specific vectorstore you are using to see what parameters are supported. As an example \n",
       "\u001b[1m[\u001b[0mPinecone\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.Pinecon\u001b[0m\n",
       "\u001b[4;94meVectorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.similarity_search\u001b[0m\u001b[4;94m)\u001b[0m several parameters that \n",
       "are important general concepts: Many vectorstores support \n",
       "\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/integrations/vectorstores/pinecone/#query-directly\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m which controls the number \n",
       "of Documents to return, and `filter`, which allows for filtering documents by metadata.\n",
       "\n",
       "*   `query \u001b[1m(\u001b[0mstr\u001b[1m)\u001b[0m ‚Äì Text to look up documents similar to.`\n",
       "*   `k \u001b[1m(\u001b[0mint\u001b[1m)\u001b[0m ‚Äì Number of Documents to return. Defaults to \u001b[1;36m4\u001b[0m.`\n",
       "*   `filter \u001b[1m(\u001b[0mdict | \u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m ‚Äì Dictionary of \u001b[1;35margument\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m to filter on metadata`\n",
       "\n",
       "Further reading\n",
       "\n",
       "*   See the \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/how_to/vectorstores/\u001b[0m\u001b[4;94m)\u001b[0m for more details on how to use the \n",
       "`similarity_search` method.\n",
       "*   See the \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/integrations/vectorstores/\u001b[0m\u001b[4;94m)\u001b[0m for more details on arguments that can \n",
       "be passed in to the `similarity_search` method for specific vectorstores.\n",
       "\n",
       "### Metadata filtering\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/vectorstores/#metadata-filtering\u001b[0m \u001b[32m\"Direct link \u001b[0m\n",
       "\u001b[32mto Metadata filtering\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "While vectorstore implement a search algorithm to efficiently search over _all_ the embedded documents to find the \n",
       "most similar ones, many also support filtering on metadata. This allows structured filters to reduce the size of \n",
       "the similarity search space. These two concepts work well together:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m.  **Semantic search**: Query the unstructured data directly, often using via embedding or keyword similarity.\n",
       "\u001b[1;36m2\u001b[0m.  **Metadata search**: Apply structured query to the metadata, filering specific documents.\n",
       "\n",
       "Vector store support for metadata filtering is typically dependent on the underlying vector store implementation.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if \n",
       "relying on caching the **exact** inputs into the model. For example, how likely do you think that multiple \n",
       "conversations start with the exact same message? What about the exact same three messages?\n",
       "\n",
       "An alternative approach is to use semantic caching, where you cache responses based on the meaning of the input \n",
       "rather than the exact input itself. This can be effective in some situations, but not in others.\n",
       "\n",
       "A semantic cache introduces a dependency on another model on the critical path of your application <span style=\"font-weight: bold\">(</span>e.g., the \n",
       "semantic cache may rely on an <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/embedding_models/)</span> to convert text to a \n",
       "vector representation<span style=\"font-weight: bold\">)</span>, and it's not guaranteed to capture the meaning of the input accurately.\n",
       "\n",
       "However, there might be situations where caching chat model responses is beneficial. For example, if you have a \n",
       "chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the \n",
       "model provider, costs, and improve response times.\n",
       "\n",
       "Please see the <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/how_to/chat_model_caching/)</span> guide for more details.\n",
       "\n",
       "*   How-to guides on using chat models: <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/how_to/#chat-models).</span>\n",
       "*   List of supported chat models: <span style=\"font-weight: bold\">(</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/integrations/chat/).</span>\n",
       "\n",
       "### Conceptual guides<span style=\"font-weight: bold\">[</span>‚Äã<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/chat_models/#conceptual-guides</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"Direct link to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Conceptual guides\"</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "*   <span style=\"font-weight: bold\">[</span>Messages<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/messages/)</span>\n",
       "*   <span style=\"font-weight: bold\">[</span>Tool calling<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/tool_calling/)</span>\n",
       "*   <span style=\"font-weight: bold\">[</span>Multimodality<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/multimodality/)</span>\n",
       "*   <span style=\"font-weight: bold\">[</span>Structured outputs<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://python.langchain.com/docs/concepts/structured_outputs/)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if \n",
       "relying on caching the **exact** inputs into the model. For example, how likely do you think that multiple \n",
       "conversations start with the exact same message? What about the exact same three messages?\n",
       "\n",
       "An alternative approach is to use semantic caching, where you cache responses based on the meaning of the input \n",
       "rather than the exact input itself. This can be effective in some situations, but not in others.\n",
       "\n",
       "A semantic cache introduces a dependency on another model on the critical path of your application \u001b[1m(\u001b[0me.g., the \n",
       "semantic cache may rely on an \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/embedding_models/\u001b[0m\u001b[4;94m)\u001b[0m to convert text to a \n",
       "vector representation\u001b[1m)\u001b[0m, and it's not guaranteed to capture the meaning of the input accurately.\n",
       "\n",
       "However, there might be situations where caching chat model responses is beneficial. For example, if you have a \n",
       "chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the \n",
       "model provider, costs, and improve response times.\n",
       "\n",
       "Please see the \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/how_to/chat_model_caching/\u001b[0m\u001b[4;94m)\u001b[0m guide for more details.\n",
       "\n",
       "*   How-to guides on using chat models: \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/how_to/#chat-models\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "*   List of supported chat models: \u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/integrations/chat/\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m.\u001b[0m\n",
       "\n",
       "### Conceptual guides\u001b[1m[\u001b[0m‚Äã\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/chat_models/#conceptual-guides\u001b[0m \u001b[32m\"Direct link to \u001b[0m\n",
       "\u001b[32mConceptual guides\"\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "*   \u001b[1m[\u001b[0mMessages\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/messages/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "*   \u001b[1m[\u001b[0mTool calling\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/tool_calling/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "*   \u001b[1m[\u001b[0mMultimodality\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/multimodality/\u001b[0m\u001b[4;94m)\u001b[0m\n",
       "*   \u001b[1m[\u001b[0mStructured outputs\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://python.langchain.com/docs/concepts/structured_outputs/\u001b[0m\u001b[4;94m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loop through the retrieved documents and print the content of each\n",
    "for doc in retrieved_docs:\n",
    "\tprint(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
