{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b5643f",
   "metadata": {},
   "source": [
    "# Multi Query Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loaders = [\n",
    "    TextLoader(\"data/langchain.md\"),\n",
    "    TextLoader(\"data/langchain2.md\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OllamaEmbeddings(model='snowflake-arctic-embed:33m')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf63df3",
   "metadata": {},
   "source": [
    "## Smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f4e1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c889bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The splitter to use to create smaller chunks\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "sub_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id\n",
    "    sub_docs.extend(_sub_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f055aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(sub_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d006fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': '621845ba-bbca-4f97-a170-f4c2d87bcb97', 'source': 'data/langchain2.md'}, page_content='This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See [here](https://jupyter.org/install) for instructions on how to install.\\n\\n### Installation[\\u200b](https://python.langchain.com/docs/tutorials/summarization/#installation \"Direct link to Installation\")\\n\\nTo install LangChain run:\\n\\n*   Pip\\n*   Conda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.vectorstore.similarity_search(\"LangChain\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f9ffdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9931"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retriever.invoke(\"LangChain\")[0].page_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49b5a197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9931"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.multi_vector import SearchType\n",
    "\n",
    "retriever.search_type = SearchType.mmr\n",
    "\n",
    "len(retriever.invoke(\"LangChain\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3114f",
   "metadata": {},
   "source": [
    "## Associating summaries with a document for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12106958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm= ChatOllama(model='llama3.2:1b')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "187acba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "298bbc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd5a29b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c6c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edbdfdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can also add the original chunks to the vectorstore if we so want\n",
    "# for i, doc in enumerate(docs):\n",
    "#     doc.metadata[id_key] = doc_ids[i]\n",
    "# retriever.vectorstore.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c7340d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': 'ed4f36d0-1118-4dec-b0fc-549844c8418f'}, page_content='LangChain is an open-source AI development environment that provides a wide range of features and tools for building, training, and deploying machine learning models. It includes built-in support for natural language processing (NLP) tasks such as text classification, sentiment analysis, and document similarity.\\n\\nBelow are some key points about the code snippet provided:\\n\\n1.  **Retrieval using raw input query**: The original code snippet is using the `raw_input_query` to retrieve documents from a vector store.\\n2.  **Query analysis**: The updated code snippet includes a line of code that adds metadata filters to the documents in the vector store based on their sections.\\n\\nHere\\'s a more detailed breakdown of how this might work:\\n\\n*   When building an NLP model, you typically need to preprocess and transform your data into a format suitable for training the model. In LangChain, this is often done using techniques such as tokenization, stemming or lemmatization, and vectorization.\\n*   Once your preprocessed data has been stored in a vector store, you can use various tools and APIs provided by LangChain to perform NLP tasks on it.\\n\\nHere\\'s an updated code snippet that includes the query analysis:\\n\\n```python\\nfrom langchain import InMemoryVectorStore\\n\\n# Initialize LangChain\\nvector_store = InMemoryVectorStore()\\n\\n# Add some documents to the vector store with metadata filters\\nmetadata = {\\'section\\': \\'beginning\\'}\\nall_splits = [\\n    {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 8, \\'section\\': \\'beginning\\'},\\n    {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 9},\\n]\\n\\nfor document in all_splits:\\n    vector_store.add(document)\\n    \\n# Perform NLP tasks on the preprocessed data\\nfrom langchain import RLM, PromptTemplate\\n\\ndef query_analysis(doc_id):\\n    # Load the document\\n    doc = vector_store.get(doc_id)\\n    \\n    if doc.metadata[\\'section\\'] == \\'beginning\\':\\n        return \"You asked a question at the beginning.\"\\n    elif doc.metadata[\\'section\\'] == \\'middle\\':\\n        return \"You asked a question in the middle.\"\\n    else:\\n        return \"You asked a question at the end.\"\\n\\n# Use the query analysis function to analyze queries\\nfor i, document in enumerate(all_splits):\\n    print(f\"Document {i+1}: {query_analysis(document.id)}\")\\n```\\n\\nThis code snippet demonstrates how you can use LangChain\\'s built-in support for NLP tasks, along with its vector store and metadata filtering features. It includes a query analysis function that can be used to analyze queries made by the user.\\n\\n**Customizing the prompt**\\n\\nAs shown above, you can load prompts (e.g., [this RAG prompt](https://smith.langchain.com/hub/rlm/rag-prompt)) from the prompt hub and use them in your application. For example:\\n\\n```python\\nfrom langchain import PromptTemplate\\n\\n# Load a custom prompt template\\ncustom_rag_prompt = PromptTemplate.from_template(template)\\n\\n# Use the custom prompt to generate answers\\ndef answer(prompt):\\n    # Load the prompt\\n    prompt = vector_store.get(prompt.id)\\n    \\n    return custom_rag_prompt.generate(prompt)\\n\\n# Test the function with the current document ID\\ndocument_id = 1\\nprint(answer(document_id))\\n```\\n\\nThis code snippet demonstrates how you can load a custom prompt template and use it to generate answers. You can also customize the prompt by adding your own questions or conditions.\\n\\n**In-memory vector store**\\n\\nLangChain includes an in-memory vector store that allows for fast data storage and retrieval. Here\\'s an updated code snippet that uses this in-memory vector store:\\n\\n```python\\nfrom langchain import InMemoryVectorStore\\n\\n# Initialize the in-memory vector store\\nvector_store = InMemoryVectorStore()\\n\\n# Add some documents to the vector store with metadata filters\\nmetadata = {\\'section\\': \\'beginning\\'}\\nall_splits = [\\n    {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 8, \\'section\\': \\'beginning\\'},\\n    {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 9},\\n]\\n\\nfor document in all_splits:\\n    vector_store.add(document)\\n    \\n# Perform NLP tasks on the preprocessed data\\nfrom langchain import RLM, PromptTemplate\\n\\ndef query_analysis(doc_id):\\n    # Load the document\\n    doc = vector_store.get(doc_id)\\n    \\n    if doc.metadata[\\'section\\'] == \\'beginning\\':\\n        return \"You asked a question at the beginning.\"\\n    elif doc.metadata[\\'section\\'] == \\'middle\\':\\n        return \"You asked a question in the middle.\"\\n    else:\\n        return \"You asked a question at the end.\"\\n\\n# Use the query analysis function to analyze queries\\nfor i, document in enumerate(all_splits):\\n    print(f\"Document {i+1}: {query_analysis(document.id)}\")\\n```\\n\\nThis code snippet demonstrates how you can use LangChain\\'s in-memory vector store to perform NLP tasks on preprocessed data.')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_docs = retriever.vectorstore.similarity_search(\"LangChain\")\n",
    "\n",
    "print(sub_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e53180f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9960"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"LangChain\")\n",
    "\n",
    "len(retrieved_docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
