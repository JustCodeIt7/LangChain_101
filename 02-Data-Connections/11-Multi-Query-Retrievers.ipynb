{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b5643f",
   "metadata": {},
   "source": [
    "# Multi Query Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loaders = [\n",
    "    TextLoader(\"data/langchain.md\"),\n",
    "    TextLoader(\"data/langchain2.md\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OllamaEmbeddings(model='snowflake-arctic-embed:33m')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf63df3",
   "metadata": {},
   "source": [
    "## Smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f4e1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c889bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The splitter to use to create smaller chunks\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "sub_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id\n",
    "    sub_docs.extend(_sub_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f055aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(sub_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d006fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': 'a6465e67-d932-4bb5-ab5a-e3233e5321d6', 'source': 'data/langchain.md'}, page_content='*   Provides support for [async programming](https://python.langchain.com/docs/concepts/async/), [efficient batching](https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch), [a rich streaming API](https://python.langchain.com/docs/concepts/streaming/).')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.vectorstore.similarity_search(\"LangChain\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': 'a6465e67-d932-4bb5-ab5a-e3233e5321d6', 'source': 'data/langchain.md'}, page_content='*   Provides support for [async programming](https://python.langchain.com/docs/concepts/async/), [efficient batching](https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch), [a rich streaming API](https://python.langchain.com/docs/concepts/streaming/).')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.vectorstore.similarity_search(\"LangChain\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': 'a6465e67-d932-4bb5-ab5a-e3233e5321d6', 'source': 'data/langchain.md'}, page_content='*   Provides support for [async programming](https://python.langchain.com/docs/concepts/async/), [efficient batching](https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch), [a rich streaming API](https://python.langchain.com/docs/concepts/streaming/).')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever.vectorstore.similarity_search(\"LangChain\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f9ffdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9488"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retriever.invoke(\"LangChain\")[0].page_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49b5a197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9488"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.multi_vector import SearchType\n",
    "\n",
    "retriever.search_type = SearchType.mmr\n",
    "\n",
    "len(retriever.invoke(\"LangChain\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3114f",
   "metadata": {},
   "source": [
    "## Associating summaries with a document for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12106958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm= ChatOllama(model='llama3.2:1b')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "187acba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "298bbc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd5a29b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c6c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edbdfdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can also add the original chunks to the vectorstore if we so want\n",
    "# for i, doc in enumerate(docs):\n",
    "#     doc.metadata[id_key] = doc_ids[i]\n",
    "# retriever.vectorstore.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4c7340d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='The document discusses LangChain, a library for interacting with large language models (LLMs). It provides an overview of how LLMs work, including their capabilities and limitations. The document also explains the different types of LLMs available, such as chat models, and how they differ from traditional LLMs.\n",
      "\n",
      "Some key points discussed in the document include:\n",
      "\n",
      "*   **Interface**: LangChain chat models implement the `BaseChatModel` interface, which allows for standard streaming, async programming, optimized batching, and more.\n",
      "*   **Standard parameters**: Many chat models have standardized parameters that can be used to configure the model, such as temperature, maximum token count, and max wait time.\n",
      "*   **Message formats**: LangChain supports two message formats: LangChain's own message format and OpenAI's message format.\n",
      "*   **Key methods**: The document explains the key methods of a chat model, including `invoke`, `stream`, `batch`, `bind_tools`, and `with_structured_output`.\n",
      "\n",
      "The document also highlights some differences between traditional LLMs and chat models:\n",
      "\n",
      "*   **Chat models vs. traditional LLMs**: Chat models are typically used for tasks such as language translation, text summarization, or conversational dialogue systems.\n",
      "*   **Interoperability with traditional LLMs**: Chat models may not be directly compatible with traditional LLMs due to differences in their architecture and interface.\n",
      "\n",
      "Overall, the document provides a comprehensive overview of LangChain chat models, their capabilities, and limitations. It also highlights some key differences between chat models and traditional LLMs.' metadata={'doc_id': '2a138dc2-e964-4303-8d6e-5bdbaf97b176'}\n"
     ]
    }
   ],
   "source": [
    "sub_docs = retriever.vectorstore.similarity_search(\"LangChain\")\n",
    "\n",
    "print(sub_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e53180f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9488"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"LangChain\")\n",
    "\n",
    "len(retrieved_docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
