{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a337645",
   "metadata": {},
   "source": [
    "# Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a573d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    TextLoader(\"data/langchain.md\"),\n",
    "    TextLoader(\"data/langchain2.md\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5258c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a text splitter that will be used to create child documents from larger parent documents.\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "# Initialize a vector store named \"full_documents\" which will index the child chunks of the documents.\n",
    "# The OllamaEmbeddings model \"snowflake-arctic-embed:33m\" is used to generate embeddings for these chunks.\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OllamaEmbeddings(model=\"snowflake-arctic-embed:33m\")\n",
    ")\n",
    "# Set up an in-memory storage layer that will store the parent documents.\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Create a retriever that uses the previously defined vector store, document store, and child splitter.\n",
    "# This retriever will be able to fetch relevant parent documents based on queries and split them into child chunks as needed.\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcea20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(docs, ids=None)\n",
    "list(store.yield_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28bc633",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"LangChian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf9613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15683"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"LangChian\")\n",
    "len(retrieved_docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d9e1e",
   "metadata": {},
   "source": [
    "## Retrieving larger chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77fc9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=OllamaEmbeddings(model=\"snowflake-arctic-embed:33m\")\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever that uses the previously defined vector store, document store, child splitter, and parent splitter.\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3dff64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.add_documents(docs)\n",
    "len(list(store.yield_keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4528c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "### Streaming[​](https://python.langchain.com/docs/tutorials/summarization/#streaming \"Direct link to Streaming\")\n"
     ]
    }
   ],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"LangChian\")\n",
    "\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa2cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   128k token OpenAI `gpt-4o`\n",
      "*   200k token Anthropic `claude-3-5-sonnet-20240620`\n",
      "\n",
      "The chain will take a list of documents, insert them all into a prompt, and pass that prompt to an LLM:\n",
      "\n",
      "```\n",
      "from langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain.chains.llm import LLMChainfrom langchain_core.prompts import ChatPromptTemplate# Define promptprompt = ChatPromptTemplate.from_messages(    [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")])# Instantiate chainchain = create_stuff_documents_chain(llm, prompt)# Invoke chainresult = chain.invoke({\"context\": docs})print(result)\n",
      "```\n",
      "### Streaming[​](https://python.langchain.com/docs/tutorials/summarization/#streaming \"Direct link to Streaming\")\n",
      "\n",
      "Note that we can also stream the result token-by-token:\n",
      "\n",
      "```\n",
      "for token in chain.stream({\"context\": docs}):    print(token, end=\"|\")\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Go deeper[​](https://python.langchain.com/docs/tutorials/summarization/#go-deeper \"Direct link to Go deeper\")\n",
      "\n",
      "*   You can easily customize the prompt.\n",
      "*   You can easily try different LLMs, (e.g., [Claude](https://python.langchain.com/docs/integrations/chat/anthropic/)) via the `llm` parameter.\n",
      "\n",
      "Map-Reduce: summarize long texts via parallelization[​](https://python.langchain.com/docs/tutorials/summarization/#map-reduce \"Direct link to Map-Reduce: summarize long texts via parallelization\")\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Let's unpack the map reduce approach. For this, we'll first map each document to an individual summary using an LLM. Then we'll reduce or consolidate those summaries into a single global summary.\n",
      "\n",
      "Note that the map step is typically parallelized over the input documents.\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"LangChian\")\n",
    "\n",
    "len(retrieved_docs[0].page_content)\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6704c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
