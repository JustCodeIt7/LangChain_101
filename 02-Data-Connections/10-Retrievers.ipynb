{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a337645",
   "metadata": {},
   "source": [
    "# Retrievers\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.\n",
    "\n",
    "Interface:\n",
    "- Input: A Query (string)\n",
    "- Output: A list of documents (standardized LangChain Document objects)\n",
    "\n",
    "Common retrievers include:\n",
    "- Vector store retrievers\n",
    "- Search api retrievers\n",
    "- Relational database retrievers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ddc754",
   "metadata": {},
   "source": [
    "### Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a573d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    TextLoader(\"data/langchain.md\"),\n",
    "    TextLoader(\"data/langchain2.md\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61725d9c",
   "metadata": {},
   "source": [
    "## Retrieving Documents\n",
    "\n",
    "**Conflicting needs in document retrieval:**\n",
    "\n",
    "- Need for small chunks to maintain embedding accuracy\n",
    "- Need for longer chunks to preserve context\n",
    "\n",
    "Steps:\n",
    "1. Split and store small chunks of data.\n",
    "2.\tThe retriever first fetches the small chunks.\n",
    "3.\tIt then looks up the parent IDs for those chunks.\n",
    "4.\tFinally, it returns the larger documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7376b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a text splitter that will be used to create child documents from larger parent documents.\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "\n",
    "# Initialize a vector store named \"full_documents\" which will index the child chunks of the documents.\n",
    "# The OllamaEmbeddings model \"snowflake-arctic-embed:33m\" is used to generate embeddings for these chunks.\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OllamaEmbeddings(model=\"snowflake-arctic-embed:33m\")\n",
    ")\n",
    "# Set up an in-memory storage layer that will store the parent documents.\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Create a retriever that uses the previously defined vector store, document store, and child splitter.\n",
    "# This retriever will be able to fetch relevant parent documents based on queries and split them into child chunks as needed.\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5627de50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6052ec2b-3bec-4ed6-ab9e-73148f36109a',\n",
       " '5a539fb2-1e79-43ff-9c87-8862cd0897b7']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.add_documents(docs, ids=None)\n",
    "list(store.yield_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7285c943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'doc_id': '6052ec2b-3bec-4ed6-ab9e-73148f36109a', 'source': 'data/langchain.md'}, page_content='| Parameter      | Description                                                                                                                                                                                                                                                                                                                                                          |')]\n"
     ]
    }
   ],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"What is LangChian\", k=1)\n",
    "print(sub_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcf98fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21344\n",
      "[Document(metadata={'source': 'data/langchain.md'}, page_content='Title: Chat models | ü¶úÔ∏èüîó LangChain\\n\\nURL Source: https://python.langchain.com/docs/concepts/chat_models/\\n\\nMarkdown Content:\\nOverview[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#overview \"Direct link to Overview\")\\n-------------------------------------------------------------------------------------------------------\\n\\nLarge Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.\\n\\nModern LLMs are typically accessed through a chat model interface that takes a list of [messages](https://python.langchain.com/docs/concepts/messages/) as input and returns a [message](https://python.langchain.com/docs/concepts/messages/) as output.\\n\\nThe newest generation of chat models offer additional capabilities:\\n\\n*   [Tool calling](https://python.langchain.com/docs/concepts/tool_calling/): Many popular chat models offer a native [tool calling](https://python.langchain.com/docs/concepts/tool_calling/) API. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.\\n*   [Structured output](https://python.langchain.com/docs/concepts/structured_outputs/): A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\n*   [Multimodality](https://python.langchain.com/docs/concepts/multimodality/): The ability to work with data other than text; for example, images, audio, and video.\\n\\nFeatures[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#features \"Direct link to Features\")\\n-------------------------------------------------------------------------------------------------------\\n\\nLangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.\\n\\n*   Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](https://python.langchain.com/docs/integrations/chat/) for an up-to-date list of supported models.\\n*   Use either LangChain\\'s [messages](https://python.langchain.com/docs/concepts/messages/) format or OpenAI format.\\n*   Standard [tool calling API](https://python.langchain.com/docs/concepts/tool_calling/): standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model.\\n*   Standard API for [structuring outputs](https://python.langchain.com/docs/concepts/structured_outputs/#structured-output-method) via the `with_structured_output` method.\\n*   Provides support for [async programming](https://python.langchain.com/docs/concepts/async/), [efficient batching](https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch), [a rich streaming API](https://python.langchain.com/docs/concepts/streaming/).\\n*   Integration with [LangSmith](https://docs.smith.langchain.com/) for monitoring and debugging production-grade applications based on LLMs.\\n*   Additional features like standardized [token usage](https://python.langchain.com/docs/concepts/messages/#aimessage), [rate limiting](https://python.langchain.com/docs/concepts/chat_models/#rate-limiting), [caching](https://python.langchain.com/docs/concepts/chat_models/#caching) and more.\\n\\nIntegrations[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#integrations \"Direct link to Integrations\")\\n-------------------------------------------------------------------------------------------------------------------\\n\\nLangChain has many chat model integrations that allow you to use a wide variety of models from different providers.\\n\\nThese integrations are one of two types:\\n\\n1.  **Official models**: These are models that are officially supported by LangChain and/or model provider. You can find these models in the `langchain-<provider>` packages.\\n2.  **Community models**: There are models that are mostly contributed and supported by the community. You can find these models in the `langchain-community` package.\\n\\nLangChain chat models are named with a convention that prefixes \"Chat\" to their class names (e.g., `ChatOllama`, `ChatAnthropic`, `ChatOpenAI`, etc.).\\n\\nPlease review the [chat model integrations](https://python.langchain.com/docs/integrations/chat/) for a list of supported models.\\n\\nnote\\n\\nModels that do **not** include the prefix \"Chat\" in their name or include \"LLM\" as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.\\n\\nInterface[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#interface \"Direct link to Interface\")\\n----------------------------------------------------------------------------------------------------------\\n\\nLangChain chat models implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface. Because `BaseChatModel` also implements the [Runnable Interface](https://python.langchain.com/docs/concepts/runnables/), chat models support a [standard streaming interface](https://python.langchain.com/docs/concepts/streaming/), [async programming](https://python.langchain.com/docs/concepts/async/), optimized [batching](https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch), and more. Please see the [Runnable Interface](https://python.langchain.com/docs/concepts/runnables/) for more details.\\n\\nMany of the key methods of chat models operate on [messages](https://python.langchain.com/docs/concepts/messages/) as input and return messages as output.\\n\\nChat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the [standard parameters](https://python.langchain.com/docs/concepts/chat_models/#standard-parameters) section for more details.\\n\\nnote\\n\\nIn documentation, we will often use the terms \"LLM\" and \"Chat Model\" interchangeably. This is because most modern LLMs are exposed to users via a chat model interface.\\n\\nHowever, LangChain also has implementations of older LLMs that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. These models are typically named without the \"Chat\" prefix (e.g., `Ollama`, `Anthropic`, `OpenAI`, etc.). These models implement the [BaseLLM](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.BaseLLM.html#langchain_core.language_models.llms.BaseLLM) interface and may be named with the \"LLM\" suffix (e.g., `OllamaLLM`, `AnthropicLLM`, `OpenAILLM`, etc.). Generally, users should not use these models.\\n\\n### Key methods[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#key-methods \"Direct link to Key methods\")\\n\\nThe key methods of a chat model are:\\n\\n1.  **invoke**: The primary method for interacting with a chat model. It takes a list of [messages](https://python.langchain.com/docs/concepts/messages/) as input and returns a list of messages as output.\\n2.  **stream**: A method that allows you to stream the output of a chat model as it is generated.\\n3.  **batch**: A method that allows you to batch multiple requests to a chat model together for more efficient processing.\\n4.  **bind\\\\_tools**: A method that allows you to bind a tool to a chat model for use in the model\\'s execution context.\\n5.  **with\\\\_structured\\\\_output**: A wrapper around the `invoke` method for models that natively support [structured output](https://python.langchain.com/docs/concepts/structured_outputs/).\\n\\nOther important methods can be found in the [BaseChatModel API Reference](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html).\\n\\n### Inputs and outputs[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#inputs-and-outputs \"Direct link to Inputs and outputs\")\\n\\nModern LLMs are typically accessed through a chat model interface that takes [messages](https://python.langchain.com/docs/concepts/messages/) as input and returns [messages](https://python.langchain.com/docs/concepts/messages/) as output. Messages are typically associated with a role (e.g., \"system\", \"human\", \"assistant\") and one or more content blocks that contain text or potentially multimodal data (e.g., images, audio, video).\\n\\nLangChain supports two message formats to interact with chat models:\\n\\n1.  **LangChain Message Format**: LangChain\\'s own message format, which is used by default and is used internally by LangChain.\\n2.  **OpenAI\\'s Message Format**: OpenAI\\'s message format.\\n\\n### Standard parameters[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#standard-parameters \"Direct link to Standard parameters\")\\n\\nMany chat models have standardized parameters that can be used to configure the model:\\n\\n| Parameter      | Description                                                                                                                                                                                                                                                                                                                                                          |\\n| -------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `model`        | The name or identifier of the specific AI model you want to use (e.g., `\"gpt-3.5-turbo\"` or `\"gpt-4\"`).                                                                                                                                                                                                                                                              |\\n| `temperature`  | Controls the randomness of the model\\'s output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused.                                                                                                                                                                                  |\\n| `timeout`      | The maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesn‚Äôt hang indefinitely.                                                                                                                                                                                                                     |\\n| `max_tokens`   | Limits the total number of tokens (words and punctuation) in the response. This controls how long the output can be.                                                                                                                                                                                                                                                 |\\n| `stop`         | Specifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response.                                                                                                                                                                                                    |\\n| `max_retries`  | The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.                                                                                                                                                                                                                              |\\n| `api_key`      | The API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model.                                                                                                                                                                                                                                    |\\n| `base_url`     | The URL of the API endpoint where requests are sent. This is typically provided by the model\\'s provider and is necessary for directing your requests.                                                                                                                                                                                                                |\\n| `rate_limiter` | An optional [BaseRateLimiter](https://python.langchain.com/api_reference/core/rate_limiters/langchain_core.rate_limiters.BaseRateLimiter.html#langchain_core.rate_limiters.BaseRateLimiter) to space out requests to avoid exceeding rate limits. See [rate-limiting](https://python.langchain.com/docs/concepts/chat_models/#rate-limiting) below for more details. |\\n\\nSome important things to note:\\n\\n*   Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max\\\\_tokens can\\'t be supported on these.\\n*   Standard parameters are currently only enforced on integrations that have their own integration packages (e.g. `langchain-openai`, `langchain-anthropic`, etc.), they\\'re not enforced on models in `langchain-community`.\\n\\nChat models also accept other parameters that are specific to that integration. To find all the parameters supported by a Chat model head to the their respective [API reference](https://python.langchain.com/api_reference/) for that model.\\n\\nChat models can call [tools](https://python.langchain.com/docs/concepts/tools/) to perform tasks such as fetching data from a database, making API requests, or running custom code. Please see the [tool calling](https://python.langchain.com/docs/concepts/tool_calling/) guide for more information.\\n\\nStructured outputs[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#structured-outputs \"Direct link to Structured outputs\")\\n-------------------------------------------------------------------------------------------------------------------------------------\\n\\nChat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely useful for information extraction tasks. Please read more about the technique in the [structured outputs](https://python.langchain.com/docs/concepts/structured_outputs/) guide.\\n\\nMultimodality[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#multimodality \"Direct link to Multimodality\")\\n----------------------------------------------------------------------------------------------------------------------\\n\\nLarge Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as [multimodality](https://python.langchain.com/docs/concepts/multimodality/).\\n\\nCurrently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.\\n\\nContext window[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#context-window \"Direct link to Context window\")\\n-------------------------------------------------------------------------------------------------------------------------\\n\\nA chat model\\'s context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.\\n\\nIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can \"remember\" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\\n\\nThe size of the input is measured in [tokens](https://python.langchain.com/docs/concepts/tokens/) which are the unit of processing that the model uses.\\n\\nAdvanced topics[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#advanced-topics \"Direct link to Advanced topics\")\\n----------------------------------------------------------------------------------------------------------------------------\\n\\n### Rate-limiting[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#rate-limiting \"Direct link to Rate-limiting\")\\n\\nMany chat model providers impose a limit on the number of requests that can be made in a given time period.\\n\\nIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\\n\\nYou have a few options to deal with rate limits:\\n\\n1.  Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the [how to handle rate limits](https://python.langchain.com/docs/how_to/chat_model_rate_limiting/) for more information on how to use this feature.\\n2.  Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the [standard parameters](https://python.langchain.com/docs/concepts/chat_models/#standard-parameters) section for more information.\\n3.  Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.\\n\\n### Caching[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#caching \"Direct link to Caching\")\\n\\nChat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.\\n\\nThe reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the **exact** inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?\\n\\nAn alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.\\n\\nA semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](https://python.langchain.com/docs/concepts/embedding_models/) to convert text to a vector representation), and it\\'s not guaranteed to capture the meaning of the input accurately.\\n\\nHowever, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve response times.\\n\\nPlease see the [how to cache chat model responses](https://python.langchain.com/docs/how_to/chat_model_caching/) guide for more details.\\n\\n*   How-to guides on using chat models: [how-to guides](https://python.langchain.com/docs/how_to/#chat-models).\\n*   List of supported chat models: [chat model integrations](https://python.langchain.com/docs/integrations/chat/).\\n\\n### Conceptual guides[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#conceptual-guides \"Direct link to Conceptual guides\")\\n\\n*   [Messages](https://python.langchain.com/docs/concepts/messages/)\\n*   [Tool calling](https://python.langchain.com/docs/concepts/tool_calling/)\\n*   [Multimodality](https://python.langchain.com/docs/concepts/multimodality/)\\n*   [Structured outputs](https://python.langchain.com/docs/concepts/structured_outputs/)'), Document(metadata={'source': 'data/langchain2.md'}, page_content='Title: Vector stores | ü¶úÔ∏èüîó LangChain\\n\\nURL Source: https://python.langchain.com/docs/concepts/vectorstores/\\n\\nMarkdown Content:\\nNote\\n\\nThis conceptual overview focuses on text-based indexing and retrieval for simplicity. However, embedding models can be [multi-modal](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings) and vector stores can be used to store and retrieve a variety of data types beyond text.\\n\\nOverview[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#overview \"Direct link to Overview\")\\n--------------------------------------------------------------------------------------------------------\\n\\nVector stores are specialized data stores that enable indexing and retrieving information based on vector representations.\\n\\nThese vectors, called [embeddings](https://python.langchain.com/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.\\n\\nVector stores are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches.\\n\\n![Image 5: Vector stores](https://python.langchain.com/assets/images/vectorstores-2540b4bc355b966c99b0f02cfdddb273.png)\\n\\nIntegrations[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#integrations \"Direct link to Integrations\")\\n--------------------------------------------------------------------------------------------------------------------\\n\\nLangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations.\\n\\nPlease see the [full list of LangChain vectorstore integrations](https://python.langchain.com/docs/integrations/vectorstores/).\\n\\nInterface[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#interface \"Direct link to Interface\")\\n-----------------------------------------------------------------------------------------------------------\\n\\nLangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations.\\n\\nThe interface consists of basic methods for writing, deleting and searching for documents in the vector store.\\n\\nThe key methods are:\\n\\n*   `add_documents`: Add a list of texts to the vector store.\\n*   `delete_documents`: Delete a list of documents from the vector store.\\n*   `similarity_search`: Search for similar documents to a given query.\\n\\nInitialization[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#initialization \"Direct link to Initialization\")\\n--------------------------------------------------------------------------------------------------------------------------\\n\\nMost vectors in LangChain accept an embedding model as an argument when initializing the vector store.\\n\\nWe will use LangChain\\'s [InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) implementation to illustrate the API.\\n\\n```\\nfrom langchain_core.vectorstores import InMemoryVectorStore# Initialize with an embedding modelvector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())\\n```\\n\\nAdding documents[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#adding-documents \"Direct link to Adding documents\")\\n--------------------------------------------------------------------------------------------------------------------------------\\n\\nTo add documents, use the `add_documents` method.\\n\\nThis API works with a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects. `Document` objects all have `page_content` and `metadata` attributes, making them a universal way to store unstructured text and associated metadata.\\n\\n```\\nfrom langchain_core.documents import Documentdocument_1 = Document(    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",    metadata={\"source\": \"tweet\"},)document_2 = Document(    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",    metadata={\"source\": \"news\"},)documents = [document_1, document_2]vector_store.add_documents(documents=documents)\\n```\\n\\nYou should usually provide IDs for the documents you add to the vector store, so that instead of adding the same document multiple times, you can update the existing document.\\n\\n```\\nvector_store.add_documents(documents=documents, ids=[\"doc1\", \"doc2\"])\\n```\\n\\nDelete[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#delete \"Direct link to Delete\")\\n--------------------------------------------------------------------------------------------------\\n\\nTo delete documents, use the `delete_documents` method which takes a list of document IDs to delete.\\n\\n```\\nvector_store.delete_documents(ids=[\"doc1\"])\\n```\\n\\nSearch[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#search \"Direct link to Search\")\\n--------------------------------------------------------------------------------------------------\\n\\nVector stores embed and store the documents that added. If we pass in a query, the vectorstore will embed the query, perform a similarity search over the embedded documents, and return the most similar ones. This captures two important concepts: first, there needs to be a way to measure the similarity between the query and _any_ [embedded](https://python.langchain.com/docs/concepts/embedding_models/) document. Second, there needs to be an algorithm to efficiently perform this similarity search across _all_ embedded documents.\\n\\n### Similarity metrics[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#similarity-metrics \"Direct link to Similarity metrics\")\\n\\nA critical advantage of embeddings vectors is they can be compared using many simple mathematical operations:\\n\\n*   **Cosine Similarity**: Measures the cosine of the angle between two vectors.\\n*   **Euclidean Distance**: Measures the straight-line distance between two points.\\n*   **Dot Product**: Measures the projection of one vector onto another.\\n\\nThe choice of similarity metric can sometimes be selected when initializing the vectorstore. Please refer to the documentation of the specific vectorstore you are using to see what similarity metrics are supported.\\n\\nFurther reading\\n\\n*   See [this documentation](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity) from Google on similarity metrics to consider with embeddings.\\n*   See Pinecone\\'s [blog post](https://www.pinecone.io/learn/vector-similarity/) on similarity metrics.\\n*   See OpenAI\\'s [FAQ](https://platform.openai.com/docs/guides/embeddings/faq) on what similarity metric to use with OpenAI embeddings.\\n\\n### Similarity search[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#similarity-search \"Direct link to Similarity search\")\\n\\nGiven a similarity metric to measure the distance between the embedded query and any embedded document, we need an algorithm to efficiently search over _all_ the embedded documents to find the most similar ones. There are various ways to do this. As an example, many vectorstores implement [HNSW (Hierarchical Navigable Small World)](https://www.pinecone.io/learn/series/faiss/hnsw/), a graph-based index structure that allows for efficient similarity search. Regardless of the search algorithm used under the hood, the LangChain vectorstore interface has a `similarity_search` method for all integrations. This will take the search query, create an embedding, find similar documents, and return them as a list of [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html).\\n\\n```\\nquery = \"my query\"docs = vectorstore.similarity_search(query)\\n```\\n\\nMany vectorstores support search parameters to be passed with the `similarity_search` method. See the documentation for the specific vectorstore you are using to see what parameters are supported. As an example [Pinecone](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.similarity_search) several parameters that are important general concepts: Many vectorstores support [the `k`](https://python.langchain.com/docs/integrations/vectorstores/pinecone/#query-directly), which controls the number of Documents to return, and `filter`, which allows for filtering documents by metadata.\\n\\n*   `query (str) ‚Äì Text to look up documents similar to.`\\n*   `k (int) ‚Äì Number of Documents to return. Defaults to 4.`\\n*   `filter (dict | None) ‚Äì Dictionary of argument(s) to filter on metadata`\\n\\nFurther reading\\n\\n*   See the [how-to guide](https://python.langchain.com/docs/how_to/vectorstores/) for more details on how to use the `similarity_search` method.\\n*   See the [integrations page](https://python.langchain.com/docs/integrations/vectorstores/) for more details on arguments that can be passed in to the `similarity_search` method for specific vectorstores.\\n\\n### Metadata filtering[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#metadata-filtering \"Direct link to Metadata filtering\")\\n\\nWhile vectorstore implement a search algorithm to efficiently search over _all_ the embedded documents to find the most similar ones, many also support filtering on metadata. This allows structured filters to reduce the size of the similarity search space. These two concepts work well together:\\n\\n1.  **Semantic search**: Query the unstructured data directly, often using via embedding or keyword similarity.\\n2.  **Metadata search**: Apply structured query to the metadata, filering specific documents.\\n\\nVector store support for metadata filtering is typically dependent on the underlying vector store implementation.\\n\\nHere is example usage with [Pinecone](https://python.langchain.com/docs/integrations/vectorstores/pinecone/#query-directly), showing that we filter for all documents that have the metadata key `source` with value `tweet`.\\n\\n```\\nvectorstore.similarity_search(    \"LangChain provides abstractions to make working with LLMs easy\",    k=2,    filter={\"source\": \"tweet\"},)\\n```\\n\\nAdvanced search and retrieval techniques[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#advanced-search-and-retrieval-techniques \"Direct link to Advanced search and retrieval techniques\")\\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity. For example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results. As a second example, some [vector stores](https://python.langchain.com/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches. At the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`. See this [how-to guide on hybrid search](https://python.langchain.com/docs/how_to/hybrid/) for more details.\\n\\n| Name                                                                                                                                                                                                                                            | When to use                                           | Description                                                                                                                                  |\\n| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |\\n| [Hybrid search](https://python.langchain.com/docs/integrations/retrievers/pinecone_hybrid_search/)                                                                                                                                              | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. [Paper](https://arxiv.org/abs/2210.11934). |\\n| [Maximal Marginal Relevance (MMR)](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.max_marginal_relevance_search) | When needing to diversify search results.             | MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.                                        |')]\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is LangChian\")\n",
    "print(len(retrieved_docs[0].page_content))\n",
    "print(retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc84758",
   "metadata": {},
   "source": [
    "## Retrieving Large Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77fc9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=OllamaEmbeddings(model=\"snowflake-arctic-embed:33m\")\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bcea78",
   "metadata": {},
   "source": [
    "### ParentDocumentRetriever\n",
    "    - Splits and stores small chunks for embedding/indexing\n",
    "    - During retrieval, fetches small chunks first\n",
    "    - Then looks up and returns the parent documents of those chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd74a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever that uses the previously defined vector store, document store, child splitter, and parent splitter.\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a3dff64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add documents to the retriever\n",
    "retriever.add_documents(docs)\n",
    "\n",
    "# Get the total number of keys in the store\n",
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4528c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'doc_id': '50054b35-f67a-4614-8e08-58340bb732b8', 'source': 'data/langchain.md'}, page_content='LangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.'), Document(metadata={'doc_id': '50054b35-f67a-4614-8e08-58340bb732b8', 'source': 'data/langchain.md'}, page_content='*   Standard API for [structuring outputs](https://python.langchain.com/docs/concepts/structured_outputs/#structured-output-method) via the `with_structured_output` method.\\n*   Provides support for [async programming](https://python.langchain.com/docs/concepts/async/), [efficient batching](https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch), [a rich streaming API](https://python.langchain.com/docs/concepts/streaming/).'), Document(metadata={'doc_id': '0d93b96d-ff50-430d-b2fd-8fb862d224d9', 'source': 'data/langchain2.md'}, page_content='![Image 5: Vector stores](https://python.langchain.com/assets/images/vectorstores-2540b4bc355b966c99b0f02cfdddb273.png)\\n\\nIntegrations[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#integrations \"Direct link to Integrations\")\\n--------------------------------------------------------------------------------------------------------------------\\n\\nLangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations.'), Document(metadata={'doc_id': 'd6a0d7ea-1611-4aec-8b58-52b416d7ee4e', 'source': 'data/langchain.md'}, page_content='### Conceptual guides[\\u200b](https://python.langchain.com/docs/concepts/chat_models/#conceptual-guides \"Direct link to Conceptual guides\")\\n\\n*   [Messages](https://python.langchain.com/docs/concepts/messages/)\\n*   [Tool calling](https://python.langchain.com/docs/concepts/tool_calling/)\\n*   [Multimodality](https://python.langchain.com/docs/concepts/multimodality/)\\n*   [Structured outputs](https://python.langchain.com/docs/concepts/structured_outputs/)'), Document(metadata={'doc_id': '0d93b96d-ff50-430d-b2fd-8fb862d224d9', 'source': 'data/langchain2.md'}, page_content='LangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations.\\n\\nPlease see the [full list of LangChain vectorstore integrations](https://python.langchain.com/docs/integrations/vectorstores/).\\n\\nInterface[\\u200b](https://python.langchain.com/docs/concepts/vectorstores/#interface \"Direct link to Interface\")\\n-----------------------------------------------------------------------------------------------------------')]\n"
     ]
    }
   ],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"what is LangChain used for\", k=5)\n",
    "\n",
    "print(sub_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adfa2cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989\n",
      "LangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.\n",
      "\n",
      "*   Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](https://python.langchain.com/docs/integrations/chat/) for an up-to-date list of supported models.\n",
      "*   Use either LangChain's [messages](https://python.langchain.com/docs/concepts/messages/) format or OpenAI format.\n",
      "*   Standard [tool calling API](https://python.langchain.com/docs/concepts/tool_calling/): standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model.\n",
      "*   Standard API for [structuring outputs](https://python.langchain.com/docs/concepts/structured_outputs/#structured-output-method) via the `with_structured_output` method.\n",
      "*   Provides support for [async programming](https://python.langchain.com/docs/concepts/async/), [efficient batching](https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch), [a rich streaming API](https://python.langchain.com/docs/concepts/streaming/).\n",
      "*   Integration with [LangSmith](https://docs.smith.langchain.com/) for monitoring and debugging production-grade applications based on LLMs.\n",
      "*   Additional features like standardized [token usage](https://python.langchain.com/docs/concepts/messages/#aimessage), [rate limiting](https://python.langchain.com/docs/concepts/chat_models/#rate-limiting), [caching](https://python.langchain.com/docs/concepts/chat_models/#caching) and more.\n",
      "\n",
      "Integrations[‚Äã](https://python.langchain.com/docs/concepts/chat_models/#integrations \"Direct link to Integrations\")\n",
      "-------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"what is LangChain used for\")\n",
    "\n",
    "print(len(retrieved_docs[0].page_content))\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d987dc21",
   "metadata": {},
   "source": [
    "## Putting it all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbb6704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model='llama3.2:1b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19029deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)  \n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "014e6d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a library that provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use Large Language Models (LLMs).\n"
     ]
    }
   ],
   "source": [
    "# Function to format documents by joining their content\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])  \n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} \n",
    "    | prompt  # Apply the prompt template\n",
    "    | model  # Use the language model to generate a response\n",
    "    | StrOutputParser()  # Parse the output string\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What is LangChain\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eba918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
