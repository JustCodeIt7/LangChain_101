{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a337645",
   "metadata": {},
   "source": [
    "# Retrievers\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.\n",
    "\n",
    "Interface:\n",
    "- Input: A Query (string)\n",
    "- Output: A list of documents (standardized LangChain Document objects)\n",
    "\n",
    "Common retrievers include:\n",
    "- Search api retrievers\n",
    "- Relational database retrievers\n",
    "- Lexical search retrievers\n",
    "- Vector store retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a573d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    TextLoader(\"data/langchain.md\"),\n",
    "    TextLoader(\"data/langchain2.md\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5258c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a text splitter that will be used to create child documents from larger parent documents.\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "\n",
    "# Initialize a vector store named \"full_documents\" which will index the child chunks of the documents.\n",
    "# The OllamaEmbeddings model \"snowflake-arctic-embed:33m\" is used to generate embeddings for these chunks.\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OllamaEmbeddings(model=\"snowflake-arctic-embed:33m\")\n",
    ")\n",
    "# Set up an in-memory storage layer that will store the parent documents.\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Create a retriever that uses the previously defined vector store, document store, and child splitter.\n",
    "# This retriever will be able to fetch relevant parent documents based on queries and split them into child chunks as needed.\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfcea20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cb07008a-8983-4d1d-a58e-8297c349766a',\n",
       " '342d6baa-cd63-4755-838e-20c87d4ea508']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.add_documents(docs, ids=None)\n",
    "list(store.yield_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c28bc633",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"LangChian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4cf9613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21344"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"LangChian\")\n",
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d9e1e",
   "metadata": {},
   "source": [
    "## Retrieving larger chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77fc9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=OllamaEmbeddings(model=\"snowflake-arctic-embed:33m\")\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever that uses the previously defined vector store, document store, child splitter, and parent splitter.\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a3dff64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.add_documents(docs)\n",
    "len(list(store.yield_keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4528c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and more. Please see the [Runnable Interface](https://python.langchain.com/docs/concepts/runnables/) for more details.\n"
     ]
    }
   ],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"LangChian\")\n",
    "\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adfa2cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain chat models implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface. Because `BaseChatModel` also implements the [Runnable Interface](https://python.langchain.com/docs/concepts/runnables/), chat models support a [standard streaming interface](https://python.langchain.com/docs/concepts/streaming/), [async programming](https://python.langchain.com/docs/concepts/async/), optimized [batching](https://python.langchain.com/docs/concepts/runnables/#optimized-parallel-execution-batch), and more. Please see the [Runnable Interface](https://python.langchain.com/docs/concepts/runnables/) for more details.\n",
      "\n",
      "Many of the key methods of chat models operate on [messages](https://python.langchain.com/docs/concepts/messages/) as input and return messages as output.\n",
      "\n",
      "Chat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the [standard parameters](https://python.langchain.com/docs/concepts/chat_models/#standard-parameters) section for more details.\n",
      "\n",
      "note\n",
      "\n",
      "In documentation, we will often use the terms \"LLM\" and \"Chat Model\" interchangeably. This is because most modern LLMs are exposed to users via a chat model interface.\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"LangChian\")\n",
    "\n",
    "len(retrieved_docs[0].page_content)\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d987dc21",
   "metadata": {},
   "source": [
    "## Using Retrievers in LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbb6704c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context, it seems that the question is asking what the president said about technology. However, since there are no documents associated with this query in the vector store, the search will not return any results.\n",
      "\n",
      "To find a document that matches the query \"What did the president say about technology?\", you can add an additional document to the vector store before searching:\n",
      "\n",
      "```python\n",
      "from langchain_core.documents import Document\n",
      "\n",
      "document_1 = Document(    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",    metadata={\"source\": \"tweet\"},)document_2 = Document(    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",    metadata={\"source\": \"news\"},)documents = [document_1, document_2]vector_store.add_documents(documents=documents)\n",
      "```\n",
      "\n",
      "After adding the second document to the vector store:\n",
      "\n",
      "```python\n",
      "query = \"What did the president say about technology?\"\n",
      "vector_store.delete_documents(ids=[\"doc1\"])  # Delete the first document\n",
      "search_result = vector_store.search(query)  # Perform search and get a list of matching documents\n",
      "if search_result:\n",
      "    print(\"Matching documents:\")\n",
      "    for doc in search_result:\n",
      "        print(doc.page_content)\n",
      "else:\n",
      "    print(\"No results found.\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOllama(model='llama3.2:1b')\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What did the president say about technology?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336599ca",
   "metadata": {},
   "source": [
    "## Advanced retrieval patterns\n",
    "\n",
    "### Ensemble retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17afb012",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
