{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a337645",
   "metadata": {},
   "source": [
    "# Retrievers\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.\n",
    "\n",
    "Interface:\n",
    "- Input: A Query (string)\n",
    "- Output: A list of documents (standardized LangChain Document objects)\n",
    "\n",
    "Common retrievers include:\n",
    "- Vector store retrievers\n",
    "- Search api retrievers\n",
    "- Relational database retrievers\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "76ddc754",
   "metadata": {},
   "source": [
    "### Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "id": "2a573d40",
   "metadata": {},
   "source": [
    "loaders = [\n",
    "    TextLoader(\"data/langchain.md\"),\n",
    "    TextLoader(\"data/langchain2.md\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "61725d9c",
   "metadata": {},
   "source": [
    "## Retrieving Documents\n",
    "\n",
    "**Conflicting needs in document retrieval:**\n",
    "\n",
    "- Need for small chunks to maintain embedding accuracy\n",
    "- Need for longer chunks to preserve context\n",
    "\n",
    "Steps:\n",
    "1. Split and store small chunks of data.\n",
    "2.\tThe retriever first fetches the small chunks.\n",
    "3.\tIt then looks up the parent IDs for those chunks.\n",
    "4.\tFinally, it returns the larger documents."
   ]
  },
  {
   "cell_type": "code",
   "id": "a7376b4e",
   "metadata": {},
   "source": [
    "# Define a text splitter that will be used to create child documents from larger parent documents.\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "\n",
    "# Initialize a vector store named \"full_documents\" which will index the child chunks of the documents.\n",
    "# The OllamaEmbeddings model \"snowflake-arctic-embed:33m\" is used to generate embeddings for these chunks.\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OllamaEmbeddings(model=\"snowflake-arctic-embed:33m\")\n",
    ")\n",
    "# Set up an in-memory storage layer that will store the parent documents.\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Create a retriever that uses the previously defined vector store, document store, and child splitter.\n",
    "# This retriever will be able to fetch relevant parent documents based on queries and split them into child chunks as needed.\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5627de50",
   "metadata": {},
   "source": [
    "retriever.add_documents(docs, ids=None)\n",
    "list(store.yield_keys())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7285c943",
   "metadata": {},
   "source": [
    "sub_docs = vectorstore.similarity_search(\"What is LangChian\", k=1)\n",
    "print(sub_docs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dcf98fbe",
   "metadata": {},
   "source": [
    "retrieved_docs = retriever.invoke(\"What is LangChian\")\n",
    "print(len(retrieved_docs[0].page_content))\n",
    "print(retrieved_docs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bdc84758",
   "metadata": {},
   "source": [
    "## Retrieving Large Chunks"
   ]
  },
  {
   "cell_type": "code",
   "id": "77fc9133",
   "metadata": {},
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=OllamaEmbeddings(model=\"snowflake-arctic-embed:33m\")\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "58bcea78",
   "metadata": {},
   "source": [
    "### ParentDocumentRetriever\n",
    "    - Splits and stores small chunks for embedding/indexing\n",
    "    - During retrieval, fetches small chunks first\n",
    "    - Then looks up and returns the parent documents of those chunks"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd74a96e",
   "metadata": {},
   "source": [
    "# Create a retriever that uses the previously defined vector store, document store, child splitter, and parent splitter.\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2a3dff64",
   "metadata": {},
   "source": [
    "# Add documents to the retriever\n",
    "retriever.add_documents(docs)\n",
    "\n",
    "# Get the total number of keys in the store\n",
    "len(list(store.yield_keys()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4528c8f",
   "metadata": {},
   "source": [
    "sub_docs = vectorstore.similarity_search(\"what is LangChain used for\", k=5)\n",
    "\n",
    "print(sub_docs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "adfa2cfa",
   "metadata": {},
   "source": [
    "retrieved_docs = retriever.invoke(\"what is LangChain used for\")\n",
    "\n",
    "print(len(retrieved_docs[0].page_content))\n",
    "print(retrieved_docs[0].page_content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d987dc21",
   "metadata": {},
   "source": [
    "## Putting it all together "
   ]
  },
  {
   "cell_type": "code",
   "id": "bbb6704c",
   "metadata": {},
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model='llama3.2:1b')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "19029deb",
   "metadata": {},
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(prompt)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "014e6d65",
   "metadata": {},
   "source": [
    "# Function to format documents by joining their content\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt  # Apply the prompt template\n",
    "        | model  # Use the language model to generate a response\n",
    "        | StrOutputParser()  # Parse the output string\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What is LangChain\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75eba918",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
