{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a337645",
   "metadata": {},
   "source": [
    "# Retrievers\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.\n",
    "\n",
    "Retrievers accept a string query as input and return a list of Document's as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a573d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    TextLoader(\"data/langchain.md\"),\n",
    "    TextLoader(\"data/langchain2.md\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5258c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a text splitter that will be used to create child documents from larger parent documents.\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "# Initialize a vector store named \"full_documents\" which will index the child chunks of the documents.\n",
    "# The OllamaEmbeddings model \"snowflake-arctic-embed:33m\" is used to generate embeddings for these chunks.\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OllamaEmbeddings(model=\"snowflake-arctic-embed:33m\")\n",
    ")\n",
    "# Set up an in-memory storage layer that will store the parent documents.\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Create a retriever that uses the previously defined vector store, document store, and child splitter.\n",
    "# This retriever will be able to fetch relevant parent documents based on queries and split them into child chunks as needed.\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcea20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(docs, ids=None)\n",
    "list(store.yield_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28bc633",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"LangChian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf9613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15683"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"LangChian\")\n",
    "len(retrieved_docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d9e1e",
   "metadata": {},
   "source": [
    "## Retrieving larger chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=OllamaEmbeddings(model=\"snowflake-arctic-embed:33m\")\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever that uses the previously defined vector store, document store, child splitter, and parent splitter.\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3dff64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.add_documents(docs)\n",
    "len(list(store.yield_keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4528c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "### Streaming[​](https://python.langchain.com/docs/tutorials/summarization/#streaming \"Direct link to Streaming\")\n"
     ]
    }
   ],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"LangChian\")\n",
    "\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa2cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   128k token OpenAI `gpt-4o`\n",
      "*   200k token Anthropic `claude-3-5-sonnet-20240620`\n",
      "\n",
      "The chain will take a list of documents, insert them all into a prompt, and pass that prompt to an LLM:\n",
      "\n",
      "```\n",
      "from langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain.chains.llm import LLMChainfrom langchain_core.prompts import ChatPromptTemplate# Define promptprompt = ChatPromptTemplate.from_messages(    [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")])# Instantiate chainchain = create_stuff_documents_chain(llm, prompt)# Invoke chainresult = chain.invoke({\"context\": docs})print(result)\n",
      "```\n",
      "### Streaming[​](https://python.langchain.com/docs/tutorials/summarization/#streaming \"Direct link to Streaming\")\n",
      "\n",
      "Note that we can also stream the result token-by-token:\n",
      "\n",
      "```\n",
      "for token in chain.stream({\"context\": docs}):    print(token, end=\"|\")\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "### Go deeper[​](https://python.langchain.com/docs/tutorials/summarization/#go-deeper \"Direct link to Go deeper\")\n",
      "\n",
      "*   You can easily customize the prompt.\n",
      "*   You can easily try different LLMs, (e.g., [Claude](https://python.langchain.com/docs/integrations/chat/anthropic/)) via the `llm` parameter.\n",
      "\n",
      "Map-Reduce: summarize long texts via parallelization[​](https://python.langchain.com/docs/tutorials/summarization/#map-reduce \"Direct link to Map-Reduce: summarize long texts via parallelization\")\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Let's unpack the map reduce approach. For this, we'll first map each document to an individual summary using an LLM. Then we'll reduce or consolidate those summaries into a single global summary.\n",
      "\n",
      "Note that the map step is typically parallelized over the input documents.\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"LangChian\")\n",
    "\n",
    "len(retrieved_docs[0].page_content)\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d987dc21",
   "metadata": {},
   "source": [
    "## Using Retrievers in LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbb6704c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You don't need to use LangGraph to build a RAG application. The code snippet you provided is a straightforward example of how to implement two steps in an RAG application, retrieve and generate text based on user input.\n",
      "\n",
      "LangGraph provides additional features such as visualization of the control flow of your application, which can be useful for understanding how different components interact with each other. However, for simple applications like this one, using individual component invocations is sufficient.\n",
      "\n",
      "If you want to use LangGraph, you can create a separate graph for each component and add edges between them to represent their dependencies. Alternatively, you can use the `StateGraph` class provided by LangGraph, which allows you to define a single sequence of components that form your application.\n",
      "\n",
      "It's worth noting that LangChain is a broader language model framework that provides a lot of utility functions and tools for building and training models. If you're looking to build more complex applications, you may want to consider using LangChain instead of implementing everything yourself from scratch.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOllama(model='llama3.2:1b')\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What did the president say about technology?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17afb012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
