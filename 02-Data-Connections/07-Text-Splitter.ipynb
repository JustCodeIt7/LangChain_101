{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22b1cc7",
   "metadata": {},
   "source": [
    "# Text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:10:34.857862Z",
     "start_time": "2024-11-29T19:10:34.534996Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text = \"\"\"The world of street food is a vibrant tapestry of flavors and cultures. From sizzling skewers to spicy tacos, every corner of the globe has its own unique offerings. Vendors often  up shop in bustling markets or on busy street corners, attracting hungry passersby with mouthwatering aromas.\n",
    "\n",
    "One of the best things about street food is its accessibility. Itâ€™s quick, affordable, and often made fresh right in front of you. Whether itâ€™s a steaming bowl of pho in Vietnam or a crispy samosa in India, thereâ€™s something for everyone to enjoy.\n",
    "\n",
    "Street food also brings people together. Friends and families gather around food stalls, sharing dishes and stories. Itâ€™s a social experience that transcends language and culture, creating connections over a shared love of good eats.\n",
    "\n",
    "Finally, street food is constantly evolving. Chefs are experimenting with traditional recipes, adding modern twists and fusion flavors. This creativity keeps the scene exciting and ensures thereâ€™s always something new to try.\"\"\"\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6262ff",
   "metadata": {},
   "source": [
    "\n",
    "## Length Based Splitter\n",
    "\n",
    "- Token-based: Splits text based on the number of tokens, which is useful when working with language models.\n",
    "- Character-based: Splits text based on the number of characters, which can be more consistent across different types of text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c4d29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:10:36.883551Z",
     "start_time": "2024-11-29T19:10:36.880422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3\n",
      "page_content='The world of street food is a vibrant tapestry of flavors and cultures. From sizzling skewers to spicy tacos, every corner of the globe has its own unique offerings. Vendors often  up shop in bustling markets or on busy street corners, attracting hungry passersby with mouthwatering aromas.'\n",
      "The world of street food is a vibrant tapestry of flavors and cultures. From sizzling skewers to spicy tacos, every corner of the globe has its own unique offerings. Vendors often  up shop in bustling markets or on busy street corners, attracting hungry passersby with mouthwatering aromas.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=20\n",
    ")\n",
    "\n",
    "\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "print(type(docs))\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "print(docs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51b117dd8269f2",
   "metadata": {},
   "source": [
    "## Text-strucutre based splitter\n",
    "\n",
    "- The RecursiveCharacterTextSplitter attempts to keep larger units (e.g., paragraphs) intact.\n",
    "- If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).\n",
    "- This process continues down to the word level if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56200d94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T19:10:39.281501Z",
     "start_time": "2024-11-29T19:10:39.277815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "8\n",
      "page_content='The world of street food is a vibrant tapestry of flavors and cultures. From sizzling skewers to spicy tacos, every corner of the globe has its own unique offerings. Vendors often  up shop in bustling'\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)\n",
    "\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "print(type(docs))\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3cad7a",
   "metadata": {},
   "source": [
    "### Code Splitter"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 15,
>>>>>>> 18cad64b2654d94d36db6c7f1ea7966b69decbf5
   "id": "8b892b75",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -------------- Imports --------------\n",
      "import os\n",
      "import shutil\n",
      "import tempfile\n",
      "import streamlit as st\n",
      "from dotenv import load_dotenv\n",
      "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
      "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from langchain_community.document_loaders import PyPDFLoader\n",
      "from langchain_community.vectorstores import Chroma\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "from langchain.chains import ConversationalRetrievalChain\n",
      "\n",
      "# Load environment variables\n",
      "load_dotenv()\n",
      "\n",
      "\n",
      "# ----------------------- App Configuration -----------------------\n",
      "def configure_page():\n",
      "    \"\"\"Configure the Streamlit page settings for the chat app.\"\"\"\n",
      "    # Set the page title, icon, layout, and initial sidebar state\n",
      "    st.set_page_config(\n",
      "        page_title=\"PDF Chat App\",\n",
      "        page_icon=\"ðŸ“š\",\n",
      "        layout=\"centered\",\n",
      "        initial_sidebar_state=\"expanded\",\n",
      "    )\n",
      "    st.title(\"ðŸ’¬ Chat with your PDF\")\n",
      "    with st.expander(\"Check State\"):\n",
      "        st.write(st.session_state)\n",
      "\n",
      "\n",
      "def initialize_session_state():\n",
      "    \"\"\"Initialize the session state variables for the chat app.\"\"\"\n",
      "    # Initialize messages history\n",
      "    if \"messages\" not in st.session_state:\n",
      "        st.session_state.messages = []\n",
      "    # Initialize conversation chain\n",
      "    if \"conversation\" not in st.session_state:\n",
      "        st.session_state.conversation = None\n",
      "    # Initialize PDF processing status\n",
      "    if \"pdf_processed\" not in st.session_state:\n",
      "        st.session_state.pdf_processed = None\n",
      "    # Initialize vector store persistence directory\n",
      "    if \"persist_directory\" not in st.session_state:\n",
      "        st.session_state.persist_directory = None\n",
      "    # Initialize default model\n",
      "    if \"model\" not in st.session_state:\n",
      "        st.session_state.model = \"llama3.2\"\n",
      "\n",
      "\n",
      "# ----------------------- Model Setup -----------------------\n",
      "@st.cache_resource\n",
      "def get_chat_model(model_name):\n",
      "    \"\"\"Get the chat model based on the selected model name.\"\"\"\n",
      "    if model_name == \"gpt-3.5-turbo\":\n",
      "        return ChatOpenAI(\n",
      "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
      "            model=model_name,\n",
      "            streaming=True,\n",
      "        )\n",
      "    return ChatOllama(model=model_name, streaming=True)\n",
      "\n",
      "\n",
      "@st.cache_resource\n",
      "def get_embeddings():\n",
      "    \"\"\"Get the embeddings model for processing the PDF.\"\"\"\n",
      "    return OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
      "\n",
      "\n",
      "# ----------------------- PDF Processing -----------------------\n",
      "def process_pdf(pdf_file):\n",
      "    \"\"\"Process the uploaded PDF file and create a vector store.\"\"\"\n",
      "    # Save the uploaded PDF to a temporary file\n",
      "    tmp_file_path = save_temp_pdf(pdf_file)\n",
      "    # Load documents from the temporary PDF file\n",
      "    documents = load_pdf(tmp_file_path)\n",
      "    # Split the loaded documents into chunks\n",
      "    chunks = split_pdf(documents)\n",
      "    # Create a directory for persisting the vector store\n",
      "    persist_directory = create_chroma_persist_directory()\n",
      "    # Create a vector store from the document chunks\n",
      "    vectorstore = create_vectorstore(chunks, persist_directory)\n",
      "    # Remove the temporary PDF file after processing\n",
      "    os.unlink(tmp_file_path)\n",
      "    return vectorstore\n",
      "\n",
      "\n",
      "def save_temp_pdf(pdf_file):\n",
      "    \"\"\"Save the uploaded PDF file to a temporary file and return the file path.\"\"\"\n",
      "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
      "        tmp_file.write(pdf_file.getvalue())\n",
      "        return tmp_file.name\n",
      "\n",
      "\n",
      "def load_pdf(tmp_file_path):\n",
      "    \"\"\"Load the PDF file using PyPDFLoader and return the documents.\"\"\"\n",
      "    loader = PyPDFLoader(tmp_file_path)\n",
      "    return loader.load()\n",
      "\n",
      "\n",
      "def split_pdf(documents):\n",
      "    \"\"\"Split the PDF documents into chunks for processing.\"\"\"\n",
      "    # [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
      "    text_splitter = RecursiveCharacterTextSplitter(\n",
      "        chunk_size=1000,\n",
      "        chunk_overlap=200,\n",
      "        length_function=len,\n",
      "    )\n",
      "    return text_splitter.split_documents(documents)\n",
      "\n",
      "\n",
      "def create_chroma_persist_directory():\n",
      "    \"\"\"Create a directory for persisting the Chroma vector store.\"\"\"\n",
      "    persist_directory = \"db\"\n",
      "    # Store the directory name in session state\n",
      "    st.session_state.persist_directory = persist_directory\n",
      "    return persist_directory\n",
      "\n",
      "\n",
      "def create_vectorstore(chunks, persist_directory):\n",
      "    \"\"\"Create a Chroma vector store from the document chunks.\"\"\"\n",
      "    embedding_model = get_embeddings()\n",
      "    # Create a Chroma vector store from the documents and embeddings\n",
      "    return Chroma.from_documents(\n",
      "        documents=chunks,\n",
      "        embedding=embedding_model,\n",
      "        persist_directory=persist_directory,\n",
      "    )\n",
      "\n",
      "\n",
      "# ----------------------- Chat Interface -----------------------\n",
      "def initialize_conversation(vectorstore, chat_model):\n",
      "    \"\"\"Initialize a conversational retrieval chain with the given vector store and chat model.\"\"\"\n",
      "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
      "\n",
      "    # Initialize a conversational retrieval chain with given parameters\n",
      "    return ConversationalRetrievalChain.from_llm(\n",
      "        llm=chat_model,\n",
      "        retriever=vectorstore.as_retriever(\n",
      "            search_kwargs={\"k\": 5}\n",
      "        ),\n",
      "        memory=memory,\n",
      "        verbose=True,\n",
      "    )\n",
      "\n",
      "\n",
      "def display_chat_messages():\n",
      "    \"\"\"Display the chat messages in the chat interface.\"\"\"\n",
      "    # Iterate through each message in the chat history\n",
      "    for message in st.session_state.messages:\n",
      "        # Check if the message is from the user\n",
      "        if isinstance(message, HumanMessage):\n",
      "            with st.chat_message(\"user\"):\n",
      "                st.write(message.content)\n",
      "        # Check if the message is from the assistant\n",
      "        elif isinstance(message, AIMessage):\n",
      "            with st.chat_message(\"assistant\"):\n",
      "                st.write(message.content)\n",
      "        # Check if the message is a system message\n",
      "        elif isinstance(message, SystemMessage):\n",
      "            with st.chat_message(\"system\"):\n",
      "                st.write(message.content)\n",
      "\n",
      "\n",
      "def handle_user_input(conversation):\n",
      "    \"\"\"Handle user input and chat interactions with the assistant.\"\"\"\n",
      "    # Get user input from the chat input widget\n",
      "    if prompt := st.chat_input(\"Ask questions about your PDF\"):\n",
      "        # Create a HumanMessage instance with the input prompt and append it to session state\n",
      "        user_message = HumanMessage(content=prompt)\n",
      "        st.session_state.messages.append(user_message)\n",
      "        # Display the user message in the chat\n",
      "        with st.chat_message(\"user\"):\n",
      "            st.write(prompt)\n",
      "        with st.chat_message(\"assistant\"):\n",
      "            message_placeholder = st.empty()\n",
      "            # Wrap the conversation logic in a try-except block\n",
      "            try:\n",
      "                # Get the response from the conversation model\n",
      "                response = conversation({\"question\": prompt})\n",
      "                answer = response.get(\"answer\", \"I'm not sure how to respond to that.\")\n",
      "            except Exception as e:\n",
      "                # Handle any exceptions and set answer to the error message\n",
      "                answer = f\"Error: {e}\"\n",
      "            message_placeholder.markdown(answer)\n",
      "            # Create an AIMessage instance with the answer and append it to session state\n",
      "            assistant_message = AIMessage(content=answer)\n",
      "            st.session_state.messages.append(assistant_message)\n",
      "\n",
      "\n",
      "# ----------------------- Sidebar Functions -----------------------\n",
      "def handle_sidebar():\n",
      "    \"\"\"Handle the sidebar interactions and return the selected model.\"\"\"\n",
      "    selected_model = st.sidebar.selectbox(\n",
      "        \"Select Model\", (\"llama3.2\", \"llama3.2:1b\", \"qwen2.5:0.5b\", \"gpt-3.5-turbo\")\n",
      "    )\n",
      "    st.session_state.model = selected_model\n",
      "    st.sidebar.divider()\n",
      "    if st.sidebar.button(\"Clear Chat\"):\n",
      "        clear_chat()\n",
      "    if st.sidebar.button(\"Clear Cache\"):\n",
      "        clear_cache()\n",
      "    st.sidebar.markdown(\"---\")\n",
      "    st.sidebar.markdown(\"### Model Information\")\n",
      "    st.sidebar.write(f\"Current Model: {selected_model}\")\n",
      "    return selected_model\n",
      "\n",
      "\n",
      "def clear_chat():\n",
      "    \"\"\"Clear the chat history and reset the conversation state.\"\"\"\n",
      "    st.session_state.messages = []\n",
      "    st.session_state.conversation = None\n",
      "    cleanup_chroma_db()\n",
      "    st.rerun()\n",
      "\n",
      "\n",
      "def clear_cache():\n",
      "    \"\"\"Clear the Streamlit cache to reset the application state.\"\"\"\n",
      "    st.cache_data.clear()\n",
      "    st.cache_resource.clear()\n",
      "\n",
      "\n",
      "def cleanup_chroma_db():\n",
      "    \"\"\"Clean up the Chroma database directory if it exists.\"\"\"\n",
      "    persist_directory = st.session_state.get(\"persist_directory\")\n",
      "\n",
      "    # Check if the directory exists and attempt to remove it\n",
      "    if persist_directory and os.path.exists(persist_directory):\n",
      "        try:\n",
      "            shutil.rmtree(persist_directory)\n",
      "            st.session_state.persist_directory = None\n",
      "        except Exception as e:\n",
      "            st.error(f\"Error cleaning up Chroma DB: {e}\")\n",
      "\n",
      "\n",
      "# ----------------------- PDF Upload Handler -----------------------\n",
      "def handle_pdf_upload(pdf_file, chat_model):\n",
      "    \"\"\"Handle the PDF upload and process the PDF to initialize the chat conversation.\"\"\"\n",
      "    # Check if the PDF has already been processed\n",
      "    if st.session_state.pdf_processed != pdf_file.name:\n",
      "        # Display a processing message while the PDF is being processed\n",
      "        with st.spinner(\"Processing PDF...\"):\n",
      "            # Clean up the previous Chroma database if it exists\n",
      "            cleanup_chroma_db()\n",
      "            # Process the uploaded PDF to create a vector store\n",
      "            vectorstore = process_pdf(pdf_file)\n",
      "            # Initialize a new conversation with the vector store and chat model\n",
      "            st.session_state.conversation = initialize_conversation(\n",
      "                vectorstore, chat_model\n",
      "            )\n",
      "            # Mark the PDF as processed in the session state\n",
      "            st.session_state.pdf_processed = pdf_file.name\n",
      "            # Reset the messages in the session state\n",
      "            st.session_state.messages = []\n",
      "            st.success(\"PDF processed successfully!\")\n",
      "    # Display chat messages if available\n",
      "    display_chat_messages()\n",
      "\n",
      "    # Handle user input if a conversation is initialized\n",
      "    if st.session_state.conversation:\n",
      "        handle_user_input(st.session_state.conversation)\n",
      "\n",
      "# ----------------------- Main Application -----------------------\n",
      "def main():\n",
      "    \"\"\"Main function to run the Streamlit app.\"\"\"\n",
      "    configure_page()\n",
      "    # Initialize the session state for maintaining chat state\n",
      "    initialize_session_state()\n",
      "    # Handle sidebar interactions and get the selected model\n",
      "    selected_model = handle_sidebar()\n",
      "    # Get the chat model based on the selected model from the sidebar\n",
      "    chat_model = get_chat_model(selected_model)\n",
      "\n",
      "    # File uploader for PDF files\n",
      "    pdf_file = st.file_uploader(\"Upload your PDF\", type=[\"pdf\"])\n",
      "\n",
      "    # If a PDF file is uploaded, handle processing and chat initialization\n",
      "    if pdf_file:\n",
      "        handle_pdf_upload(pdf_file, chat_model)\n",
      "    else:\n",
      "        st.info(\"Please upload a PDF file to start chatting.\")\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ]
=======
     "data": {
      "text/plain": [
       "['cpp',\n",
       " 'go',\n",
       " 'java',\n",
       " 'kotlin',\n",
       " 'js',\n",
       " 'ts',\n",
       " 'php',\n",
       " 'proto',\n",
       " 'python',\n",
       " 'rst',\n",
       " 'ruby',\n",
       " 'rust',\n",
       " 'scala',\n",
       " 'swift',\n",
       " 'markdown',\n",
       " 'latex',\n",
       " 'html',\n",
       " 'sol',\n",
       " 'csharp',\n",
       " 'cobol',\n",
       " 'c',\n",
       " 'lua',\n",
       " 'perl',\n",
       " 'haskell',\n",
       " 'elixir',\n",
       " 'powershell']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 18cad64b2654d94d36db6c7f1ea7966b69decbf5
    }
   ],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
<<<<<<< HEAD
    "[e.value for e in Language]\n",
=======
    "\n",
>>>>>>> 18cad64b2654d94d36db6c7f1ea7966b69decbf5
    "\n",
    "# Read python file\n",
    "\n",
    "with open(\"data/sample.py\", \"r\") as file:\n",
    "    code = file.read()\n",
<<<<<<< HEAD
    "    \n",
    "print(code)\n"
=======
    "[e.value for e in Language]\n",
    "# print(code)"
>>>>>>> 18cad64b2654d94d36db6c7f1ea7966b69decbf5
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 16,
>>>>>>> 18cad64b2654d94d36db6c7f1ea7966b69decbf5
   "id": "fd2453e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='# -------------- Imports --------------\\nimport os'),\n",
       " Document(metadata={}, page_content='import shutil\\nimport tempfile'),\n",
       " Document(metadata={}, page_content='import streamlit as st'),\n",
       " Document(metadata={}, page_content='from dotenv import load_dotenv'),\n",
       " Document(metadata={}, page_content='from langchain.schema import HumanMessage,'),\n",
       " Document(metadata={}, page_content='AIMessage, SystemMessage'),\n",
       " Document(metadata={}, page_content='from langchain_ollama import ChatOllama,'),\n",
       " Document(metadata={}, page_content='OllamaEmbeddings'),\n",
       " Document(metadata={}, page_content='from langchain_openai import ChatOpenAI'),\n",
       " Document(metadata={}, page_content='from langchain.text_splitter import'),\n",
       " Document(metadata={}, page_content='RecursiveCharacterTextSplitter'),\n",
       " Document(metadata={}, page_content='from langchain_community.document_loaders import'),\n",
       " Document(metadata={}, page_content='PyPDFLoader'),\n",
       " Document(metadata={}, page_content='from langchain_community.vectorstores import'),\n",
       " Document(metadata={}, page_content='Chroma'),\n",
       " Document(metadata={}, page_content='from langchain.memory import'),\n",
       " Document(metadata={}, page_content='ConversationBufferMemory'),\n",
       " Document(metadata={}, page_content='from langchain.chains import'),\n",
       " Document(metadata={}, page_content='ConversationalRetrievalChain'),\n",
       " Document(metadata={}, page_content='# Load environment variables\\nload_dotenv()'),\n",
       " Document(metadata={}, page_content='# ----------------------- App Configuration'),\n",
       " Document(metadata={}, page_content='-----------------------'),\n",
       " Document(metadata={}, page_content='def configure_page():'),\n",
       " Document(metadata={}, page_content='\"\"\"Configure the Streamlit page settings for'),\n",
       " Document(metadata={}, page_content='the chat app.\"\"\"'),\n",
       " Document(metadata={}, page_content='# Set the page title, icon, layout, and'),\n",
       " Document(metadata={}, page_content='initial sidebar state'),\n",
       " Document(metadata={}, page_content='st.set_page_config('),\n",
       " Document(metadata={}, page_content='page_title=\"PDF Chat App\",'),\n",
       " Document(metadata={}, page_content='page_icon=\"ðŸ“š\",\\n        layout=\"centered\",'),\n",
       " Document(metadata={}, page_content='initial_sidebar_state=\"expanded\",\\n    )'),\n",
       " Document(metadata={}, page_content='st.title(\"ðŸ’¬ Chat with your PDF\")'),\n",
       " Document(metadata={}, page_content='with st.expander(\"Check State\"):'),\n",
       " Document(metadata={}, page_content='st.write(st.session_state)'),\n",
       " Document(metadata={}, page_content='def initialize_session_state():'),\n",
       " Document(metadata={}, page_content='\"\"\"Initialize the session state variables for'),\n",
       " Document(metadata={}, page_content='the chat app.\"\"\"'),\n",
       " Document(metadata={}, page_content='# Initialize messages history'),\n",
       " Document(metadata={}, page_content='if \"messages\" not in st.session_state:'),\n",
       " Document(metadata={}, page_content='st.session_state.messages = []'),\n",
       " Document(metadata={}, page_content='# Initialize conversation chain'),\n",
       " Document(metadata={}, page_content='if \"conversation\" not in st.session_state:'),\n",
       " Document(metadata={}, page_content='st.session_state.conversation = None'),\n",
       " Document(metadata={}, page_content='# Initialize PDF processing status'),\n",
       " Document(metadata={}, page_content='if \"pdf_processed\" not in st.session_state:'),\n",
       " Document(metadata={}, page_content='st.session_state.pdf_processed = None'),\n",
       " Document(metadata={}, page_content='# Initialize vector store persistence'),\n",
       " Document(metadata={}, page_content='directory'),\n",
       " Document(metadata={}, page_content='if \"persist_directory\" not in'),\n",
       " Document(metadata={}, page_content='st.session_state:'),\n",
       " Document(metadata={}, page_content='st.session_state.persist_directory = None'),\n",
       " Document(metadata={}, page_content='# Initialize default model'),\n",
       " Document(metadata={}, page_content='if \"model\" not in st.session_state:'),\n",
       " Document(metadata={}, page_content='st.session_state.model = \"llama3.2\"'),\n",
       " Document(metadata={}, page_content='# ----------------------- Model Setup'),\n",
       " Document(metadata={}, page_content='-----------------------'),\n",
       " Document(metadata={}, page_content='@st.cache_resource'),\n",
       " Document(metadata={}, page_content='def get_chat_model(model_name):'),\n",
       " Document(metadata={}, page_content='\"\"\"Get the chat model based on the selected'),\n",
       " Document(metadata={}, page_content='model name.\"\"\"'),\n",
       " Document(metadata={}, page_content='if model_name == \"gpt-3.5-turbo\":'),\n",
       " Document(metadata={}, page_content='return ChatOpenAI('),\n",
       " Document(metadata={}, page_content='api_key=os.getenv(\"OPENAI_API_KEY\"),'),\n",
       " Document(metadata={}, page_content='model=model_name,'),\n",
       " Document(metadata={}, page_content='streaming=True,\\n        )'),\n",
       " Document(metadata={}, page_content='return ChatOllama(model=model_name,'),\n",
       " Document(metadata={}, page_content='streaming=True)'),\n",
       " Document(metadata={}, page_content='@st.cache_resource'),\n",
       " Document(metadata={}, page_content='def get_embeddings():'),\n",
       " Document(metadata={}, page_content='\"\"\"Get the embeddings model for processing'),\n",
       " Document(metadata={}, page_content='the PDF.\"\"\"'),\n",
       " Document(metadata={}, page_content='return'),\n",
       " Document(metadata={}, page_content='OllamaEmbeddings(model=\"mxbai-embed-large\")'),\n",
       " Document(metadata={}, page_content='# ----------------------- PDF Processing'),\n",
       " Document(metadata={}, page_content='-----------------------'),\n",
       " Document(metadata={}, page_content='def process_pdf(pdf_file):'),\n",
       " Document(metadata={}, page_content='\"\"\"Process the uploaded PDF file and create a'),\n",
       " Document(metadata={}, page_content='vector store.\"\"\"'),\n",
       " Document(metadata={}, page_content='# Save the uploaded PDF to a temporary file'),\n",
       " Document(metadata={}, page_content='tmp_file_path = save_temp_pdf(pdf_file)'),\n",
       " Document(metadata={}, page_content='# Load documents from the temporary PDF file'),\n",
       " Document(metadata={}, page_content='documents = load_pdf(tmp_file_path)'),\n",
       " Document(metadata={}, page_content='# Split the loaded documents into chunks'),\n",
       " Document(metadata={}, page_content='chunks = split_pdf(documents)'),\n",
       " Document(metadata={}, page_content='# Create a directory for persisting the'),\n",
       " Document(metadata={}, page_content='vector store'),\n",
       " Document(metadata={}, page_content='persist_directory ='),\n",
       " Document(metadata={}, page_content='create_chroma_persist_directory()'),\n",
       " Document(metadata={}, page_content='# Create a vector store from the document'),\n",
       " Document(metadata={}, page_content='chunks'),\n",
       " Document(metadata={}, page_content='vectorstore = create_vectorstore(chunks,'),\n",
       " Document(metadata={}, page_content='persist_directory)'),\n",
       " Document(metadata={}, page_content='# Remove the temporary PDF file after'),\n",
       " Document(metadata={}, page_content='processing'),\n",
       " Document(metadata={}, page_content='os.unlink(tmp_file_path)'),\n",
       " Document(metadata={}, page_content='return vectorstore'),\n",
       " Document(metadata={}, page_content='def save_temp_pdf(pdf_file):'),\n",
       " Document(metadata={}, page_content='\"\"\"Save the uploaded PDF file to a temporary'),\n",
       " Document(metadata={}, page_content='file and return the file path.\"\"\"'),\n",
       " Document(metadata={}, page_content='with'),\n",
       " Document(metadata={}, page_content='tempfile.NamedTemporaryFile(delete=False,'),\n",
       " Document(metadata={}, page_content='suffix=\".pdf\") as tmp_file:'),\n",
       " Document(metadata={}, page_content='tmp_file.write(pdf_file.getvalue())'),\n",
       " Document(metadata={}, page_content='return tmp_file.name'),\n",
       " Document(metadata={}, page_content='def load_pdf(tmp_file_path):'),\n",
       " Document(metadata={}, page_content='\"\"\"Load the PDF file using PyPDFLoader and'),\n",
       " Document(metadata={}, page_content='return the documents.\"\"\"'),\n",
       " Document(metadata={}, page_content='loader = PyPDFLoader(tmp_file_path)'),\n",
       " Document(metadata={}, page_content='return loader.load()'),\n",
       " Document(metadata={}, page_content='def split_pdf(documents):'),\n",
       " Document(metadata={}, page_content='\"\"\"Split the PDF documents into chunks for'),\n",
       " Document(metadata={}, page_content='processing.\"\"\"'),\n",
       " Document(metadata={}, page_content='# [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]'),\n",
       " Document(metadata={}, page_content='text_splitter ='),\n",
       " Document(metadata={}, page_content='RecursiveCharacterTextSplitter('),\n",
       " Document(metadata={}, page_content='chunk_size=1000,'),\n",
       " Document(metadata={}, page_content='chunk_overlap=200,'),\n",
       " Document(metadata={}, page_content='length_function=len,\\n    )'),\n",
       " Document(metadata={}, page_content='return'),\n",
       " Document(metadata={}, page_content='text_splitter.split_documents(documents)'),\n",
       " Document(metadata={}, page_content='def create_chroma_persist_directory():'),\n",
       " Document(metadata={}, page_content='\"\"\"Create a directory for persisting the'),\n",
       " Document(metadata={}, page_content='Chroma vector store.\"\"\"'),\n",
       " Document(metadata={}, page_content='persist_directory = \"db\"'),\n",
       " Document(metadata={}, page_content='# Store the directory name in session state'),\n",
       " Document(metadata={}, page_content='st.session_state.persist_directory ='),\n",
       " Document(metadata={}, page_content='persist_directory'),\n",
       " Document(metadata={}, page_content='return persist_directory'),\n",
       " Document(metadata={}, page_content='def create_vectorstore(chunks,'),\n",
       " Document(metadata={}, page_content='persist_directory):'),\n",
       " Document(metadata={}, page_content='\"\"\"Create a Chroma vector store from the'),\n",
       " Document(metadata={}, page_content='document chunks.\"\"\"'),\n",
       " Document(metadata={}, page_content='embedding_model = get_embeddings()'),\n",
       " Document(metadata={}, page_content='# Create a Chroma vector store from the'),\n",
       " Document(metadata={}, page_content='documents and embeddings'),\n",
       " Document(metadata={}, page_content='return Chroma.from_documents('),\n",
       " Document(metadata={}, page_content='documents=chunks,'),\n",
       " Document(metadata={}, page_content='embedding=embedding_model,'),\n",
       " Document(metadata={}, page_content='persist_directory=persist_directory,'),\n",
       " Document(metadata={}, page_content=')'),\n",
       " Document(metadata={}, page_content='# ----------------------- Chat Interface'),\n",
       " Document(metadata={}, page_content='-----------------------'),\n",
       " Document(metadata={}, page_content='def initialize_conversation(vectorstore,'),\n",
       " Document(metadata={}, page_content='chat_model):'),\n",
       " Document(metadata={}, page_content='\"\"\"Initialize a conversational retrieval'),\n",
       " Document(metadata={}, page_content='chain with the given vector store and chat'),\n",
       " Document(metadata={}, page_content='model.\"\"\"'),\n",
       " Document(metadata={}, page_content='memory ='),\n",
       " Document(metadata={}, page_content='ConversationBufferMemory(memory_key=\"chat_history'),\n",
       " Document(metadata={}, page_content='\",'),\n",
       " Document(metadata={}, page_content='return_messages=True)'),\n",
       " Document(metadata={}, page_content='# Initialize a conversational retrieval chain'),\n",
       " Document(metadata={}, page_content='with given parameters'),\n",
       " Document(metadata={}, page_content='return ConversationalRetrievalChain.from_llm('),\n",
       " Document(metadata={}, page_content='llm=chat_model,'),\n",
       " Document(metadata={}, page_content='retriever=vectorstore.as_retriever('),\n",
       " Document(metadata={}, page_content='search_kwargs={\"k\": 5}\\n        ),'),\n",
       " Document(metadata={}, page_content='memory=memory,\\n        verbose=True,'),\n",
       " Document(metadata={}, page_content=')'),\n",
       " Document(metadata={}, page_content='def display_chat_messages():'),\n",
       " Document(metadata={}, page_content='\"\"\"Display the chat messages in the chat'),\n",
       " Document(metadata={}, page_content='interface.\"\"\"'),\n",
       " Document(metadata={}, page_content='# Iterate through each message in the chat'),\n",
       " Document(metadata={}, page_content='history'),\n",
       " Document(metadata={}, page_content='for message in st.session_state.messages:'),\n",
       " Document(metadata={}, page_content='# Check if the message is from the user'),\n",
       " Document(metadata={}, page_content='if isinstance(message, HumanMessage):'),\n",
       " Document(metadata={}, page_content='with st.chat_message(\"user\"):'),\n",
       " Document(metadata={}, page_content='st.write(message.content)'),\n",
       " Document(metadata={}, page_content='# Check if the message is from the'),\n",
       " Document(metadata={}, page_content='assistant'),\n",
       " Document(metadata={}, page_content='elif isinstance(message, AIMessage):'),\n",
       " Document(metadata={}, page_content='with st.chat_message(\"assistant\"):'),\n",
       " Document(metadata={}, page_content='st.write(message.content)'),\n",
       " Document(metadata={}, page_content='# Check if the message is a system'),\n",
       " Document(metadata={}, page_content='message'),\n",
       " Document(metadata={}, page_content='elif isinstance(message, SystemMessage):'),\n",
       " Document(metadata={}, page_content='with st.chat_message(\"system\"):'),\n",
       " Document(metadata={}, page_content='st.write(message.content)'),\n",
       " Document(metadata={}, page_content='def handle_user_input(conversation):'),\n",
       " Document(metadata={}, page_content='\"\"\"Handle user input and chat interactions'),\n",
       " Document(metadata={}, page_content='with the assistant.\"\"\"'),\n",
       " Document(metadata={}, page_content='# Get user input from the chat input widget'),\n",
       " Document(metadata={}, page_content='if prompt := st.chat_input(\"Ask questions'),\n",
       " Document(metadata={}, page_content='about your PDF\"):'),\n",
       " Document(metadata={}, page_content='# Create a HumanMessage instance with the'),\n",
       " Document(metadata={}, page_content='input prompt and append it to session state'),\n",
       " Document(metadata={}, page_content='user_message ='),\n",
       " Document(metadata={}, page_content='HumanMessage(content=prompt)'),\n",
       " Document(metadata={}, page_content='st.session_state.messages.append(user_message)'),\n",
       " Document(metadata={}, page_content='# Display the user message in the chat'),\n",
       " Document(metadata={}, page_content='with st.chat_message(\"user\"):'),\n",
       " Document(metadata={}, page_content='st.write(prompt)'),\n",
       " Document(metadata={}, page_content='with st.chat_message(\"assistant\"):'),\n",
       " Document(metadata={}, page_content='message_placeholder = st.empty()'),\n",
       " Document(metadata={}, page_content='# Wrap the conversation logic in a'),\n",
       " Document(metadata={}, page_content='try-except block'),\n",
       " Document(metadata={}, page_content='try:'),\n",
       " Document(metadata={}, page_content='# Get the response from the'),\n",
       " Document(metadata={}, page_content='conversation model'),\n",
       " Document(metadata={}, page_content='response ='),\n",
       " Document(metadata={}, page_content='conversation({\"question\": prompt})'),\n",
       " Document(metadata={}, page_content='answer = response.get(\"answer\",'),\n",
       " Document(metadata={}, page_content='\"I\\'m not sure how to respond to that.\")'),\n",
       " Document(metadata={}, page_content='except Exception as e:'),\n",
       " Document(metadata={}, page_content='# Handle any exceptions and set'),\n",
       " Document(metadata={}, page_content='answer to the error message'),\n",
       " Document(metadata={}, page_content='answer = f\"Error: {e}\"'),\n",
       " Document(metadata={}, page_content='message_placeholder.markdown(answer)'),\n",
       " Document(metadata={}, page_content='# Create an AIMessage instance with'),\n",
       " Document(metadata={}, page_content='the answer and append it to session state'),\n",
       " Document(metadata={}, page_content='assistant_message ='),\n",
       " Document(metadata={}, page_content='AIMessage(content=answer)'),\n",
       " Document(metadata={}, page_content='st.session_state.messages.append(assistant_messag'),\n",
       " Document(metadata={}, page_content='e)'),\n",
       " Document(metadata={}, page_content='# ----------------------- Sidebar Functions'),\n",
       " Document(metadata={}, page_content='-----------------------'),\n",
       " Document(metadata={}, page_content='def handle_sidebar():'),\n",
       " Document(metadata={}, page_content='\"\"\"Handle the sidebar interactions and return'),\n",
       " Document(metadata={}, page_content='the selected model.\"\"\"'),\n",
       " Document(metadata={}, page_content='selected_model = st.sidebar.selectbox('),\n",
       " Document(metadata={}, page_content='\"Select Model\", (\"llama3.2\",'),\n",
       " Document(metadata={}, page_content='\"llama3.2:1b\", \"qwen2.5:0.5b\", \"gpt-3.5-turbo\")'),\n",
       " Document(metadata={}, page_content=')\\n    st.session_state.model = selected_model'),\n",
       " Document(metadata={}, page_content='st.sidebar.divider()'),\n",
       " Document(metadata={}, page_content='if st.sidebar.button(\"Clear Chat\"):'),\n",
       " Document(metadata={}, page_content='clear_chat()'),\n",
       " Document(metadata={}, page_content='if st.sidebar.button(\"Clear Cache\"):'),\n",
       " Document(metadata={}, page_content='clear_cache()'),\n",
       " Document(metadata={}, page_content='st.sidebar.markdown(\"---\")'),\n",
       " Document(metadata={}, page_content='st.sidebar.markdown(\"### Model Information\")'),\n",
       " Document(metadata={}, page_content='st.sidebar.write(f\"Current Model:'),\n",
       " Document(metadata={}, page_content='{selected_model}\")'),\n",
       " Document(metadata={}, page_content='return selected_model'),\n",
       " Document(metadata={}, page_content='def clear_chat():'),\n",
       " Document(metadata={}, page_content='\"\"\"Clear the chat history and reset the'),\n",
       " Document(metadata={}, page_content='conversation state.\"\"\"'),\n",
       " Document(metadata={}, page_content='st.session_state.messages = []'),\n",
       " Document(metadata={}, page_content='st.session_state.conversation = None'),\n",
       " Document(metadata={}, page_content='cleanup_chroma_db()\\n    st.rerun()'),\n",
       " Document(metadata={}, page_content='def clear_cache():'),\n",
       " Document(metadata={}, page_content='\"\"\"Clear the Streamlit cache to reset the'),\n",
       " Document(metadata={}, page_content='application state.\"\"\"'),\n",
       " Document(metadata={}, page_content='st.cache_data.clear()'),\n",
       " Document(metadata={}, page_content='st.cache_resource.clear()'),\n",
       " Document(metadata={}, page_content='def cleanup_chroma_db():'),\n",
       " Document(metadata={}, page_content='\"\"\"Clean up the Chroma database directory if'),\n",
       " Document(metadata={}, page_content='it exists.\"\"\"'),\n",
       " Document(metadata={}, page_content='persist_directory ='),\n",
       " Document(metadata={}, page_content='st.session_state.get(\"persist_directory\")'),\n",
       " Document(metadata={}, page_content='# Check if the directory exists and attempt'),\n",
       " Document(metadata={}, page_content='to remove it'),\n",
       " Document(metadata={}, page_content='if persist_directory and'),\n",
       " Document(metadata={}, page_content='os.path.exists(persist_directory):'),\n",
       " Document(metadata={}, page_content='try:'),\n",
       " Document(metadata={}, page_content='shutil.rmtree(persist_directory)'),\n",
       " Document(metadata={}, page_content='st.session_state.persist_directory ='),\n",
       " Document(metadata={}, page_content='None'),\n",
       " Document(metadata={}, page_content='except Exception as e:'),\n",
       " Document(metadata={}, page_content='st.error(f\"Error cleaning up Chroma'),\n",
       " Document(metadata={}, page_content='DB: {e}\")'),\n",
       " Document(metadata={}, page_content='# ----------------------- PDF Upload Handler'),\n",
       " Document(metadata={}, page_content='-----------------------'),\n",
       " Document(metadata={}, page_content='def handle_pdf_upload(pdf_file, chat_model):'),\n",
       " Document(metadata={}, page_content='\"\"\"Handle the PDF upload and process the PDF'),\n",
       " Document(metadata={}, page_content='to initialize the chat conversation.\"\"\"'),\n",
       " Document(metadata={}, page_content='# Check if the PDF has already been processed'),\n",
       " Document(metadata={}, page_content='if st.session_state.pdf_processed !='),\n",
       " Document(metadata={}, page_content='pdf_file.name:'),\n",
       " Document(metadata={}, page_content='# Display a processing message while the'),\n",
       " Document(metadata={}, page_content='PDF is being processed'),\n",
       " Document(metadata={}, page_content='with st.spinner(\"Processing PDF...\"):'),\n",
       " Document(metadata={}, page_content='# Clean up the previous Chroma'),\n",
       " Document(metadata={}, page_content='database if it exists'),\n",
       " Document(metadata={}, page_content='cleanup_chroma_db()'),\n",
       " Document(metadata={}, page_content='# Process the uploaded PDF to create'),\n",
       " Document(metadata={}, page_content='a vector store'),\n",
       " Document(metadata={}, page_content='vectorstore = process_pdf(pdf_file)'),\n",
       " Document(metadata={}, page_content='# Initialize a new conversation with'),\n",
       " Document(metadata={}, page_content='the vector store and chat model'),\n",
       " Document(metadata={}, page_content='st.session_state.conversation ='),\n",
       " Document(metadata={}, page_content='initialize_conversation('),\n",
       " Document(metadata={}, page_content='vectorstore, chat_model'),\n",
       " Document(metadata={}, page_content=')'),\n",
       " Document(metadata={}, page_content='# Mark the PDF as processed in the'),\n",
       " Document(metadata={}, page_content='session state'),\n",
       " Document(metadata={}, page_content='st.session_state.pdf_processed ='),\n",
       " Document(metadata={}, page_content='pdf_file.name'),\n",
       " Document(metadata={}, page_content='# Reset the messages in the session'),\n",
       " Document(metadata={}, page_content='state'),\n",
       " Document(metadata={}, page_content='st.session_state.messages = []'),\n",
       " Document(metadata={}, page_content='st.success(\"PDF processed'),\n",
       " Document(metadata={}, page_content='successfully!\")'),\n",
       " Document(metadata={}, page_content='# Display chat messages if available'),\n",
       " Document(metadata={}, page_content='display_chat_messages()'),\n",
       " Document(metadata={}, page_content='# Handle user input if a conversation is'),\n",
       " Document(metadata={}, page_content='initialized'),\n",
       " Document(metadata={}, page_content='if st.session_state.conversation:'),\n",
       " Document(metadata={}, page_content='handle_user_input(st.session_state.conversation)'),\n",
       " Document(metadata={}, page_content='# ----------------------- Main Application'),\n",
       " Document(metadata={}, page_content='-----------------------'),\n",
       " Document(metadata={}, page_content='def main():'),\n",
       " Document(metadata={}, page_content='\"\"\"Main function to run the Streamlit app.\"\"\"'),\n",
       " Document(metadata={}, page_content='configure_page()'),\n",
       " Document(metadata={}, page_content='# Initialize the session state for'),\n",
       " Document(metadata={}, page_content='maintaining chat state'),\n",
       " Document(metadata={}, page_content='initialize_session_state()'),\n",
       " Document(metadata={}, page_content='# Handle sidebar interactions and get the'),\n",
       " Document(metadata={}, page_content='selected model'),\n",
       " Document(metadata={}, page_content='selected_model = handle_sidebar()'),\n",
       " Document(metadata={}, page_content='# Get the chat model based on the selected'),\n",
       " Document(metadata={}, page_content='model from the sidebar'),\n",
       " Document(metadata={}, page_content='chat_model = get_chat_model(selected_model)'),\n",
       " Document(metadata={}, page_content='# File uploader for PDF files'),\n",
       " Document(metadata={}, page_content='pdf_file = st.file_uploader(\"Upload your'),\n",
       " Document(metadata={}, page_content='PDF\", type=[\"pdf\"])'),\n",
       " Document(metadata={}, page_content='# If a PDF file is uploaded, handle'),\n",
       " Document(metadata={}, page_content='processing and chat initialization'),\n",
       " Document(metadata={}, page_content='if pdf_file:'),\n",
       " Document(metadata={}, page_content='handle_pdf_upload(pdf_file, chat_model)'),\n",
       " Document(metadata={}, page_content='else:'),\n",
       " Document(metadata={}, page_content='st.info(\"Please upload a PDF file to'),\n",
       " Document(metadata={}, page_content='start chatting.\")'),\n",
       " Document(metadata={}, page_content='if __name__ == \"__main__\":\\n    main()')]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 5,
=======
     "execution_count": 16,
>>>>>>> 18cad64b2654d94d36db6c7f1ea7966b69decbf5
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "from sympy import python\n",
    "\n",
    "\n",
=======
>>>>>>> 18cad64b2654d94d36db6c7f1ea7966b69decbf5
    "python_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=50, chunk_overlap=0)\n",
    "\n",
    "python_docs = python_splitter.create_documents([code])\n",
    "print(len(python_docs))\n",
    "python_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061316f",
   "metadata": {},
   "source": [
    "## Document-structured based\n",
    "- Preserves the logical organization of the document\n",
    "- Maintains context within each chunk\n",
    "- Can be more effective for downstream tasks like retrieval or summarization\n",
    "\n",
    "**Examples of structure-based splitting:**\n",
    "- Markdown: Split based on headers (e.g., #, ##, ###)\n",
    "- HTML: Split using tags\n",
    "- JSON: Split by object or array elements\n",
    "- Code: Split by functions, classes, or logical blocks\n",
    "\n",
    "### Markdown Splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3ffab3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Title of the document\\n\\nThis is a sample document. It is written in markdown format.\\n\\n## Section 1\\n\\nThis is the first section of the document.\\n\\n### Subsection 1.1\\n\\nThis is the first subsection of the document.\\n\\n### Subsection 1.2\\n\\nThis is the second subsection of the document.\\n\\n## Section 2\\n\\nThis is the second section of the document.\\n\\n\\n## Section 3\\n\\nThis is the third section of the document.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "# read markdown file\n",
    "with open(\"data/sample.md\", \"r\") as f:\n",
    "    md_text = f.read()\n",
    "    \n",
    "md_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Title of the document'}, page_content='# Title of the document  \\nThis is a sample document. It is written in markdown format.'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1'}, page_content='## Section 1  \\nThis is the first section of the document.'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.1'}, page_content='### Subsection 1.1  \\nThis is the first subsection of the document.'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.2'}, page_content='### Subsection 1.2  \\nThis is the second subsection of the document.'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 2'}, page_content='## Section 2  \\nThis is the second section of the document.'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 3'}, page_content='## Section 3  \\nThis is the third section of the document.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "\n",
    "docs = markdown_splitter.split_text(md_text)\n",
    "\n",
    "print(type(docs))\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e53d5c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Title of the document'}, page_content='# Title of'),\n",
       " Document(metadata={'Header 1': 'Title of the document'}, page_content='the'),\n",
       " Document(metadata={'Header 1': 'Title of the document'}, page_content='document'),\n",
       " Document(metadata={'Header 1': 'Title of the document'}, page_content='This is a'),\n",
       " Document(metadata={'Header 1': 'Title of the document'}, page_content='a sample'),\n",
       " Document(metadata={'Header 1': 'Title of the document'}, page_content='document.'),\n",
       " Document(metadata={'Header 1': 'Title of the document'}, page_content='It is'),\n",
       " Document(metadata={'Header 1': 'Title of the document'}, page_content='written'),\n",
       " Document(metadata={'Header 1': 'Title of the document'}, page_content='in'),\n",
       " Document(metadata={'Header 1': 'Title of the document'}, page_content='markdown'),\n",
       " Document(metadata={'Header 1': 'Title of the document'}, page_content='format.'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1'}, page_content='## Section'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1'}, page_content='1'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1'}, page_content='This is'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1'}, page_content='the first'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1'}, page_content='section'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1'}, page_content='of the'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1'}, page_content='document.'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.1'}, page_content='###'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.1'}, page_content='Subsectio'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.1'}, page_content='ion'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.1'}, page_content='1.1'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.1'}, page_content='This is'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.1'}, page_content='the first'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.1'}, page_content='subsectio'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.1'}, page_content='ion'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.1'}, page_content='of the'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.1'}, page_content='document.'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.2'}, page_content='###'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.2'}, page_content='Subsectio'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.2'}, page_content='ion'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.2'}, page_content='1.2'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.2'}, page_content='This is'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.2'}, page_content='the'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.2'}, page_content='second'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.2'}, page_content='subsectio'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.2'}, page_content='ion'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.2'}, page_content='of the'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 1', 'Header 3': 'Subsection 1.2'}, page_content='document.'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 2'}, page_content='## Section'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 2'}, page_content='2'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 2'}, page_content='This is'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 2'}, page_content='the'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 2'}, page_content='second'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 2'}, page_content='section'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 2'}, page_content='of the'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 2'}, page_content='document.'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 3'}, page_content='## Section'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 3'}, page_content='3'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 3'}, page_content='This is'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 3'}, page_content='the third'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 3'}, page_content='section'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 3'}, page_content='of the'),\n",
       " Document(metadata={'Header 1': 'Title of the document', 'Header 2': 'Section 3'}, page_content='document.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Char-level splits\n",
    "from attr import s\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 10\n",
    "chunk_overlap = 2\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0eb5d9",
   "metadata": {},
   "source": [
    "### HTML Splitter\n",
    "#### Header Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8924c0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Foo'),\n",
       " Document(metadata={'Header 1': 'Foo'}, page_content='Some intro text about Foo.  \\nBar main section Bar subsection 1 Bar subsection 2'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section'}, page_content='Some intro text about Bar.'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}, page_content='Some text about the first subtopic of Bar.'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}, page_content='Some text about the second subtopic of Bar.'),\n",
       " Document(metadata={'Header 1': 'Foo'}, page_content='Baz'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Baz'}, page_content='Some text about Baz'),\n",
       " Document(metadata={'Header 1': 'Foo'}, page_content='Some concluding text about Foo')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <body>\n",
    "        <div>\n",
    "            <h1>Foo</h1>\n",
    "            <p>Some intro text about Foo.</p>\n",
    "            <div>\n",
    "                <h2>Bar main section</h2>\n",
    "                <p>Some intro text about Bar.</p>\n",
    "                <h3>Bar subsection 1</h3>\n",
    "                <p>Some text about the first subtopic of Bar.</p>\n",
    "                <h3>Bar subsection 2</h3>\n",
    "                <p>Some text about the second subtopic of Bar.</p>\n",
    "            </div>\n",
    "            <div>\n",
    "                <h2>Baz</h2>\n",
    "                <p>Some text about Baz</p>\n",
    "            </div>\n",
    "            <br>\n",
    "            <p>Some concluding text about Foo</p>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c3e1b1",
   "metadata": {},
   "source": [
    "#### Section Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea7c3941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Foo'}, page_content='Foo \\n Some intro text about Foo.'),\n",
       " Document(metadata={'Header 2': 'Bar main section'}, page_content='Bar main section \\n Some intro text about Bar. \\n Bar subsection 1 \\n Some text about the first subtopic of Bar. \\n Bar subsection 2 \\n Some text about the second subtopic of Bar.'),\n",
       " Document(metadata={'Header 2': 'Baz'}, page_content='Baz \\n Some text about Baz \\n \\n \\n Some concluding text about Foo')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLSectionSplitter\n",
    "\n",
    "\n",
    "\n",
    "headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")]\n",
    "\n",
    "html_splitter = HTMLSectionSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad26a53",
   "metadata": {},
   "source": [
    "## Semantic Meaning Based\n",
    "\n",
    "- Start with the first few sentences and generate an embedding.\n",
    "- Move to the next group of sentences and generate another embedding (e.g., using a sliding window approach).\n",
    "- Compare the embeddings to find significant differences, which indicate potential \"break points\" between semantic sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f70fa617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='The world of street food is a vibrant tapestry of flavors and cultures. From sizzling skewers to spicy tacos, every corner of the globe has its own unique offerings. Vendors often  up shop in bustling markets or on busy street corners, attracting hungry passersby with mouthwatering aromas. One of the best things about street food is its accessibility. Itâ€™s quick, affordable, and often made fresh right in front of you. Whether itâ€™s a steaming bowl of pho in Vietnam or a crispy samosa in India, thereâ€™s something for everyone to enjoy. Street food also brings people together. Friends and families gather around food stalls, sharing dishes and stories. Itâ€™s a social experience that transcends language and culture, creating connections over a shared love of good eats. Finally, street food is constantly evolving. Chefs are experimenting with traditional recipes, adding modern twists and fusion flavors.'),\n",
       " Document(metadata={}, page_content='This creativity keeps the scene exciting and ensures thereâ€™s always something new to try.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "# embeddings: Embeddings,\n",
    "# buffer_size: int = 1,\n",
    "# add_start_index: bool = False,\n",
    "# breakpoint_threshold_type: BreakpointThresholdType = \"percentile\",\n",
    "# breakpoint_threshold_amount: Optional[float] = None,\n",
    "# number_of_chunks: Optional[int] = None,\n",
    "# sentence_split_regex: str = r\"(?<=[.?!])\\s+\",\n",
    "# min_chunk_size: Optional[int] = None,\n",
    "text_splitter = SemanticChunker(embedding_model)\n",
    "\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "print(type(docs))\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4611939",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
