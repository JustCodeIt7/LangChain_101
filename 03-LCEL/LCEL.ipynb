{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e308526",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL) Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e819ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Import LangChain components\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_ollama import ChatOllama\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f519c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOllama(model=\"llama3.2\",temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c735a",
   "metadata": {},
   "source": [
    "## SECTION 1: Basic LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30b7ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">-------------------------------------------------- \n",
       " DEMO <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: Basic LCEL Chain \n",
       " --------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "-------------------------------------------------- \n",
       " DEMO \u001b[1;36m1\u001b[0m: Basic LCEL Chain \n",
       " --------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Prompt: \n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Tell me a short joke about cats.'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{})]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Prompt: \n",
       "\u001b[1m[\u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'Tell me a short joke about cats.'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Tell me a short joke about {topic}.'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOllama</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3</span><span style=\"font-weight: bold\">)]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StrOutputParser</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfirst\u001b[0m=\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mtemplate\u001b[0m=\u001b[32m'Tell me a short joke about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m.'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mChatOllama\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2'\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mlast\u001b[0m=\u001b[1;35mStrOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chain:  Why did the cat join a band?\n",
       "\n",
       "Because it wanted to be the purr-cussionist!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chain:  Why did the cat join a band?\n",
       "\n",
       "Because it wanted to be the purr-cussionist!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"-\" * 50,'\\n',\"DEMO 1: Basic LCEL Chain\",'\\n',\"-\" * 50)\n",
    "\n",
    "# Create a simple prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}.\")\n",
    "\n",
    "# Create a basic chain using the | operator (pipe)\n",
    "basic_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "print(\"Prompt: \", prompt.format_prompt(topic=\"cats\").to_messages())\n",
    "print(basic_chain)\n",
    "print(\"Chain: \", basic_chain.invoke({\"topic\": \"cats\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ca3c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Why do programmers prefer dark mode?\n",
       "\n",
       "Because light attracts bugs.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Why do programmers prefer dark mode?\n",
       "\n",
       "Because light attracts bugs.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the chain\n",
    "result = basic_chain.invoke({\"topic\": \"programming\"})\n",
    "print(result)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e14221",
   "metadata": {},
   "source": [
    "## SECTION 2: Parallel Processing with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73272e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">-------------------------------------------------- \n",
       " DEMO <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: Parallel Processing with LCEL \n",
       " --------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "-------------------------------------------------- \n",
       " DEMO \u001b[1;36m2\u001b[0m: Parallel Processing with LCEL \n",
       " --------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Prompt: \n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'List 3 pros of AI technology.'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{})]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Prompt: \n",
       "\u001b[1m[\u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'List 3 pros of AI technology.'\u001b[0m, \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chain: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableParallel</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">steps__</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'pros'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'technology'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'technology'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'List 3 pros of {technology} technology.'</span>\n",
       "                        <span style=\"font-weight: bold\">)</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "                    <span style=\"font-weight: bold\">)</span>\n",
       "                <span style=\"font-weight: bold\">]</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOllama</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3</span><span style=\"font-weight: bold\">)]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StrOutputParser</span><span style=\"font-weight: bold\">()</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'cons'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'technology'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'technology'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'List 3 cons of {technology} technology.'</span>\n",
       "                        <span style=\"font-weight: bold\">)</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "                    <span style=\"font-weight: bold\">)</span>\n",
       "                <span style=\"font-weight: bold\">]</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOllama</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3</span><span style=\"font-weight: bold\">)]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StrOutputParser</span><span style=\"font-weight: bold\">()</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chain: \n",
       "\u001b[1;35mRunnableParallel\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33msteps__\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'pros'\u001b[0m: \u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mfirst\u001b[0m=\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'technology'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                    \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                        \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                            \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'technology'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                            \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                            \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                            \u001b[33mtemplate\u001b[0m=\u001b[32m'List 3 pros of \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtechnology\u001b[0m\u001b[32m}\u001b[0m\u001b[32m technology.'\u001b[0m\n",
       "                        \u001b[1m)\u001b[0m,\n",
       "                        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "                    \u001b[1m)\u001b[0m\n",
       "                \u001b[1m]\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mChatOllama\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2'\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mlast\u001b[0m=\u001b[1;35mStrOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'cons'\u001b[0m: \u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mfirst\u001b[0m=\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'technology'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                    \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                        \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                            \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'technology'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                            \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                            \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                            \u001b[33mtemplate\u001b[0m=\u001b[32m'List 3 cons of \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtechnology\u001b[0m\u001b[32m}\u001b[0m\u001b[32m technology.'\u001b[0m\n",
       "                        \u001b[1m)\u001b[0m,\n",
       "                        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "                    \u001b[1m)\u001b[0m\n",
       "                \u001b[1m]\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mChatOllama\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2'\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mlast\u001b[0m=\u001b[1;35mStrOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"-\" * 50,'\\n',\"DEMO 2: Parallel Processing with LCEL\",'\\n',\"-\" * 50)\n",
    "\n",
    "# Create two different prompts\n",
    "pros_prompt = ChatPromptTemplate.from_template(\n",
    "    \"List 3 pros of {technology} technology.\"\n",
    ")\n",
    "cons_prompt = ChatPromptTemplate.from_template(\n",
    "    \"List 3 cons of {technology} technology.\"\n",
    ")\n",
    "\n",
    "# Create chains for each prompt\n",
    "pros_chain = pros_prompt | llm | StrOutputParser()\n",
    "cons_chain = cons_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Create a parallel chain\n",
    "parallel_chain = RunnableParallel(pros=pros_chain, cons=cons_chain)\n",
    "\n",
    "print(\"Prompt: \", pros_prompt.format_prompt(technology=\"AI\").to_messages())\n",
    "print(\"Chain: \", parallel_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d10310d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PROS:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PROS:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here are three pros of blockchain technology:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Security and Transparency**: Blockchain technology uses a decentralized ledger system that is resistant to \n",
       "tampering, hacking, and manipulation. All transactions on the blockchain are recorded in a public ledger, making it\n",
       "transparent and secure.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Immutable and Tamper-Proof**: The blockchain's decentralized nature and use of cryptography make it virtually \n",
       "impossible to alter or manipulate transactions once they have been recorded. This ensures that data is accurate, \n",
       "reliable, and tamper-proof.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Decentralized and Autonomous**: Blockchain technology allows for a decentralized network of nodes to validate \n",
       "and record transactions, making it an autonomous system that operates independently of central authorities. This \n",
       "reduces the need for intermediaries and can lead to faster transaction processing times and lower costs.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here are three pros of blockchain technology:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Security and Transparency**: Blockchain technology uses a decentralized ledger system that is resistant to \n",
       "tampering, hacking, and manipulation. All transactions on the blockchain are recorded in a public ledger, making it\n",
       "transparent and secure.\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. **Immutable and Tamper-Proof**: The blockchain's decentralized nature and use of cryptography make it virtually \n",
       "impossible to alter or manipulate transactions once they have been recorded. This ensures that data is accurate, \n",
       "reliable, and tamper-proof.\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. **Decentralized and Autonomous**: Blockchain technology allows for a decentralized network of nodes to validate \n",
       "and record transactions, making it an autonomous system that operates independently of central authorities. This \n",
       "reduces the need for intermediaries and can lead to faster transaction processing times and lower costs.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "CONS:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "CONS:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here are three cons of blockchain technology:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Scalability Issues**: Blockchain technology is still in its early stages, and one of the major limitations is \n",
       "its scalability. As the number of transactions increases, the blockchain can become slow and congested, leading to \n",
       "high transaction fees and long processing times.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Energy Consumption**: The process of mining cryptocurrency, which is a key component of many blockchain \n",
       "networks, requires significant amounts of energy. This has raised concerns about the environmental impact of \n",
       "blockchain technology, particularly for large-scale applications.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Regulatory Uncertainty**: Blockchain technology operates in a gray area between traditional financial \n",
       "regulations and new, emerging laws. This uncertainty can make it difficult for businesses to navigate regulatory \n",
       "requirements and can create risks for companies that use blockchain technology.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here are three cons of blockchain technology:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Scalability Issues**: Blockchain technology is still in its early stages, and one of the major limitations is \n",
       "its scalability. As the number of transactions increases, the blockchain can become slow and congested, leading to \n",
       "high transaction fees and long processing times.\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. **Energy Consumption**: The process of mining cryptocurrency, which is a key component of many blockchain \n",
       "networks, requires significant amounts of energy. This has raised concerns about the environmental impact of \n",
       "blockchain technology, particularly for large-scale applications.\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. **Regulatory Uncertainty**: Blockchain technology operates in a gray area between traditional financial \n",
       "regulations and new, emerging laws. This uncertainty can make it difficult for businesses to navigate regulatory \n",
       "requirements and can create risks for companies that use blockchain technology.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Run the parallel chain\n",
    "parallel_result = parallel_chain.invoke({\"technology\": \"blockchain\"})\n",
    "print(\"PROS:\")\n",
    "print(parallel_result[\"pros\"])\n",
    "print(\"\\nCONS:\")\n",
    "print(parallel_result[\"cons\"])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c8ffd",
   "metadata": {},
   "source": [
    "## SECTION 3: Advanced Chain with Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "792963b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">-------------------------------------------------- \n",
       " DEMO <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: Advanced Chain with Data Transformation \n",
       " --------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "-------------------------------------------------- \n",
       " DEMO \u001b[1;36m3\u001b[0m: Advanced Chain with Data Transformation \n",
       " --------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Prompt: \n",
       "<span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Answer the following query: EXPLAIN QUANTUM COMPUTING\\n\\n    The query has 3 words.\\n    Provide a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">detailed explanation.\\n    '</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Prompt: \n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'Answer the following query: EXPLAIN QUANTUM COMPUTING\\n\\n    The query has 3 words.\\n    Provide a\u001b[0m\n",
       "\u001b[32mdetailed explanation.\\n    '\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chain: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableLambda</span><span style=\"font-weight: bold\">(</span>preprocess_input<span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'word_count'</span><span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'word_count'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Answer the following query: {query}\\n\\n    The query has {word_count} words.\\n   </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Provide a detailed explanation.\\n    '</span>\n",
       "                    <span style=\"font-weight: bold\">)</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOllama</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StrOutputParser</span><span style=\"font-weight: bold\">()</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableLambda</span><span style=\"font-weight: bold\">(</span>postprocess_output<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chain: \n",
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfirst\u001b[0m=\u001b[1;35mRunnableLambda\u001b[0m\u001b[1m(\u001b[0mpreprocess_input\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'query'\u001b[0m, \u001b[32m'word_count'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'query'\u001b[0m, \u001b[32m'word_count'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                        \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                        \u001b[33mtemplate\u001b[0m=\u001b[32m'Answer the following query: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mquery\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\n    The query has \u001b[0m\u001b[32m{\u001b[0m\u001b[32mword_count\u001b[0m\u001b[32m}\u001b[0m\u001b[32m words.\\n   \u001b[0m\n",
       "\u001b[32mProvide a detailed explanation.\\n    '\u001b[0m\n",
       "                    \u001b[1m)\u001b[0m,\n",
       "                    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mChatOllama\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2'\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.3\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mStrOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mlast\u001b[0m=\u001b[1;35mRunnableLambda\u001b[0m\u001b[1m(\u001b[0mpostprocess_output\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"-\" * 50,'\\n',\"DEMO 3: Advanced Chain with Data Transformation\",'\\n',\"-\" * 50)\n",
    "\n",
    "# Define a function to process input\n",
    "def preprocess_input(input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Process the input data before sending to the LLM.\"\"\"\n",
    "    # Add a Word count, capitalize the query, etc.\n",
    "    processed_data = input_data.copy()\n",
    "    processed_data[\"query\"] = input_data[\"query\"].upper()\n",
    "    processed_data[\"word_count\"] = len(input_data[\"query\"].split())\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# Define a function to process output\n",
    "def postprocess_output(output: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process the output from the LLM.\"\"\"\n",
    "    lines = output.strip().split(\"\\n\")\n",
    "    return {\n",
    "        \"summary\": lines[0] if lines else \"\",\n",
    "        \"details\": lines[1:],\n",
    "        \"response_length\": len(output),\n",
    "    }\n",
    "\n",
    "\n",
    "# Create an advanced prompt\n",
    "advanced_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following query: {query}\n",
    "\n",
    "    The query has {word_count} words.\n",
    "    Provide a detailed explanation.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create an advanced chain with preprocessing and postprocessing\n",
    "advanced_chain = (\n",
    "    RunnableLambda(preprocess_input)\n",
    "    | advanced_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | RunnableLambda(postprocess_output)\n",
    ")\n",
    "\n",
    "# Preprocess the input data\n",
    "preprocessed_data = preprocess_input({\"query\": \"Explain quantum computing\"})\n",
    "\n",
    "print(\"Prompt: \", advanced_prompt.format_prompt(**preprocessed_data).to_messages())\n",
    "print(\"Chain: \", advanced_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d52229f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summary: LCE <span style=\"font-weight: bold\">(</span>Loss Constrained Embedding<span style=\"font-weight: bold\">)</span> is a technique used to improve Large Language Model <span style=\"font-weight: bold\">(</span>LLM<span style=\"font-weight: bold\">)</span> applications. \n",
       "Here's how it works:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Summary: LCE \u001b[1m(\u001b[0mLoss Constrained Embedding\u001b[1m)\u001b[0m is a technique used to improve Large Language Model \u001b[1m(\u001b[0mLLM\u001b[1m)\u001b[0m applications. \n",
       "Here's how it works:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Details:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Details:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- **What is Loss Constrained Embedding?**\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- **What is Loss Constrained Embedding?**\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- Loss Constrained Embedding is a method for training LLMs by constraining the loss function to a specific range or\n",
       "distribution. The goal of this technique is to prevent the model from overfitting to the training data and to \n",
       "improve its generalization performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- Loss Constrained Embedding is a method for training LLMs by constraining the loss function to a specific range or\n",
       "distribution. The goal of this technique is to prevent the model from overfitting to the training data and to \n",
       "improve its generalization performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- **How does LCE improve LLM applications?**\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- **How does LCE improve LLM applications?**\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- LCE improves LLM applications in several ways:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- LCE improves LLM applications in several ways:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Improved Generalization**: By constraining the loss function, LCE helps the model to generalize better to \n",
       "new, unseen data. This is because the model is not overfitting to the training data, but rather learning a more \n",
       "robust representation of the language.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- \u001b[1;36m1\u001b[0m. **Improved Generalization**: By constraining the loss function, LCE helps the model to generalize better to \n",
       "new, unseen data. This is because the model is not overfitting to the training data, but rather learning a more \n",
       "robust representation of the language.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Reduced Overfitting**: LCE reduces the risk of overfitting, which can lead to poor performance on test data.\n",
       "By constraining the loss function, the model is forced to learn a more generalizable representation of the \n",
       "language.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- \u001b[1;36m2\u001b[0m. **Reduced Overfitting**: LCE reduces the risk of overfitting, which can lead to poor performance on test data.\n",
       "By constraining the loss function, the model is forced to learn a more generalizable representation of the \n",
       "language.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Increased Robustness**: LCE improves the robustness of the model to different types of input data. This \n",
       "means that the model can handle noisy or out-of-distribution inputs better than models trained without LCE.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- \u001b[1;36m3\u001b[0m. **Increased Robustness**: LCE improves the robustness of the model to different types of input data. This \n",
       "means that the model can handle noisy or out-of-distribution inputs better than models trained without LCE.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Improved Performance on Out-of-Vocabulary Words**: LCE improves the performance of the model on \n",
       "out-of-vocabulary <span style=\"font-weight: bold\">(</span>OOV<span style=\"font-weight: bold\">)</span> words, which are words that are not present in the training data. This is because the model\n",
       "is learning a more generalizable representation of the language, rather than relying on memorization of specific \n",
       "words.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- \u001b[1;36m4\u001b[0m. **Improved Performance on Out-of-Vocabulary Words**: LCE improves the performance of the model on \n",
       "out-of-vocabulary \u001b[1m(\u001b[0mOOV\u001b[1m)\u001b[0m words, which are words that are not present in the training data. This is because the model\n",
       "is learning a more generalizable representation of the language, rather than relying on memorization of specific \n",
       "words.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. **Faster Convergence**: LCE can lead to faster convergence during training, as the model is able to learn a \n",
       "more stable and generalizable representation of the language.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- \u001b[1;36m5\u001b[0m. **Faster Convergence**: LCE can lead to faster convergence during training, as the model is able to learn a \n",
       "more stable and generalizable representation of the language.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- **How does LCE work?**\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- **How does LCE work?**\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- LCE works by adding a penalty term to the loss function that encourages the model to stay within a certain range \n",
       "or distribution. This penalty term is typically added to the cross-entropy loss function, which measures the \n",
       "difference between the predicted probabilities and the true labels.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- LCE works by adding a penalty term to the loss function that encourages the model to stay within a certain range \n",
       "or distribution. This penalty term is typically added to the cross-entropy loss function, which measures the \n",
       "difference between the predicted probabilities and the true labels.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- The penalty term can be designed in various ways, such as:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- The penalty term can be designed in various ways, such as:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- * **Softmax regularization**: adds a penalty term to the softmax output of the model\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- * **Softmax regularization**: adds a penalty term to the softmax output of the model\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- * **L1 or L2 regularization**: adds a penalty term to the weights of the model\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- * **L1 or L2 regularization**: adds a penalty term to the weights of the model\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- * **Dropout**: adds a penalty term to the activations of the model\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- * **Dropout**: adds a penalty term to the activations of the model\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- The choice of penalty term depends on the specific application and the desired behavior of the model.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- The choice of penalty term depends on the specific application and the desired behavior of the model.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- **Conclusion**\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- **Conclusion**\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- In conclusion, Loss Constrained Embedding is a technique that improves Large Language Model applications by \n",
       "constraining the loss function to a specific range or distribution. This leads to improved generalization, reduced \n",
       "overfitting, increased robustness, improved performance on out-of-vocabulary words, and faster convergence during \n",
       "training.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- In conclusion, Loss Constrained Embedding is a technique that improves Large Language Model applications by \n",
       "constraining the loss function to a specific range or distribution. This leads to improved generalization, reduced \n",
       "overfitting, increased robustness, improved performance on out-of-vocabulary words, and faster convergence during \n",
       "training.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Response Length: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2818</span> characters\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Response Length: \u001b[1;36m2818\u001b[0m characters\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the advanced chain\n",
    "advanced_result = advanced_chain.invoke(\n",
    "    {\"query\": \"how does LCEL improve LLM applications\"}\n",
    ")\n",
    "print(f\"Summary: {advanced_result['summary']}\")\n",
    "print(\"Details:\")\n",
    "for detail in advanced_result[\"details\"]:\n",
    "    if detail.strip():\n",
    "        print(f\"- {detail.strip()}\")\n",
    "print(f\"Response Length: {advanced_result['response_length']} characters\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd6b14",
   "metadata": {},
   "source": [
    "## SECTION 4: RAG Pattern with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38c5eb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">-------------------------------------------------- \n",
       " DEMO <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: RAG Pattern with LCEL <span style=\"font-weight: bold\">(</span>Simulated<span style=\"font-weight: bold\">)</span> \n",
       " --------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "-------------------------------------------------- \n",
       " DEMO \u001b[1;36m4\u001b[0m: RAG Pattern with LCEL \u001b[1m(\u001b[0mSimulated\u001b[1m)\u001b[0m \n",
       " --------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Prompt: \n",
       "<span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Answer the question based on the following context:\\n\\n    Context:\\n    context\\n\\n    Question: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">question\\n    '</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Prompt: \n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'Answer the question based on the following context:\\n\\n    Context:\\n    context\\n\\n    Question: \u001b[0m\n",
       "\u001b[32mquestion\\n    '\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Chain: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableParallel</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">steps__</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableLambda</span><span style=\"font-weight: bold\">(</span>simulate_retrieval<span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableLambda</span><span style=\"font-weight: bold\">(</span>format_docs<span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnablePassthrough</span><span style=\"font-weight: bold\">()</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span><span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Answer the question based on the following context:\\n\\n    Context:\\n    </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{context}\\n\\n    Question: {question}\\n    '</span>\n",
       "                    <span style=\"font-weight: bold\">)</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOllama</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StrOutputParser</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Chain: \n",
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfirst\u001b[0m=\u001b[1;35mRunnableParallel\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33msteps__\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'context'\u001b[0m: \u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mfirst\u001b[0m=\u001b[1;35mRunnableLambda\u001b[0m\u001b[1m(\u001b[0msimulate_retrieval\u001b[1m)\u001b[0m,\n",
       "                \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[33mlast\u001b[0m=\u001b[1;35mRunnableLambda\u001b[0m\u001b[1m(\u001b[0mformat_docs\u001b[1m)\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[32m'question'\u001b[0m: \u001b[1;35mRunnablePassthrough\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'context'\u001b[0m, \u001b[32m'question'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'context'\u001b[0m, \u001b[32m'question'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                        \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                        \u001b[33mtemplate\u001b[0m=\u001b[32m'Answer the question based on the following context:\\n\\n    Context:\\n    \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32mcontext\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\n    Question: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mquestion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    '\u001b[0m\n",
       "                    \u001b[1m)\u001b[0m,\n",
       "                    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mChatOllama\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2'\u001b[0m, \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.3\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mlast\u001b[0m=\u001b[1;35mStrOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"-\" * 50,'\\n',\"DEMO 4: RAG Pattern with LCEL (Simulated)\",'\\n',\"-\" * 50)\n",
    "\n",
    "\n",
    "# Simulate a retriever function\n",
    "def simulate_retrieval(query: str) -> List[str]:\n",
    "    \"\"\"Simulate document retrieval based on query.\"\"\"\n",
    "    # In a real application, this would query a vector database\n",
    "    documents = [\n",
    "        \"LCEL allows for declarative chain construction in LangChain.\",\n",
    "        \"LangChain Expression Language simplifies building complex LLM applications.\",\n",
    "        \"LCEL supports streaming, async operations, and parallel execution.\",\n",
    "    ]\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Create a RAG prompt\n",
    "rag_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based on the following context:\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create a RAG chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": RunnableLambda(simulate_retrieval) | RunnableLambda(format_docs),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"Prompt: \", rag_prompt.format_prompt(context=\"context\", question=\"question\").to_messages())\n",
    "print(\"Chain: \", rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c7c4dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Based on the given context, LCEL stands for LangChain Expression Language. It is a tool that allows for declarative\n",
       "chain construction in LangChain, which simplifies building complex Large Language Model <span style=\"font-weight: bold\">(</span>LLM<span style=\"font-weight: bold\">)</span> applications.\n",
       "\n",
       "LCEL is useful because it supports streaming, async operations, and parallel execution, making it easier to build \n",
       "and manage complex applications that require these capabilities.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Based on the given context, LCEL stands for LangChain Expression Language. It is a tool that allows for declarative\n",
       "chain construction in LangChain, which simplifies building complex Large Language Model \u001b[1m(\u001b[0mLLM\u001b[1m)\u001b[0m applications.\n",
       "\n",
       "LCEL is useful because it supports streaming, async operations, and parallel execution, making it easier to build \n",
       "and manage complex applications that require these capabilities.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the RAG chain\n",
    "rag_result = rag_chain.invoke(\"What is LCEL and why is it useful?\")\n",
    "print(rag_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b17cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
