{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL) Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james/miniconda3/envs/py312/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# For YouTube Educational Content\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "\n",
    "# Import LangChain components\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_ollama import ChatOllama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOllama(model=\"llama3.2\",temperature=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1: Basic LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      " DEMO 1: Basic LCEL Chain \n",
      " --------------------------------------------------\n",
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"-\" * 50,'\\n',\"DEMO 1: Basic LCEL Chain\",'\\n',\"-\" * 50)\n",
    "\n",
    "# Create a simple prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}.\")\n",
    "\n",
    "# Create a basic chain using the | operator (pipe)\n",
    "basic_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run the chain\n",
    "result = basic_chain.invoke({\"topic\": \"programming\"})\n",
    "print(result)\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      " DEMO 2: Parallel Processing with LCEL \n",
      " --------------------------------------------------\n",
      "PROS:\n",
      "Here are three pros of blockchain technology:\n",
      "\n",
      "1. **Security and Transparency**: Blockchain technology uses a decentralized, distributed ledger that records transactions across multiple computers. This makes it virtually impossible to alter or manipulate the data once it's been recorded, ensuring the integrity and security of the information.\n",
      "\n",
      "2. **Immutable and Tamper-Proof**: The use of cryptography and complex algorithms in blockchain technology ensures that all transactions are time-stamped and linked together, creating a permanent record that cannot be altered or deleted. This makes it an ideal solution for applications where data needs to be tamper-proof.\n",
      "\n",
      "3. **Decentralized and Autonomous**: Blockchain technology operates independently of central authorities, allowing for peer-to-peer transactions without the need for intermediaries like banks or governments. This decentralized nature enables greater autonomy and control over data ownership, making it a more democratic and inclusive solution for various industries and use cases.\n",
      "\n",
      "CONS:\n",
      "Here are three cons of blockchain technology:\n",
      "\n",
      "1. **Scalability Limitations**: One of the major limitations of blockchain technology is its scalability. While it can process a large number of transactions per second, it's still not as fast or efficient as traditional payment systems like credit cards or online banking. This can lead to slow transaction processing times and high fees for users.\n",
      "\n",
      "2. **Energy Consumption**: The process of validating transactions on a blockchain network requires significant computational power, which is often provided by powerful computers that consume large amounts of energy. This has led to concerns about the environmental impact of blockchain technology, particularly in the context of cryptocurrency mining.\n",
      "\n",
      "3. **Regulatory Uncertainty**: Blockchain technology operates outside of traditional regulatory frameworks, which can create uncertainty and confusion for governments, businesses, and individuals alike. This lack of clear guidelines and regulations can make it difficult to implement blockchain technology in certain industries or applications, and can also lead to concerns about the potential for illicit activities like money laundering or terrorist financing.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ###############################################################\n",
    "# SECTION 2: Parallel Processing with LCEL\n",
    "# ###############################################################\n",
    "\n",
    "print(\"-\" * 50,'\\n',\"DEMO 2: Parallel Processing with LCEL\",'\\n',\"-\" * 50)\n",
    "\n",
    "# Create two different prompts\n",
    "pros_prompt = ChatPromptTemplate.from_template(\n",
    "    \"List 3 pros of {technology} technology.\"\n",
    ")\n",
    "cons_prompt = ChatPromptTemplate.from_template(\n",
    "    \"List 3 cons of {technology} technology.\"\n",
    ")\n",
    "\n",
    "# Create chains for each prompt\n",
    "pros_chain = pros_prompt | llm | StrOutputParser()\n",
    "cons_chain = cons_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Create a parallel chain\n",
    "parallel_chain = RunnableParallel(pros=pros_chain, cons=cons_chain)\n",
    "\n",
    "# Run the parallel chain\n",
    "parallel_result = parallel_chain.invoke({\"technology\": \"blockchain\"})\n",
    "print(\"PROS:\")\n",
    "print(parallel_result[\"pros\"])\n",
    "print(\"\\nCONS:\")\n",
    "print(parallel_result[\"cons\"])\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      " DEMO 3: Advanced Chain with Data Transformation \n",
      " --------------------------------------------------\n",
      "Summary: LCE (Local Consistency Estimation) is a technique used to improve Large Language Model (LLM) applications. Here's a detailed explanation of how LCE improves LLM applications:\n",
      "Details:\n",
      "- **What is Local Consistency Estimation (LCE)?**\n",
      "- LCE is a method for estimating the consistency of local contexts in a sequence of tokens. In natural language processing, local context refers to the surrounding words or tokens that influence the meaning of a particular word. LCE estimates the probability distribution over these local contexts, allowing models to better understand the relationships between tokens and make more accurate predictions.\n",
      "- **How does LCE improve LLM applications?**\n",
      "- LCE improves LLM applications in several ways:\n",
      "- 1. **Improved contextual understanding**: By estimating the consistency of local contexts, LCE helps models better understand the relationships between tokens and their surrounding words. This leads to improved contextual understanding, which is essential for tasks such as language modeling, text classification, and question answering.\n",
      "- 2. **Enhanced language modeling**: LCE enables models to generate more coherent and contextually relevant text by incorporating local consistency estimates into the language modeling process. This results in better language modeling performance, including metrics such as perplexity and fluency.\n",
      "- 3. **Better handling of out-of-vocabulary (OOV) words**: When encountering OOV words, LCE helps models estimate the probability distribution over local contexts, allowing them to make more informed predictions about the word's meaning or context.\n",
      "- 4. **Improved text generation**: By incorporating local consistency estimates into the text generation process, LCE enables models to generate more coherent and contextually relevant text, including tasks such as summarization, paraphrasing, and text completion.\n",
      "- 5. **Reduced overfitting**: LCE can help reduce overfitting by providing a regularization term that encourages models to be less dependent on local contexts. This leads to more generalizable models that perform better on unseen data.\n",
      "- **How does LCE work?**\n",
      "- LCE works by estimating the probability distribution over local contexts using a combination of statistical and machine learning techniques. The process involves:\n",
      "- 1. **Tokenization**: Breaking down input text into individual tokens (words or subwords).\n",
      "- 2. **Contextual windowing**: Defining a contextual window around each token, typically consisting of 7-10 surrounding tokens.\n",
      "- 3. **Local consistency estimation**: Estimating the probability distribution over local contexts using statistical and machine learning techniques, such as Markov chain Monte Carlo (MCMC) or variational inference.\n",
      "- 4. **Integration with language modeling**: Incorporating local consistency estimates into the language modeling process to improve contextual understanding and generation.\n",
      "- In summary, LCE improves LLM applications by providing a more nuanced understanding of local contexts, enhancing language modeling performance, and improving handling of OOV words and text generation tasks.\n",
      "Response Length: 3097 characters\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ###############################################################\n",
    "# SECTION 3: Advanced Chain with Data Transformation\n",
    "# ###############################################################\n",
    "print(\"-\" * 50,'\\n',\"DEMO 3: Advanced Chain with Data Transformation\",'\\n',\"-\" * 50)\n",
    "\n",
    "# Define a function to process input\n",
    "def preprocess_input(input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Process the input data before sending to the LLM.\"\"\"\n",
    "    # Add a timestamp, capitalize the query, etc.\n",
    "    processed_data = input_data.copy()\n",
    "    processed_data[\"query\"] = input_data[\"query\"].upper()\n",
    "    processed_data[\"word_count\"] = len(input_data[\"query\"].split())\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# Define a function to process output\n",
    "def postprocess_output(output: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process the output from the LLM.\"\"\"\n",
    "    lines = output.strip().split(\"\\n\")\n",
    "    return {\n",
    "        \"summary\": lines[0] if lines else \"\",\n",
    "        \"details\": lines[1:],\n",
    "        \"response_length\": len(output),\n",
    "    }\n",
    "\n",
    "\n",
    "# Create an advanced prompt\n",
    "advanced_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following query: {query}\n",
    "\n",
    "    The query has {word_count} words.\n",
    "    Provide a detailed explanation.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create an advanced chain with preprocessing and postprocessing\n",
    "advanced_chain = (\n",
    "    RunnableLambda(preprocess_input)\n",
    "    | advanced_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | RunnableLambda(postprocess_output)\n",
    ")\n",
    "\n",
    "# Run the advanced chain\n",
    "advanced_result = advanced_chain.invoke(\n",
    "    {\"query\": \"how does LCEL improve LLM applications\"}\n",
    ")\n",
    "print(f\"Summary: {advanced_result['summary']}\")\n",
    "print(\"Details:\")\n",
    "for detail in advanced_result[\"details\"]:\n",
    "    if detail.strip():\n",
    "        print(f\"- {detail.strip()}\")\n",
    "print(f\"Response Length: {advanced_result['response_length']} characters\")\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      " DEMO 4: RAG Pattern with LCEL (Simulated) \n",
      " --------------------------------------------------\n",
      "Based on the given context, LCEL stands for LangChain Expression Language. It's a tool that allows for declarative chain construction in LangChain, which simplifies building complex Large Language Model (LLM) applications. \n",
      "\n",
      "LCEL is useful because it supports streaming, async operations, and parallel execution, making it easier to build efficient and scalable LLM applications.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ###############################################################\n",
    "# SECTION 4: RAG Pattern with LCEL\n",
    "# ###############################################################\n",
    "print(\"-\" * 50,'\\n',\"DEMO 4: RAG Pattern with LCEL (Simulated)\",'\\n',\"-\" * 50)\n",
    "\n",
    "\n",
    "# Simulate a retriever function\n",
    "def simulate_retrieval(query: str) -> List[str]:\n",
    "    \"\"\"Simulate document retrieval based on query.\"\"\"\n",
    "    # In a real application, this would query a vector database\n",
    "    documents = [\n",
    "        \"LCEL allows for declarative chain construction in LangChain.\",\n",
    "        \"LangChain Expression Language simplifies building complex LLM applications.\",\n",
    "        \"LCEL supports streaming, async operations, and parallel execution.\",\n",
    "    ]\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Create a RAG prompt\n",
    "rag_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based on the following context:\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create a RAG chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": RunnableLambda(simulate_retrieval) | RunnableLambda(format_docs),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Run the RAG chain\n",
    "rag_result = rag_chain.invoke(\"What is LCEL and why is it useful?\")\n",
    "print(rag_result)\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
