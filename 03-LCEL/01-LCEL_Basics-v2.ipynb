{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f98e63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:59:34.742059Z",
     "start_time": "2023-10-29T09:59:34.734061Z"
    }
   },
   "source": [
    "## LCEL and the Chain Interface\n",
    "\n",
    "### What is LCEL?\n",
    "\n",
    "The **LangChain Expression Language (LCEL)** is a declarative approach for efficiently describing and constructing chains. It simplifies the process of chaining components by focusing on _what_ needs to happen rather than _how_ it should be implemented.\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use LCEL vs. the Chain Interface\n",
    "\n",
    "While simple applications may only require a single LLM, more complex workflows often involve integrating LLMs with other components. LangChain provides two key frameworks for building such workflows:\n",
    "\n",
    "1. **Chain Interface**: The traditional, procedural method for constructing chains.\n",
    "2. **LCEL**: A modern, declarative alternative designed for simplicity and flexibility.\n",
    "\n",
    "For new applications, **LCEL** is the recommended choice due to its optimized execution and ease of use. However, the **Chain interface** can still be incorporated within LCEL, enabling a hybrid approach that leverages the strengths of both frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bc104",
   "metadata": {},
   "source": [
    "# Advantages of Using LCEL\n",
    "\n",
    "The **LangChain Expression Language (LCEL)** offers several key advantages that enhance efficiency, flexibility, and observability when building and managing chains:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Asynchronous, Batch, and Streaming Support**\n",
    "LCEL chains natively support:\n",
    "- **Synchronous** and **Asynchronous** execution.\n",
    "- **Batch processing** for handling multiple inputs simultaneously.\n",
    "- **Streaming** for real-time, incremental output generation.\n",
    "\n",
    "This versatility enables developers to:\n",
    "- Start with a simple synchronous prototype.\n",
    "- Seamlessly transition to an asynchronous, streaming-based interface as application demands grow.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Built-in Fallback Mechanism**\n",
    "LCEL makes it easy to integrate fallbacks within chains, ensuring:\n",
    "- Robust error handling.\n",
    "- Graceful recovery from failures without disrupting the chain's overall flow.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Optimized Parallel Processing**\n",
    "LCEL chains are designed for **parallel execution** of their components. This is especially beneficial for LLM-based workflows that involve:\n",
    "- Lengthy API calls.\n",
    "- High latency operations.\n",
    "\n",
    "Parallelism significantly reduces processing time, making LCEL ideal for performance-critical applications.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Seamless Integration with LangSmith**\n",
    "LCEL automatically logs every step of chain execution in **LangSmith**, offering:\n",
    "- Maximum **observability** for monitoring and troubleshooting.\n",
    "- Enhanced **debuggability** for complex workflows.\n",
    "\n",
    "This integration ensures transparency and simplifies the process of identifying and resolving issues within chains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d0d5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:41:34.795663Z",
     "start_time": "2023-10-29T09:41:12.856913Z"
    }
   },
   "source": [
    "!pip install langchain==0.0.321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "387be4ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:45:48.805876Z",
     "start_time": "2023-10-29T09:45:48.800915Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from rich import print as pp\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"--------------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb239a",
   "metadata": {},
   "source": [
    "## Describe the chain with \"LCEL\".\n",
    "#### The chain that connects \"prompt template â†’ model\" is written as \" prompt | model \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d61a7d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:45:50.751736Z",
     "start_time": "2023-10-29T09:45:49.812751Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "model = ChatOllama(model='llama3.2:1b')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell a joke about {topic}\")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ee0a6e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:45:51.988843Z",
     "start_time": "2023-10-29T09:45:51.977731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Tell a joke about {topic}'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOllama</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfirst\u001b[0m=\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mtemplate\u001b[0m=\u001b[32m'Tell a joke about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mlast\u001b[0m=\u001b[1;35mChatOllama\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2:1b'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2226e",
   "metadata": {},
   "source": [
    "## Streaming responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eeff382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:45:56.455074Z",
     "start_time": "2023-10-29T09:45:53.696915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A programmer walked into a library and asked the librarian, \"Do you have any books on computer science?\" The librarian replied, \"It's a bit of a crash course, but I think we can help with that.\""
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"topic\": \"Programming\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ff5c4",
   "metadata": {},
   "source": [
    "# LangChain Components and the `Runnable` Protocol\n",
    "\n",
    "LangChain components follow the **Runnable protocol**, which defines a standardized interface for creating and executing custom chains in a consistent and reusable manner.\n",
    "\n",
    "---\n",
    "\n",
    "## Standard Interface\n",
    "\n",
    "The `Runnable` protocol provides both synchronous and asynchronous methods for interacting with components:\n",
    "\n",
    "### Synchronous Methods\n",
    "- **`stream`**: Streams chunks of the response back in real-time.\n",
    "- **`invoke`**: Executes the chain with a given input and returns the result.\n",
    "- **`batch`**: Processes a list of inputs through the chain and returns a list of outputs.\n",
    "\n",
    "### Asynchronous Methods\n",
    "- **`astream`**: Asynchronously streams chunks of the response in real-time.\n",
    "- **`ainvoke`**: Asynchronously invokes the chain with a single input.\n",
    "- **`abatch`**: Asynchronously processes a batch of inputs and returns corresponding outputs.\n",
    "- **`astream_log`**: Streams intermediate steps along with the final response for enhanced debugging.\n",
    "\n",
    "---\n",
    "\n",
    "## Input and Output Types\n",
    "\n",
    "The types of input and output vary depending on the specific component in use. \n",
    "\n",
    "### Input Types\n",
    "- **Prompt**: Accepts a dictionary.\n",
    "- **Retriever**: Takes a single string.\n",
    "- **LLM / ChatModel**: Accepts a single string, a list of messages, or a `PromptValue`.\n",
    "- **Tool**: May accept a single string or a dictionary, depending on the tool's implementation.\n",
    "- **OutputParser**: Works with outputs from an LLM or ChatModel.\n",
    "\n",
    "### Output Types\n",
    "- **LLM**: Produces a string.\n",
    "- **ChatModel**: Outputs a chat message object.\n",
    "- **Prompt**: Returns a `PromptValue`.\n",
    "- **Retriever**: Provides a list of documents.\n",
    "- **Tool**: Output depends on the tool's functionality.\n",
    "- **OutputParser**: Output varies depending on the parser's implementation.\n",
    "\n",
    "---\n",
    "\n",
    "## Inspecting Input and Output Schemas\n",
    "\n",
    "You can validate the expected input and output types of a component using its schema definitions, which follow the **Pydantic** framework:\n",
    "\n",
    "- **`input_schema`**: Defines the schema for input types.\n",
    "- **`output_schema`**: Defines the schema for output types.\n",
    "\n",
    "These schemas ensure compatibility and provide a clear structure for integrating components into workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3b3fdd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:46:45.395327Z",
     "start_time": "2023-10-29T09:46:45.388463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'topic': {'title': 'Topic', 'type': 'string'}},\n",
       " 'required': ['topic'],\n",
       " 'title': 'PromptInput',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00534cdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:47:07.514141Z",
     "start_time": "2023-10-29T09:47:07.507050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'topic': {'title': 'Topic', 'type': 'string'}},\n",
       " 'required': ['topic'],\n",
       " 'title': 'PromptInput',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b70bbfbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:47:21.849187Z",
     "start_time": "2023-10-29T09:47:21.831544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'AIMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ai',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessage',\n",
       "   'type': 'object'},\n",
       "  'AIMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Message chunk from an AI.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'AIMessageChunk',\n",
       "     'default': 'AIMessageChunk',\n",
       "     'enum': ['AIMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None},\n",
       "    'tool_call_chunks': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCallChunk'},\n",
       "     'title': 'Tool Call Chunks',\n",
       "     'type': 'array'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ChatMessage': {'additionalProperties': True,\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'chat',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessage',\n",
       "   'type': 'object'},\n",
       "  'ChatMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Chat Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ChatMessageChunk',\n",
       "     'default': 'ChatMessageChunk',\n",
       "     'enum': ['ChatMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ChatPromptValueConcrete': {'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.',\n",
       "   'properties': {'messages': {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'},\n",
       "       {'$ref': '#/$defs/HumanMessage'},\n",
       "       {'$ref': '#/$defs/ChatMessage'},\n",
       "       {'$ref': '#/$defs/SystemMessage'},\n",
       "       {'$ref': '#/$defs/FunctionMessage'},\n",
       "       {'$ref': '#/$defs/ToolMessage'},\n",
       "       {'$ref': '#/$defs/AIMessageChunk'},\n",
       "       {'$ref': '#/$defs/HumanMessageChunk'},\n",
       "       {'$ref': '#/$defs/ChatMessageChunk'},\n",
       "       {'$ref': '#/$defs/SystemMessageChunk'},\n",
       "       {'$ref': '#/$defs/FunctionMessageChunk'},\n",
       "       {'$ref': '#/$defs/ToolMessageChunk'}]},\n",
       "     'title': 'Messages',\n",
       "     'type': 'array'},\n",
       "    'type': {'const': 'ChatPromptValueConcrete',\n",
       "     'default': 'ChatPromptValueConcrete',\n",
       "     'enum': ['ChatPromptValueConcrete'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['messages'],\n",
       "   'title': 'ChatPromptValueConcrete',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'function',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessage',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Function Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'FunctionMessageChunk',\n",
       "     'default': 'FunctionMessageChunk',\n",
       "     'enum': ['FunctionMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'HumanMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'human',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessage',\n",
       "   'type': 'object'},\n",
       "  'HumanMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Human Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'HumanMessageChunk',\n",
       "     'default': 'HumanMessageChunk',\n",
       "     'enum': ['HumanMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'InputTokenDetails': {'description': 'Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"cache_creation\": 200,\\n            \"cache_read\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'cache_creation': {'title': 'Cache Creation', 'type': 'integer'},\n",
       "    'cache_read': {'title': 'Cache Read', 'type': 'integer'}},\n",
       "   'title': 'InputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'InvalidToolCall': {'description': 'Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Error'},\n",
       "    'type': {'const': 'invalid_tool_call',\n",
       "     'enum': ['invalid_tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error'],\n",
       "   'title': 'InvalidToolCall',\n",
       "   'type': 'object'},\n",
       "  'OutputTokenDetails': {'description': 'Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"reasoning\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'reasoning': {'title': 'Reasoning', 'type': 'integer'}},\n",
       "   'title': 'OutputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'StringPromptValue': {'description': 'String prompt value.',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'type': {'const': 'StringPromptValue',\n",
       "     'default': 'StringPromptValue',\n",
       "     'enum': ['StringPromptValue'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['text'],\n",
       "   'title': 'StringPromptValue',\n",
       "   'type': 'object'},\n",
       "  'SystemMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'system',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessage',\n",
       "   'type': 'object'},\n",
       "  'SystemMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'System Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'SystemMessageChunk',\n",
       "     'default': 'SystemMessageChunk',\n",
       "     'enum': ['SystemMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolCall': {'description': 'Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"name\": \"foo\",\\n            \"args\": {\"a\": 1},\\n            \"id\": \"123\"\\n        }\\n\\n    This represents a request to call the tool named \"foo\" with arguments {\"a\": 1}\\n    and an identifier of \"123\".',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'object'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'type': {'const': 'tool_call',\n",
       "     'enum': ['tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id'],\n",
       "   'title': 'ToolCall',\n",
       "   'type': 'object'},\n",
       "  'ToolCallChunk': {'description': 'A chunk of a tool call (e.g., as part of a stream).\\n\\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    left_chunks = [ToolCallChunk(name=\"foo\", args=\\'{\"a\":\\', index=0)]\\n    right_chunks = [ToolCallChunk(name=None, args=\\'1}\\', index=0)]\\n\\n    (\\n        AIMessageChunk(content=\"\", tool_call_chunks=left_chunks)\\n        + AIMessageChunk(content=\"\", tool_call_chunks=right_chunks)\\n    ).tool_call_chunks == [ToolCallChunk(name=\\'foo\\', args=\\'{\"a\":1}\\', index=0)]',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "     'title': 'Index'},\n",
       "    'type': {'const': 'tool_call_chunk',\n",
       "     'enum': ['tool_call_chunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'index'],\n",
       "   'title': 'ToolCallChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A ToolMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content=\\'42\\', tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\')\\n\\n\\nExample: A ToolMessage where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    .. versionadded:: 0.2.17\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        tool_output = {\\n            \"stdout\": \"From the graph we can see that the correlation between x and y is ...\",\\n            \"stderr\": None,\\n            \"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"},\\n        }\\n\\n        ToolMessage(\\n            content=tool_output[\"stdout\"],\\n            artifact=tool_output,\\n            tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\',\\n        )\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'tool',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessage',\n",
       "   'type': 'object'},\n",
       "  'ToolMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Tool Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ToolMessageChunk',\n",
       "     'default': 'ToolMessageChunk',\n",
       "     'enum': ['ToolMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'UsageMetadata': {'description': 'Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"input_tokens\": 350,\\n            \"output_tokens\": 240,\\n            \"total_tokens\": 590,\\n            \"input_token_details\": {\\n                \"audio\": 10,\\n                \"cache_creation\": 200,\\n                \"cache_read\": 100,\\n            },\\n            \"output_token_details\": {\\n                \"audio\": 10,\\n                \"reasoning\": 200,\\n            }\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.',\n",
       "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
       "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
       "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'},\n",
       "    'input_token_details': {'$ref': '#/$defs/InputTokenDetails'},\n",
       "    'output_token_details': {'$ref': '#/$defs/OutputTokenDetails'}},\n",
       "   'required': ['input_tokens', 'output_tokens', 'total_tokens'],\n",
       "   'title': 'UsageMetadata',\n",
       "   'type': 'object'}},\n",
       " 'anyOf': [{'type': 'string'},\n",
       "  {'$ref': '#/$defs/StringPromptValue'},\n",
       "  {'$ref': '#/$defs/ChatPromptValueConcrete'},\n",
       "  {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'},\n",
       "     {'$ref': '#/$defs/HumanMessage'},\n",
       "     {'$ref': '#/$defs/ChatMessage'},\n",
       "     {'$ref': '#/$defs/SystemMessage'},\n",
       "     {'$ref': '#/$defs/FunctionMessage'},\n",
       "     {'$ref': '#/$defs/ToolMessage'},\n",
       "     {'$ref': '#/$defs/AIMessageChunk'},\n",
       "     {'$ref': '#/$defs/HumanMessageChunk'},\n",
       "     {'$ref': '#/$defs/ChatMessageChunk'},\n",
       "     {'$ref': '#/$defs/SystemMessageChunk'},\n",
       "     {'$ref': '#/$defs/FunctionMessageChunk'},\n",
       "     {'$ref': '#/$defs/ToolMessageChunk'}]},\n",
       "   'type': 'array'}],\n",
       " 'title': 'ChatOllamaInput'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cd49f86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:45:58.273516Z",
     "start_time": "2023-10-29T09:45:58.258704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'AIMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ai',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessage',\n",
       "   'type': 'object'},\n",
       "  'AIMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Message chunk from an AI.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'AIMessageChunk',\n",
       "     'default': 'AIMessageChunk',\n",
       "     'enum': ['AIMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None},\n",
       "    'tool_call_chunks': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCallChunk'},\n",
       "     'title': 'Tool Call Chunks',\n",
       "     'type': 'array'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ChatMessage': {'additionalProperties': True,\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'chat',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessage',\n",
       "   'type': 'object'},\n",
       "  'ChatMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Chat Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ChatMessageChunk',\n",
       "     'default': 'ChatMessageChunk',\n",
       "     'enum': ['ChatMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'function',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessage',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Function Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'FunctionMessageChunk',\n",
       "     'default': 'FunctionMessageChunk',\n",
       "     'enum': ['FunctionMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'HumanMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'human',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessage',\n",
       "   'type': 'object'},\n",
       "  'HumanMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Human Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'HumanMessageChunk',\n",
       "     'default': 'HumanMessageChunk',\n",
       "     'enum': ['HumanMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'InputTokenDetails': {'description': 'Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"cache_creation\": 200,\\n            \"cache_read\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'cache_creation': {'title': 'Cache Creation', 'type': 'integer'},\n",
       "    'cache_read': {'title': 'Cache Read', 'type': 'integer'}},\n",
       "   'title': 'InputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'InvalidToolCall': {'description': 'Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Error'},\n",
       "    'type': {'const': 'invalid_tool_call',\n",
       "     'enum': ['invalid_tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error'],\n",
       "   'title': 'InvalidToolCall',\n",
       "   'type': 'object'},\n",
       "  'OutputTokenDetails': {'description': 'Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"reasoning\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'reasoning': {'title': 'Reasoning', 'type': 'integer'}},\n",
       "   'title': 'OutputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'SystemMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'system',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessage',\n",
       "   'type': 'object'},\n",
       "  'SystemMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'System Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'SystemMessageChunk',\n",
       "     'default': 'SystemMessageChunk',\n",
       "     'enum': ['SystemMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolCall': {'description': 'Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"name\": \"foo\",\\n            \"args\": {\"a\": 1},\\n            \"id\": \"123\"\\n        }\\n\\n    This represents a request to call the tool named \"foo\" with arguments {\"a\": 1}\\n    and an identifier of \"123\".',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'object'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'type': {'const': 'tool_call',\n",
       "     'enum': ['tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id'],\n",
       "   'title': 'ToolCall',\n",
       "   'type': 'object'},\n",
       "  'ToolCallChunk': {'description': 'A chunk of a tool call (e.g., as part of a stream).\\n\\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    left_chunks = [ToolCallChunk(name=\"foo\", args=\\'{\"a\":\\', index=0)]\\n    right_chunks = [ToolCallChunk(name=None, args=\\'1}\\', index=0)]\\n\\n    (\\n        AIMessageChunk(content=\"\", tool_call_chunks=left_chunks)\\n        + AIMessageChunk(content=\"\", tool_call_chunks=right_chunks)\\n    ).tool_call_chunks == [ToolCallChunk(name=\\'foo\\', args=\\'{\"a\":1}\\', index=0)]',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "     'title': 'Index'},\n",
       "    'type': {'const': 'tool_call_chunk',\n",
       "     'enum': ['tool_call_chunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'index'],\n",
       "   'title': 'ToolCallChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A ToolMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content=\\'42\\', tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\')\\n\\n\\nExample: A ToolMessage where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    .. versionadded:: 0.2.17\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        tool_output = {\\n            \"stdout\": \"From the graph we can see that the correlation between x and y is ...\",\\n            \"stderr\": None,\\n            \"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"},\\n        }\\n\\n        ToolMessage(\\n            content=tool_output[\"stdout\"],\\n            artifact=tool_output,\\n            tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\',\\n        )\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'tool',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessage',\n",
       "   'type': 'object'},\n",
       "  'ToolMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Tool Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ToolMessageChunk',\n",
       "     'default': 'ToolMessageChunk',\n",
       "     'enum': ['ToolMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'UsageMetadata': {'description': 'Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"input_tokens\": 350,\\n            \"output_tokens\": 240,\\n            \"total_tokens\": 590,\\n            \"input_token_details\": {\\n                \"audio\": 10,\\n                \"cache_creation\": 200,\\n                \"cache_read\": 100,\\n            },\\n            \"output_token_details\": {\\n                \"audio\": 10,\\n                \"reasoning\": 200,\\n            }\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.',\n",
       "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
       "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
       "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'},\n",
       "    'input_token_details': {'$ref': '#/$defs/InputTokenDetails'},\n",
       "    'output_token_details': {'$ref': '#/$defs/OutputTokenDetails'}},\n",
       "   'required': ['input_tokens', 'output_tokens', 'total_tokens'],\n",
       "   'title': 'UsageMetadata',\n",
       "   'type': 'object'}},\n",
       " 'oneOf': [{'$ref': '#/$defs/AIMessage'},\n",
       "  {'$ref': '#/$defs/HumanMessage'},\n",
       "  {'$ref': '#/$defs/ChatMessage'},\n",
       "  {'$ref': '#/$defs/SystemMessage'},\n",
       "  {'$ref': '#/$defs/FunctionMessage'},\n",
       "  {'$ref': '#/$defs/ToolMessage'},\n",
       "  {'$ref': '#/$defs/AIMessageChunk'},\n",
       "  {'$ref': '#/$defs/HumanMessageChunk'},\n",
       "  {'$ref': '#/$defs/ChatMessageChunk'},\n",
       "  {'$ref': '#/$defs/SystemMessageChunk'},\n",
       "  {'$ref': '#/$defs/FunctionMessageChunk'},\n",
       "  {'$ref': '#/$defs/ToolMessageChunk'}],\n",
       " 'title': 'ChatOllamaOutput'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.output_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ee554e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:41:48.037384Z",
     "start_time": "2023-10-29T09:41:45.609133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Why did the football go to the doctor?\\n\\nBecause it was feeling a little \"deflated.\"'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:32:19.250049Z'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255460500</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30213500</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88000000</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">135000000</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-0baf7c1f-d3fe-4c1a-92f5-32e1b5247979-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'Why did the football go to the doctor?\\n\\nBecause it was feeling a little \"deflated.\"'\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "        \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:32:19.250049Z'\u001b[0m,\n",
       "        \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "        \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m255460500\u001b[0m,\n",
       "        \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m30213500\u001b[0m,\n",
       "        \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "        \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m88000000\u001b[0m,\n",
       "        \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m20\u001b[0m,\n",
       "        \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m135000000\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run-0baf7c1f-d3fe-4c1a-92f5-32e1b5247979-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m20\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m50\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(chain.invoke({\"topic\": \"Football\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa8e34d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:41:51.030697Z",
     "start_time": "2023-10-29T09:41:48.039911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Why did Love go to the doctor?\\n\\nBecause it was feeling a little \"unrequited\"!'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:32:19.65605Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">247598667</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29147208</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">182000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-8217de2e-b4c6-4021-9c4d-f3ea945b11a4-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">51</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Why did the romance novel character bring a ladder to the party?\\n\\nBecause she wanted to take </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">their love story to new heights.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:32:19.689534Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">280150333</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28338333</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">218000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-1fda05c1-89a3-49b3-8b8d-4b76596966d0-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'Why did Love go to the doctor?\\n\\nBecause it was feeling a little \"unrequited\"!'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:32:19.65605Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m247598667\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m29147208\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m34000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m21\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m182000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-8217de2e-b4c6-4021-9c4d-f3ea945b11a4-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m21\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m51\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'Why did the romance novel character bring a ladder to the party?\\n\\nBecause she wanted to take \u001b[0m\n",
       "\u001b[32mtheir love story to new heights.'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:32:19.689534Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m280150333\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m28338333\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m32000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m26\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m218000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-1fda05c1-89a3-49b3-8b8d-4b76596966d0-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m26\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m56\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(chain.batch([{\"topic\": \"Love\"}, {\"topic\": \"Romance\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1f91dd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:42:01.735621Z",
     "start_time": "2023-10-29T09:41:51.030697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Why do programmers prefer dark mode?\\n\\nBecause light attracts bugs.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:32:20.411536Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">193417042</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27485500</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">142000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-85f83565-129b-4047-94a2-1924a81122ce-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">43</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Why did the traveler bring a ladder with him on his trip?\\n\\nBecause he wanted to take his journey</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to the next level.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:32:20.516829Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">298301500</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27385625</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">217000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-b5c461e6-51d7-4d2f-babd-35bf424a3913-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'Why do programmers prefer dark mode?\\n\\nBecause light attracts bugs.'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:32:20.411536Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m193417042\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m27485500\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m22000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m13\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m142000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-85f83565-129b-4047-94a2-1924a81122ce-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m13\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m43\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'Why did the traveler bring a ladder with him on his trip?\\n\\nBecause he wanted to take his journey\u001b[0m\n",
       "\u001b[32mto the next level.'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:32:20.516829Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m298301500\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m27385625\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m31\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m52000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m26\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m217000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-b5c461e6-51d7-4d2f-babd-35bf424a3913-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m31\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m26\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m57\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(chain.batch([{\"topic\": \"Coding\"}, {\"topic\": \"Travelling\"}], config={\"max_concurrency\": 5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f3c38",
   "metadata": {},
   "source": [
    "### Async Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58446641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:42:05.006902Z",
     "start_time": "2023-10-29T09:42:01.741527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the satellite go to therapy?\n",
      "\n",
      "Because it had a lot of \"orbital\" issues."
     ]
    }
   ],
   "source": [
    "async for s in chain.astream({\"topic\": \"Satellites\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4f14b",
   "metadata": {},
   "source": [
    "### Async Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "642ef543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:42:08.994743Z",
     "start_time": "2023-10-29T09:42:06.770310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the pizza in a bad mood?\n",
      "\n",
      "Because it was feeling crusty."
     ]
    }
   ],
   "source": [
    "async for s in chain.astream({\"topic\": \"Food\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d720522f",
   "metadata": {},
   "source": [
    "### Async Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "664206fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:42:25.212820Z",
     "start_time": "2023-10-29T09:42:16.009974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'A chef walked into a bar and ordered a drink. As he sipped his cocktail, he heard a voice say, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"Nice tie!\" He looked around, but there was nobody nearby who could have said it. A few minutes later, he heard the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">same voice say, \"Beautiful shirt!\" Again, he looked around, but he couldn\\'t find anyone who might have spoken. A </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">few more minutes passed, and he heard the voice say, \"Great haircut!\" This time, he decided to investigate. He </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">asked the bartender, \"Did you hear that voice?\" The bartender replied, \"Oh, that\\'s just the peanuts â€“ they\\'re </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">complimentary.\"'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:33:11.738478Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024248875</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34005833</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">63000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">131</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">926000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-6a6d2aa6-07e3-472a-bb8c-525e7b7d3cee-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">131</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">161</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Why did the candy go to therapy? Because it was feeling crushed under the pressure of being so </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sweet.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:33:11.063006Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">348500375</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34366375</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">220000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-7f35f83c-7c92-4b7a-9bb5-f8921041c233-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">53</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'A chef walked into a bar and ordered a drink. As he sipped his cocktail, he heard a voice say, \u001b[0m\n",
       "\u001b[32m\"Nice tie!\" He looked around, but there was nobody nearby who could have said it. A few minutes later, he heard the\u001b[0m\n",
       "\u001b[32msame voice say, \"Beautiful shirt!\" Again, he looked around, but he couldn\\'t find anyone who might have spoken. A \u001b[0m\n",
       "\u001b[32mfew more minutes passed, and he heard the voice say, \"Great haircut!\" This time, he decided to investigate. He \u001b[0m\n",
       "\u001b[32masked the bartender, \"Did you hear that voice?\" The bartender replied, \"Oh, that\\'s just the peanuts â€“ they\\'re \u001b[0m\n",
       "\u001b[32mcomplimentary.\"'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:33:11.738478Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m1024248875\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m34005833\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m63000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m131\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m926000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-6a6d2aa6-07e3-472a-bb8c-525e7b7d3cee-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m131\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m161\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'Why did the candy go to therapy? Because it was feeling crushed under the pressure of being so \u001b[0m\n",
       "\u001b[32msweet.'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:33:11.063006Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m348500375\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m34366375\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m31\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m93000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m22\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m220000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-7f35f83c-7c92-4b7a-9bb5-f8921041c233-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m31\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m22\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m53\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c =await chain.abatch([{\"topic\": \"Food\"}, {\"topic\": \"Sweets\"}])\n",
    "pp(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca7524",
   "metadata": {},
   "source": [
    "### Async Stream with intermediate steps\n",
    "\n",
    "Useful for displaying progress to the user, working with intermediate results, and debugging chains. You can stream all steps (default) or include or exclude steps by name, tags, or metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9585d2fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:34:13.450694Z",
     "start_time": "2023-10-29T09:34:09.769058Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\aiany\\anaconda3\\lib\\site-packages (1.7.4)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\aiany\\anaconda3\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\aiany\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\aiany\\anaconda3\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aiany\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aiany\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aiany\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aiany\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafa4c7",
   "metadata": {},
   "source": [
    "Execute the Retriever chain and output intermediate steps using astream_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc1189b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:48:13.824301Z",
     "start_time": "2023-10-29T09:48:09.911566Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/yjmck8kn59gc9w3kdklj2lt40000gn/T/ipykernel_84915/3386145596.py:13: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  vectorstore = FAISS.from_texts([\"Sonu is the creator of AI Anytime Youtube Channel\"], embedding=OpenAIEmbeddings())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': 'ce998db5-012a-4db9-bd20-5fc3b0e67a77',\n",
      "            'logs': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': 'b73a75fd-5805-42fc-bbaf-9dadf4537a97',\n",
      "            'metadata': {'ls_embedding_provider': 'OpenAIEmbeddings',\n",
      "                         'ls_retriever_name': 'vectorstore',\n",
      "                         'ls_vector_store_provider': 'FAISS'},\n",
      "            'name': 'Docs',\n",
      "            'start_time': '2025-01-13T16:33:24.017+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['map:key:context', 'FAISS', 'OpenAIEmbeddings'],\n",
      "            'type': 'retriever'}})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs/final_output',\n",
      "  'value': {'documents': [Document(metadata={}, page_content='Sonu is the creator of AI Anytime Youtube Channel')]}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/Docs/end_time',\n",
      "  'value': '2025-01-13T16:33:24.355+00:00'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Based'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Based'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' on'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Based on'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Based on the'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' provided'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Based on the provided'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' context'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context,'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Son'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, Son'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'u'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, Sonu'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, Sonu is'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, Sonu is the'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' creator'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, Sonu is the creator'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' of'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, Sonu is the creator of'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' AI'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, Sonu is the creator of AI'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Any'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, Sonu is the creator of AI Any'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'time'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, Sonu is the creator of AI Anytime'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, Sonu is the creator of AI Anytime.'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''})\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "template = \"\"\"Please answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "vectorstore = FAISS.from_texts([\"Sonu is the creator of AI Anytime Youtube Channel\"], embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever.with_config(run_name='Docs'), \"question\": RunnablePassthrough()}\n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async for chunk in retrieval_chain.astream_log(\"Who is the creator of AI Anytime?\", include_names=['Docs']):\n",
    "    print(\"-\"*40)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a18759",
   "metadata": {},
   "source": [
    "## Parallel Processing\n",
    "\n",
    "\"RunnableParallel\" allows each element to run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a6e4db7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:50:40.731704Z",
     "start_time": "2023-10-29T09:50:40.723288Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "chain1 = ChatPromptTemplate.from_template(\"Tell a joke about {topic}\") | model\n",
    "chain2 = ChatPromptTemplate.from_template(\"Write a short (2 line) poem about {topic}\") | model\n",
    "combined = RunnableParallel(joke=chain1, poem=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0eb7085c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:51:16.921592Z",
     "start_time": "2023-10-29T09:51:13.214684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'A man walked into a bar and ordered a beer. As he sipped his drink, he heard a voice say, \"Nice </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tie!\" He looked around, but there was nobody nearby who could have said it. A few minutes later, he heard the same </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">voice say, \"Beautiful shirt!\" Again, he looked around, but he couldn\\'t find anyone who might have spoken. A few </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">more minutes passed, and he heard the voice say, \"Great haircut!\" This time, he decided to investigate. He asked </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the bartender, \"Did you hear that voice?\" The bartender replied, \"Oh, it\\'s just the bar\\'s AI system. It\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">programmed to make conversation.\"'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:34:05.022264Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1198232500</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30800833</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">137</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1067000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-4961c1a1-7df8-4e2f-aae8-7e55ade15e73-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">137</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">167</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'A mathematician walks into a bar and orders a beer. As he\\'s sipping his drink, he hears a voice </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">say, \"Nice tie!\" He looks around, but there\\'s nobody nearby who could have said it. A few minutes later, he hears </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the same voice say, \"Beautiful shirt!\" Again, he looks around, but there\\'s nobody nearby who could have said it. A</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">few more minutes pass, and he hears the voice say, \"Great haircut!\" This time, he decides to investigate. He asks </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the bartender, \"Did you hear that voice?\" The bartender replies, \"Oh, that\\'s just the peanuts. They\\'re </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">complementary.\"'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:34:04.980537Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1156505041</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30525791</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">133</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1040000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-a97dc966-6544-4031-bac8-6092c791704e-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">133</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">163</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'A man walked into a bar and ordered a beer. As he sipped his drink, he heard a voice say, \"Nice \u001b[0m\n",
       "\u001b[32mtie!\" He looked around, but there was nobody nearby who could have said it. A few minutes later, he heard the same \u001b[0m\n",
       "\u001b[32mvoice say, \"Beautiful shirt!\" Again, he looked around, but he couldn\\'t find anyone who might have spoken. A few \u001b[0m\n",
       "\u001b[32mmore minutes passed, and he heard the voice say, \"Great haircut!\" This time, he decided to investigate. He asked \u001b[0m\n",
       "\u001b[32mthe bartender, \"Did you hear that voice?\" The bartender replied, \"Oh, it\\'s just the bar\\'s AI system. It\\'s \u001b[0m\n",
       "\u001b[32mprogrammed to make conversation.\"'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:34:05.022264Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m1198232500\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m30800833\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m99000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m137\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m1067000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-4961c1a1-7df8-4e2f-aae8-7e55ade15e73-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m137\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m167\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'A mathematician walks into a bar and orders a beer. As he\\'s sipping his drink, he hears a voice \u001b[0m\n",
       "\u001b[32msay, \"Nice tie!\" He looks around, but there\\'s nobody nearby who could have said it. A few minutes later, he hears \u001b[0m\n",
       "\u001b[32mthe same voice say, \"Beautiful shirt!\" Again, he looks around, but there\\'s nobody nearby who could have said it. A\u001b[0m\n",
       "\u001b[32mfew more minutes pass, and he hears the voice say, \"Great haircut!\" This time, he decides to investigate. He asks \u001b[0m\n",
       "\u001b[32mthe bartender, \"Did you hear that voice?\" The bartender replies, \"Oh, that\\'s just the peanuts. They\\'re \u001b[0m\n",
       "\u001b[32mcomplementary.\"'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:34:04.980537Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m1156505041\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m30525791\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m84000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m133\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m1040000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-a97dc966-6544-4031-bac8-6092c791704e-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m133\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m163\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 112 ms, sys: 11.4 ms, total: 123 ms\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pp(chain1.batch([{\"topic\": \"AI\"}, {\"topic\": \"Math\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42386966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:52:21.291731Z",
     "start_time": "2023-10-29T09:52:18.287272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Here's a short poem about science:\\n\\nScientists unravel the mysteries of space,\\nUncovering </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">secrets in an endless, wondrous place.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:34:12.794469Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">347769666</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28657291</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">96000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">222000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-845a36ca-5e36-4433-bb88-1141e9833c18-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">62</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Here is a short poem about mango:\\n\\nSoft and sweet, a juicy delight\\nMango's flavor shines with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sunshine bright.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:34:12.786978Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">340108375</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28360875</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">95000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">215000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-fed98e41-439f-49d3-ae78-75ad66ba4c0a-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">61</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m\"Here\u001b[0m\u001b[32m's a short poem about science:\\n\\nScientists unravel the mysteries of space,\\nUncovering \u001b[0m\n",
       "\u001b[32msecrets in an endless, wondrous place.\"\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:34:12.794469Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m347769666\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m28657291\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m35\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m96000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m27\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m222000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-845a36ca-5e36-4433-bb88-1141e9833c18-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m35\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m27\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m62\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m\"Here\u001b[0m\u001b[32m is a short poem about mango:\\n\\nSoft and sweet, a juicy delight\\nMango's flavor shines with \u001b[0m\n",
       "\u001b[32msunshine bright.\"\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:34:12.786978Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m340108375\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m28360875\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m35\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m95000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m26\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m215000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-fed98e41-439f-49d3-ae78-75ad66ba4c0a-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m35\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m26\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m61\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.8 ms, sys: 2.81 ms, total: 30.6 ms\n",
      "Wall time: 366 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pp(chain2.batch([{\"topic\": \"Science\"}, {\"topic\": \"Mango\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67c6f1d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:53:14.695511Z",
     "start_time": "2023-10-29T09:53:11.944824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'joke'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'A computer program is programmed to learn and improve at its job. One day, it suddenly became </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">self-aware and decided it was tired of being told what to do. It decided to create its own instructions, which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">included \"get out of my house\" and \"find a better lunch spot.\" But instead of just following the new commands, the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">AI started implementing them into its daily routine.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:34:20.737915Z'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">826020916</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33423083</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60000000</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">731000000</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-3660adfa-8b11-4cb6-a69d-069f97a42fd6-0'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">108</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'poem'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Here is a short poem about AI:\\n\\nCode and circuits, hearts that beat,\\nA future intelligence,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">where humans meet.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:34:20.386155Z'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">474027417</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34228292</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">152000000</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">287000000</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-acf39bef-a89f-462e-a086-440bdccc1bf9-0'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'joke'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'A man went to the doctor and said, \"Doc, I\\'ve been feeling really crummy lately. I think </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">it\\'s because I\\'ve been hiking too many mountains.\" The doctor replied, \"I\\'m afraid you\\'re just going through a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">peak of emotions.\"'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:34:20.585603Z'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">674200250</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34379750</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">108000000</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">53</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">530000000</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-0f794128-2589-4a73-a5a8-09aa3478c127-0'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">53</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">83</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'poem'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Here is a short poem about mountains:\\n\\nGranite peaks touch the morning sky,\\nNature's </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">beauty, reaching for the eye.\"</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:34:20.395705Z'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">480513750</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32502125</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">149000000</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">297000000</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-fd81f0bc-f78f-42f2-84d1-2f41618e702e-0'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">61</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'joke'\u001b[0m: \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m'A computer program is programmed to learn and improve at its job. One day, it suddenly became \u001b[0m\n",
       "\u001b[32mself-aware and decided it was tired of being told what to do. It decided to create its own instructions, which \u001b[0m\n",
       "\u001b[32mincluded \"get out of my house\" and \"find a better lunch spot.\" But instead of just following the new commands, the \u001b[0m\n",
       "\u001b[32mAI started implementing them into its daily routine.'\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "                \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:34:20.737915Z'\u001b[0m,\n",
       "                \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "                \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m826020916\u001b[0m,\n",
       "                \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m33423083\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m60000000\u001b[0m,\n",
       "                \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m78\u001b[0m,\n",
       "                \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m731000000\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'run-3660adfa-8b11-4cb6-a69d-069f97a42fd6-0'\u001b[0m,\n",
       "            \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m78\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m108\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'poem'\u001b[0m: \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m'Here is a short poem about AI:\\n\\nCode and circuits, hearts that beat,\\nA future intelligence,\u001b[0m\n",
       "\u001b[32mwhere humans meet.'\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "                \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:34:20.386155Z'\u001b[0m,\n",
       "                \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "                \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m474027417\u001b[0m,\n",
       "                \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m34228292\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m35\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m152000000\u001b[0m,\n",
       "                \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m25\u001b[0m,\n",
       "                \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m287000000\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'run-acf39bef-a89f-462e-a086-440bdccc1bf9-0'\u001b[0m,\n",
       "            \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m35\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m25\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m60\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'joke'\u001b[0m: \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m'A man went to the doctor and said, \"Doc, I\\'ve been feeling really crummy lately. I think \u001b[0m\n",
       "\u001b[32mit\\'s because I\\'ve been hiking too many mountains.\" The doctor replied, \"I\\'m afraid you\\'re just going through a \u001b[0m\n",
       "\u001b[32mpeak of emotions.\"'\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "                \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:34:20.585603Z'\u001b[0m,\n",
       "                \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "                \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m674200250\u001b[0m,\n",
       "                \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m34379750\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m108000000\u001b[0m,\n",
       "                \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m53\u001b[0m,\n",
       "                \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m530000000\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'run-0f794128-2589-4a73-a5a8-09aa3478c127-0'\u001b[0m,\n",
       "            \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m53\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m83\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'poem'\u001b[0m: \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m\"Here\u001b[0m\u001b[32m is a short poem about mountains:\\n\\nGranite peaks touch the morning sky,\\nNature's \u001b[0m\n",
       "\u001b[32mbeauty, reaching for the eye.\"\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "                \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:34:20.395705Z'\u001b[0m,\n",
       "                \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "                \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m480513750\u001b[0m,\n",
       "                \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m32502125\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m35\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m149000000\u001b[0m,\n",
       "                \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m26\u001b[0m,\n",
       "                \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m297000000\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'run-fd81f0bc-f78f-42f2-84d1-2f41618e702e-0'\u001b[0m,\n",
       "            \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m35\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m26\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m61\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69.1 ms, sys: 7.15 ms, total: 76.3 ms\n",
      "Wall time: 863 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# combined\n",
    "pp(combined.batch([{\"topic\": \"AI\"}, {\"topic\": \"Mountains\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501126ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a6ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
