{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a771d1c",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "The **L**ang**C**hain **E**xpression **L**anguage (LCEL) abstracts key Python concepts into a streamlined format, facilitating a \"minimalist\" code layer for constructing chains of LangChain components. LCEL offers robust support for:\n",
    "\n",
    "1. Rapid development of chains.\n",
    "2. Advanced features like streaming, asynchronous processing, parallel execution, and more.\n",
    "3. Seamless integration with LangSmith and LangServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133b5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from rich import print as pp\n",
    "\n",
    "# Create a prompt template for generating book summaries\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the book titled '{book_title}' in three sentences.\"\n",
    ")\n",
    "\n",
    "# Use a specific LLM model\n",
    "model = ChatOllama(model=\"llama3.2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb40dca",
   "metadata": {},
   "source": [
    "## LCEL Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff840235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/yjmck8kn59gc9w3kdklj2lt40000gn/T/ipykernel_52966/2803915055.py:6: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Create a chain for summarizing books\n",
    "chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=model,\n",
    "    output_parser=output_parser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6b88d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/yjmck8kn59gc9w3kdklj2lt40000gn/T/ipykernel_52966/355639390.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  out = chain.run(book_title=\"1984 by George Orwell\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of \"1984\" by George Orwell:\n",
      "\n",
      "In the dystopian novel \"1984,\" the totalitarian government of Oceania, led by a figure known as Big Brother, exercises total control over its citizens, suppressing individual freedom and independent thought through propaganda, surveillance, and manipulation. The protagonist, Winston Smith, a low-ranking member of the ruling Party, begins to question the official ideology and starts an illicit love affair with a fellow worker, Julia, which ultimately leads to their discovery by the authorities and their subsequent torture and brainwashing. Ultimately, Winston is driven mad by the relentless pursuit of truth and individuality, and he surrenders his independence to Big Brother, exemplifying the chilling notion that \"war is peace,\" \"freedom is slavery,\" and \"ignorance is strength.\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the chain\n",
    "out = chain.run(book_title=\"1984 by George Orwell\")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b76d042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of \"To Kill a Mockingbird\" in three sentences:\n",
      "\n",
      "Set in the Deep South during the 1930s, the novel follows the experiences of Scout Finch, a young girl who lives with her older brother Jem and their father, Atticus, in the fictional town of Maycomb. When a black man named Tom Robinson is falsely accused of raping a white woman, Atticus agrees to defend him in court despite knowing he'll face prejudice and hostility, teaching Scout and Jem valuable lessons about justice, morality, and empathy. Ultimately, the trial ends in tragedy when the all-white jury delivers a guilty verdict, but Atticus's defense and teachings have a lasting impact on his children and the community, leaving them with a newfound understanding of the importance of treating all people with kindness and respect.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Alternative LCEL-style chain\n",
    "lcel_chain = prompt | model | output_parser\n",
    "\n",
    "# Run the chain\n",
    "out = lcel_chain.invoke({\"book_title\": \"To Kill a Mockingbird\"})\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f329bd57",
   "metadata": {},
   "source": [
    "## How the Pipe Operator Works\n",
    "To truly grasp LCEL, let's examine how the pipe operation functions. It takes output from the **right** and feeds it to the **left**â€”but since this isn't standard Python, how is it implemented? We can create our own version using simple functions.\n",
    "\n",
    "We'll utilize the `__or__` method in Python class objects. When we combine two classes like `chain = class_a | class_b`, the Python interpreter checks for the presence of the `__or__` method in these classes. If it exists, the expression `|` is translated to `chain = class_a.__or__(class_b)`. \n",
    "\n",
    "This means both of the following patterns yield the same result:\n",
    "\n",
    "```python\n",
    "# Object approach\n",
    "chain = class_a.__or__(class_b)\n",
    "chain(\"some input\")\n",
    "\n",
    "# Pipe approach\n",
    "chain = class_a | class_b\n",
    "chain(\"some input\")\n",
    "```\n",
    "\n",
    "With this understanding, we can create a `Runnable` class that takes a function and transforms it into a chainable function using the pipe operator `|`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f8878d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            return other(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)\n",
    "\n",
    "\n",
    "def subtract_three(x):\n",
    "    return x - 3\n",
    "\n",
    "\n",
    "def divide_by_two(x):\n",
    "    return x / 2\n",
    "\n",
    "\n",
    "subtract_three = Runnable(subtract_three)\n",
    "divide_by_two = Runnable(divide_by_two)\n",
    "\n",
    "chain = subtract_three | divide_by_two\n",
    "print(chain(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd5224",
   "metadata": {},
   "source": [
    "## LCEL Deep Dive\n",
    "Now that we understand what this syntax is doing under the hood, let's explore it within the context of LCEL and see a few of the additional methods that LangChain has provided to maximize flexibility when working with LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ef3ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Create an embedding model\n",
    "embedding = OllamaEmbeddings(model=\"snowflake-arctic-embed:33m\") \n",
    "# Create vector stores with different content\n",
    "vecstore_a = InMemoryVectorStore.from_texts(\n",
    "    [\"Python is a programming language.\", \"It is popular for data science.\"],\n",
    "    embedding=embedding\n",
    ")\n",
    "vecstore_b = InMemoryVectorStore.from_texts(\n",
    "    [\"Python supports object-oriented programming.\", \"It is used for web development.\"],\n",
    "    embedding=embedding\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bed7be",
   "metadata": {},
   "source": [
    "Here we have used `RunnableParallel` to create two parallel streams of information, one for `\"context\"` that is information fed in from `retriever_a`, and another for `\"question\"` which is the _passthrough_ information, ie the information that is passed through from our `chain.invoke(\"when was James born?\")` call.\n",
    "\n",
    "Using this information the chain is close to answering the question but it doesn't have enough information, it is missing the information that we have stored in `retriever_b`. Fortunately, we can have multiple parallel information streams using the `RunnableParallel` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf69a452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableParallel</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">steps__</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">VectorStoreRetriever</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">tags</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'InMemoryVectorStore'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'OllamaEmbeddings'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">vectorstore</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">langchain_core.vectorstores.in_memory.InMemoryVectorStore</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x12ad98c90</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">search_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnablePassthrough</span><span style=\"font-weight: bold\">()</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span><span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Provide details about Python:\\n\\nContext: {context}\\n\\nQuestion: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{question}\\n\\nAnswer:'</span>\n",
       "                    <span style=\"font-weight: bold\">)</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOllama</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StrOutputParser</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfirst\u001b[0m=\u001b[1;35mRunnableParallel\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33msteps__\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'context'\u001b[0m: \u001b[1;35mVectorStoreRetriever\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mtags\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'InMemoryVectorStore'\u001b[0m, \u001b[32m'OllamaEmbeddings'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[33mvectorstore\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mlangchain_core.vectorstores.in_memory.InMemoryVectorStore\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x12ad98c90\u001b[0m\u001b[1m>\u001b[0m,\n",
       "                \u001b[33msearch_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[32m'question'\u001b[0m: \u001b[1;35mRunnablePassthrough\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'context'\u001b[0m, \u001b[32m'question'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'context'\u001b[0m, \u001b[32m'question'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                        \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                        \u001b[33mtemplate\u001b[0m=\u001b[32m'Provide details about Python:\\n\\nContext: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mcontext\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\nQuestion: \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32mquestion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\nAnswer:'\u001b[0m\n",
       "                    \u001b[1m)\u001b[0m,\n",
       "                    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mChatOllama\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mlast\u001b[0m=\u001b[1;35mStrOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a versatile and widely-used programming language that has numerous applications in various fields. Some of the main uses of Python include:\n",
      "\n",
      "1. **Data Science and Machine Learning**: Python is a popular choice for data scientists and machine learning engineers due to its extensive libraries such as NumPy, pandas, and scikit-learn, which provide efficient data structures and algorithms for data analysis and modeling.\n",
      "2. **Web Development**: Python can be used for web development with frameworks like Django and Flask, which enable rapid development of secure and scalable web applications.\n",
      "3. **Automation**: Python's easy-to-learn syntax and vast number of libraries make it an ideal language for automating tasks, such as data processing, file management, and system administration.\n",
      "4. **Scientific Computing**: Python is widely used in scientific computing for tasks like numerical analysis, visualization, and simulation due to its integration with libraries like SciPy, NumPy, and Matplotlib.\n",
      "5. **Artificial Intelligence and Deep Learning**: Python is a popular choice for AI and deep learning applications, thanks to the availability of extensive libraries such as TensorFlow, Keras, and PyTorch.\n",
      "6. **Game Development**: Python can be used for game development, particularly with frameworks like Pygame and Panda3D, which provide an easy-to-use interface for creating games.\n",
      "7. **Network Security**: Python is often used in network security for tasks like penetration testing, vulnerability assessment, and security research due to its extensive libraries and tools like Nmap and Scapy.\n",
      "8. **Education**: Python is widely taught in introductory programming courses due to its simplicity, readability, and ease of use, making it an ideal language for beginners.\n",
      "9. **Research**: Python's versatility and large community make it a popular choice for research applications, particularly in fields like physics, engineering, and computer science.\n",
      "10. **Business Applications**: Python is used in various business applications, such as data analysis, report generation, and automation of business processes.\n",
      "\n",
      "These are just a few examples of the many uses of Python. Its versatility, flexibility, and large community make it a popular choice for a wide range of applications.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import (\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough\n",
    ")\n",
    "\n",
    "retriever_a = vecstore_a.as_retriever()\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "prompt_str = \"\"\"Provide details about Python:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "\n",
    "retriever = RunnableParallel(\n",
    "    {'context': retriever_a, 'question': RunnablePassthrough()},\n",
    ")\n",
    "\n",
    "chain = retriever | prompt | model | output_parser\n",
    "\n",
    "pp(chain)\n",
    "\n",
    "out = chain.invoke(\"What are the uses of Python?\")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96196bc3",
   "metadata": {},
   "source": [
    "## Runnable Lambdas\n",
    "The `RunnableLambda` is a LangChain abstraction that allows us to turn Python functions into pipe-compatible function, similar to the `Runnable` class we created near the beginning of this notebook.\n",
    "\n",
    "Let's try it out with our earlier `add_five` and `multiply_by_two` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dd9a961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableLambda</span><span style=\"font-weight: bold\">(</span>calc_cube<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableLambda</span><span style=\"font-weight: bold\">(</span>calc_reciprocal<span style=\"font-weight: bold\">))</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\u001b[33mfirst\u001b[0m=\u001b[1;35mRunnableLambda\u001b[0m\u001b[1m(\u001b[0mcalc_cube\u001b[1m)\u001b[0m, \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mlast\u001b[0m=\u001b[1;35mRunnableLambda\u001b[0m\u001b[1m(\u001b[0mcalc_reciprocal\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calc_cube(x):\n",
    "    return x ** 3\n",
    "\n",
    "def calc_reciprocal(x):\n",
    "    return 1 / x\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "calc_cube = RunnableLambda(calc_cube)\n",
    "calc_reciprocal = RunnableLambda(calc_reciprocal)\n",
    "\n",
    "chain = calc_cube | calc_reciprocal\n",
    "pp(chain)\n",
    "\n",
    "result = chain.invoke(4)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a64c308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Share an interesting fact about {topic}.'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOllama</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StrOutputParser</span><span style=\"font-weight: bold\">()]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableLambda</span><span style=\"font-weight: bold\">(</span>extract_first_sentence<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfirst\u001b[0m=\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mtemplate\u001b[0m=\u001b[32m'Share an interesting fact about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m.'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mChatOllama\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2'\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mStrOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mlast\u001b[0m=\u001b[1;35mRunnableLambda\u001b[0m\u001b[1m(\u001b[0mextract_first_sentence\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an interesting fact about Machine Learning:\n",
      "\n",
      "Did you know that the concept of Machine Learning was first introduced by Arthur Samuel in 1956? He proposed a rule-based system called \"Checkers\" which could learn to play the game through self-play and improvement.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_first_sentence(x):\n",
    "    return x.split(\".\")[0] + \".\"\n",
    "\n",
    "get_first_sentence = RunnableLambda(extract_first_sentence)\n",
    "\n",
    "prompt_str = \"Share an interesting fact about {topic}.\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "\n",
    "chain = prompt | model | output_parser | get_first_sentence\n",
    "pp(chain)\n",
    "\n",
    "fact = chain.invoke({\"topic\": \"Machine Learning\"})\n",
    "print(fact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b9e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db72db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
