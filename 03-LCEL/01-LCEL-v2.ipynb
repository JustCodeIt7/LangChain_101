{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a771d1c",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "The **L**ang**C**hain **E**xpression **L**anguage (LCEL) abstracts key Python concepts into a streamlined format, facilitating a \"minimalist\" code layer for constructing chains of LangChain components. LCEL offers robust support for:\n",
    "\n",
    "1. Rapid development of chains.\n",
    "2. Advanced features like streaming, asynchronous processing, parallel execution, and more.\n",
    "3. Seamless integration with LangSmith and LangServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133b5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from rich import print as pp\n",
    "\n",
    "# Create a prompt template for generating book summaries\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the book titled '{book_title}' in three sentences.\"\n",
    ")\n",
    "\n",
    "# Use a specific LLM model\n",
    "model = ChatOllama(model=\"llama3.2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb40dca",
   "metadata": {},
   "source": [
    "## LCEL Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff840235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/yjmck8kn59gc9w3kdklj2lt40000gn/T/ipykernel_58803/2803915055.py:6: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Create a chain for summarizing books\n",
    "chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=model,\n",
    "    output_parser=output_parser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6b88d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/yjmck8kn59gc9w3kdklj2lt40000gn/T/ipykernel_58803/355639390.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  out = chain.run(book_title=\"1984 by George Orwell\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of \"1984\" by George Orwell:\n",
      "\n",
      "In a dystopian future, the totalitarian government of Oceania, led by a figure known as Big Brother, exercises total control over its citizens through constant surveillance and propaganda. The protagonist, Winston Smith, begins to rebel against the government's all-pervasive influence, starting an illicit love affair with a fellow worker, Julia, and secretly writing in a forbidden diary. Ultimately, Winston's desire for freedom and individuality is crushed by the authorities, who capture him and subject him to a brutal process of physical and psychological torture, erasing his very existence from history.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the chain\n",
    "out = chain.run(book_title=\"1984 by George Orwell\")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b76d042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of \"To Kill a Mockingbird\" in three sentences:\n",
      "\n",
      "Set in the Deep South during the 1930s, the novel follows the experiences of Scout Finch and her family as they navigate issues of racism, injustice, and childhood innocence in the small town of Maycomb, Alabama. The story centers around the defense of Tom Robinson, a black man falsely accused of raping a white woman, as Scout's father, Atticus Finch, agrees to take on the case despite knowing he'll face prejudice and hostility. Through Atticus's courageous yet principled defense of Tom, the novel explores themes of empathy, compassion, and understanding, ultimately leading to a poignant exploration of the complexities of human nature and the importance of doing what is right in the face of overwhelming opposition.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Alternative LCEL-style chain\n",
    "lcel_chain = prompt | model | output_parser\n",
    "\n",
    "# Run the chain\n",
    "out = lcel_chain.invoke({\"book_title\": \"To Kill a Mockingbird\"})\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f329bd57",
   "metadata": {},
   "source": [
    "## How the Pipe Operator Works\n",
    "To truly grasp LCEL, let's examine how the pipe operation functions. It takes output from the **right** and feeds it to the **left**â€”but since this isn't standard Python, how is it implemented? We can create our own version using simple functions.\n",
    "\n",
    "We'll utilize the `__or__` method in Python class objects. When we combine two classes like `chain = class_a | class_b`, the Python interpreter checks for the presence of the `__or__` method in these classes. If it exists, the expression `|` is translated to `chain = class_a.__or__(class_b)`. \n",
    "\n",
    "This means both of the following patterns yield the same result:\n",
    "\n",
    "```python\n",
    "# Object approach\n",
    "chain = class_a.__or__(class_b)\n",
    "chain(\"some input\")\n",
    "\n",
    "# Pipe approach\n",
    "chain = class_a | class_b\n",
    "chain(\"some input\")\n",
    "```\n",
    "\n",
    "With this understanding, we can create a `Runnable` class that takes a function and transforms it into a chainable function using the pipe operator `|`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f8878d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            return other(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)\n",
    "\n",
    "\n",
    "def subtract_three(x):\n",
    "    return x - 3\n",
    "\n",
    "\n",
    "def divide_by_two(x):\n",
    "    return x / 2\n",
    "\n",
    "\n",
    "subtract_three = Runnable(subtract_three)\n",
    "divide_by_two = Runnable(divide_by_two)\n",
    "\n",
    "chain = subtract_three | divide_by_two\n",
    "print(chain(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd5224",
   "metadata": {},
   "source": [
    "## LCEL Deep Dive\n",
    "Now that we understand what this syntax is doing under the hood, let's explore it within the context of LCEL and see a few of the additional methods that LangChain has provided to maximize flexibility when working with LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef3ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Create an embedding model\n",
    "embedding = OllamaEmbeddings(model=\"snowflake-arctic-embed:33m\") \n",
    "# Create vector stores with different content\n",
    "vecstore_a = InMemoryVectorStore.from_texts(\n",
    "    [\"Python is a programming language.\", \"It is popular for data science.\"],\n",
    "    embedding=embedding\n",
    ")\n",
    "vecstore_b = InMemoryVectorStore.from_texts(\n",
    "    [\"Python supports object-oriented programming.\", \"It is used for web development.\"],\n",
    "    embedding=embedding\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bed7be",
   "metadata": {},
   "source": [
    "Here we have used `RunnableParallel` to create two parallel streams of information, one for `\"context\"` that is information fed in from `retriever_a`, and another for `\"question\"` which is the _passthrough_ information, ie the information that is passed through from our `chain.invoke(\"when was James born?\")` call.\n",
    "\n",
    "Using this information the chain is close to answering the question but it doesn't have enough information, it is missing the information that we have stored in `retriever_b`. Fortunately, we can have multiple parallel information streams using the `RunnableParallel` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf69a452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableParallel</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">steps__</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">VectorStoreRetriever</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">tags</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'InMemoryVectorStore'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'OllamaEmbeddings'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">vectorstore</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">langchain_core.vectorstores.in_memory.InMemoryVectorStore</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11db48c10</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">search_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnablePassthrough</span><span style=\"font-weight: bold\">()</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span><span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Provide details about Python:\\n\\nContext: {context}\\n\\nQuestion: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">{question}\\n\\nAnswer:'</span>\n",
       "                    <span style=\"font-weight: bold\">)</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOllama</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StrOutputParser</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfirst\u001b[0m=\u001b[1;35mRunnableParallel\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33msteps__\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'context'\u001b[0m: \u001b[1;35mVectorStoreRetriever\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mtags\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'InMemoryVectorStore'\u001b[0m, \u001b[32m'OllamaEmbeddings'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[33mvectorstore\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mlangchain_core.vectorstores.in_memory.InMemoryVectorStore\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x11db48c10\u001b[0m\u001b[1m>\u001b[0m,\n",
       "                \u001b[33msearch_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[32m'question'\u001b[0m: \u001b[1;35mRunnablePassthrough\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'context'\u001b[0m, \u001b[32m'question'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'context'\u001b[0m, \u001b[32m'question'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                        \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                        \u001b[33mtemplate\u001b[0m=\u001b[32m'Provide details about Python:\\n\\nContext: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mcontext\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\nQuestion: \u001b[0m\n",
       "\u001b[32m{\u001b[0m\u001b[32mquestion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\nAnswer:'\u001b[0m\n",
       "                    \u001b[1m)\u001b[0m,\n",
       "                    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mChatOllama\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mlast\u001b[0m=\u001b[1;35mStrOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a versatile and widely-used programming language with a multitude of applications across various domains. Some of the main uses of Python include:\n",
      "\n",
      "1. **Data Science and Machine Learning**: Python is an essential tool in data science, particularly when it comes to machine learning, deep learning, and natural language processing. Libraries like NumPy, pandas, and scikit-learn provide efficient data structures and algorithms for data analysis, modeling, and visualization.\n",
      "2. **Web Development**: Python can be used for web development using popular frameworks like Django and Flask. These frameworks enable developers to build scalable, secure, and maintainable web applications quickly.\n",
      "3. **Automation and Scripting**: Python's easy-to-learn syntax and vast number of libraries make it an ideal choice for automating tasks, such as data processing, file management, and system administration.\n",
      "4. **Artificial Intelligence and Robotics**: Python is used in AI and robotics to build intelligent systems that can learn, reason, and interact with the environment. Libraries like TensorFlow, Keras, and OpenCV provide tools for building and training AI models.\n",
      "5. **Scientific Computing**: Python is widely used in scientific computing for tasks such as numerical simulations, data analysis, and visualization. Libraries like NumPy, SciPy, and Matplotlib provide efficient algorithms and tools for scientific computing.\n",
      "6. **Education**: Python is often taught in introductory programming courses due to its simplicity, readability, and ease of use, making it an ideal language for beginners.\n",
      "7. **Game Development**: Python can be used for game development using libraries like Pygame and Panda3D. These libraries provide a simple and efficient way to build 2D and 3D games.\n",
      "8. **Network Security**: Python is used in network security for tasks such as vulnerability assessment, penetration testing, and security analysis. Libraries like Scapy and Nmap provide tools for network exploration and exploitation.\n",
      "9. **Business Intelligence**: Python can be used for business intelligence tasks such as data visualization, reporting, and data mining. Libraries like pandas, NumPy, and Matplotlib provide efficient algorithms and tools for business intelligence.\n",
      "10. **Research**: Python is widely used in research for tasks such as data analysis, simulations, and visualization. Libraries like SciPy, NumPy, and Pandas provide efficient algorithms and tools for research.\n",
      "\n",
      "In summary, Python's versatility, ease of use, and vast number of libraries make it a popular choice for a wide range of applications across various domains.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import (\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough\n",
    ")\n",
    "\n",
    "retriever_a = vecstore_a.as_retriever()\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "prompt_str = \"\"\"Provide details about Python:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "\n",
    "retriever = RunnableParallel(\n",
    "    {'context': retriever_a, 'question': RunnablePassthrough()},\n",
    ")\n",
    "\n",
    "chain = retriever | prompt | model | output_parser\n",
    "\n",
    "pp(chain)\n",
    "\n",
    "out = chain.invoke(\"What are the uses of Python?\")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96196bc3",
   "metadata": {},
   "source": [
    "## Runnable Lambdas\n",
    "The `RunnableLambda` is a LangChain abstraction that allows us to turn Python functions into pipe-compatible function, similar to the `Runnable` class we created near the beginning of this notebook.\n",
    "\n",
    "Let's try it out with our earlier `add_five` and `multiply_by_two` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dd9a961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableLambda</span><span style=\"font-weight: bold\">(</span>calc_cube<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableLambda</span><span style=\"font-weight: bold\">(</span>calc_reciprocal<span style=\"font-weight: bold\">))</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\u001b[33mfirst\u001b[0m=\u001b[1;35mRunnableLambda\u001b[0m\u001b[1m(\u001b[0mcalc_cube\u001b[1m)\u001b[0m, \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mlast\u001b[0m=\u001b[1;35mRunnableLambda\u001b[0m\u001b[1m(\u001b[0mcalc_reciprocal\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calc_cube(x):\n",
    "    return x ** 3\n",
    "\n",
    "def calc_reciprocal(x):\n",
    "    return 1 / x\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "calc_cube = RunnableLambda(calc_cube)\n",
    "calc_reciprocal = RunnableLambda(calc_reciprocal)\n",
    "\n",
    "chain = calc_cube | calc_reciprocal\n",
    "pp(chain)\n",
    "\n",
    "result = chain.invoke(4)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a64c308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Share an interesting fact about {topic}.'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOllama</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StrOutputParser</span><span style=\"font-weight: bold\">()]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableLambda</span><span style=\"font-weight: bold\">(</span>extract_first_sentence<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfirst\u001b[0m=\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mtemplate\u001b[0m=\u001b[32m'Share an interesting fact about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m.'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mChatOllama\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2'\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mStrOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mlast\u001b[0m=\u001b[1;35mRunnableLambda\u001b[0m\u001b[1m(\u001b[0mextract_first_sentence\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an interesting fact about Machine Learning:\n",
      "\n",
      "**The \"Curse of Dimensionality\"**\n",
      "\n",
      "In the 1960s, mathematician Richard Ashford discovered that as the number of features (or dimensions) in a dataset increases, the amount of data required to achieve accurate predictions also increases exponentially.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_first_sentence(x):\n",
    "    return x.split(\".\")[0] + \".\"\n",
    "\n",
    "get_first_sentence = RunnableLambda(extract_first_sentence)\n",
    "\n",
    "prompt_str = \"Share an interesting fact about {topic}.\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "\n",
    "chain = prompt | model | output_parser | get_first_sentence\n",
    "pp(chain)\n",
    "\n",
    "fact = chain.invoke({\"topic\": \"Machine Learning\"})\n",
    "print(fact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b9e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db72db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
