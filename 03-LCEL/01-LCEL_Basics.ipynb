{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f98e63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:59:34.742059Z",
     "start_time": "2023-10-29T09:59:34.734061Z"
    }
   },
   "source": [
    "## LCEL and the Chain Interface\n",
    "\n",
    "### What is LCEL?\n",
    "\n",
    "The **LangChain Expression Language (LCEL)** is a declarative approach for efficiently describing and constructing chains. It simplifies the process of chaining components by focusing on _what_ needs to happen rather than _how_ it should be implemented.\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use LCEL vs. the Chain Interface\n",
    "\n",
    "While simple applications may only require a single LLM, more complex workflows often involve integrating LLMs with other components. LangChain provides two key frameworks for building such workflows:\n",
    "\n",
    "1. **Chain Interface**: The traditional, procedural method for constructing chains.\n",
    "2. **LCEL**: A modern, declarative alternative designed for simplicity and flexibility.\n",
    "\n",
    "For new applications, **LCEL** is the recommended choice due to its optimized execution and ease of use. However, the **Chain interface** can still be incorporated within LCEL, enabling a hybrid approach that leverages the strengths of both frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bc104",
   "metadata": {},
   "source": [
    "# Advantages of Using LCEL\n",
    "\n",
    "The **LangChain Expression Language (LCEL)** offers several key advantages that enhance efficiency, flexibility, and observability when building and managing chains:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Asynchronous, Batch, and Streaming Support**\n",
    "LCEL chains natively support:\n",
    "- **Synchronous** and **Asynchronous** execution.\n",
    "- **Batch processing** for handling multiple inputs simultaneously.\n",
    "- **Streaming** for real-time, incremental output generation.\n",
    "\n",
    "This versatility enables developers to:\n",
    "- Start with a simple synchronous prototype.\n",
    "- Seamlessly transition to an asynchronous, streaming-based interface as application demands grow.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Built-in Fallback Mechanism**\n",
    "LCEL makes it easy to integrate fallbacks within chains, ensuring:\n",
    "- Robust error handling.\n",
    "- Graceful recovery from failures without disrupting the chain's overall flow.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Optimized Parallel Processing**\n",
    "LCEL chains are designed for **parallel execution** of their components. This is especially beneficial for LLM-based workflows that involve:\n",
    "- Lengthy API calls.\n",
    "- High latency operations.\n",
    "\n",
    "Parallelism significantly reduces processing time, making LCEL ideal for performance-critical applications.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Seamless Integration with LangSmith**\n",
    "LCEL automatically logs every step of chain execution in **LangSmith**, offering:\n",
    "- Maximum **observability** for monitoring and troubleshooting.\n",
    "- Enhanced **debuggability** for complex workflows.\n",
    "\n",
    "This integration ensures transparency and simplifies the process of identifying and resolving issues within chains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d0d5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:41:34.795663Z",
     "start_time": "2023-10-29T09:41:12.856913Z"
    }
   },
   "source": [
    "!pip install langchain==0.0.321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "387be4ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:45:48.805876Z",
     "start_time": "2023-10-29T09:45:48.800915Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from rich import print as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb239a",
   "metadata": {},
   "source": [
    "## Describe the chain with \"LCEL\".\n",
    "#### The chain that connects \"prompt template â†’ model\" is written as \" prompt | model \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d61a7d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:45:50.751736Z",
     "start_time": "2023-10-29T09:45:49.812751Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model='llama3.2:1b')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell a joke about {topic}\")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee0a6e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:45:51.988843Z",
     "start_time": "2023-10-29T09:45:51.977731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Tell a joke about {topic}'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOllama</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfirst\u001b[0m=\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mtemplate\u001b[0m=\u001b[32m'Tell a joke about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mlast\u001b[0m=\u001b[1;35mChatOllama\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2:1b'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2226e",
   "metadata": {},
   "source": [
    "## Streaming responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eeff382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:45:56.455074Z",
     "start_time": "2023-10-29T09:45:53.696915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A programmer walked into a bar and ordered a beer. As he was sipping his drink, he heard a voice say, \"Nice tie!\" He looked around, but there was nobody nearby who could have said it. A few minutes later, he heard the same voice say, \"Beautiful shirt!\" Again, he looked around, but he couldn't find anyone who might have spoken. A few more minutes passed, and he heard the voice say, \"Great haircut!\" This time, he decided to investigate further. He asked the bartender, \"Did you hear that voice?\"\n",
      "\n",
      "The bartender replied, \"Oh, that's just the peanuts. They're complementary.\""
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"topic\": \"Programming\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ff5c4",
   "metadata": {},
   "source": [
    "# LangChain Components and the `Runnable` Protocol\n",
    "\n",
    "LangChain components follow the **Runnable protocol**, which defines a standardized interface for creating and executing custom chains in a consistent and reusable manner.\n",
    "\n",
    "---\n",
    "\n",
    "## Standard Interface\n",
    "\n",
    "The `Runnable` protocol provides both synchronous and asynchronous methods for interacting with components:\n",
    "\n",
    "### Synchronous Methods\n",
    "- **`stream`**: Streams chunks of the response back in real-time.\n",
    "- **`invoke`**: Executes the chain with a given input and returns the result.\n",
    "- **`batch`**: Processes a list of inputs through the chain and returns a list of outputs.\n",
    "\n",
    "### Asynchronous Methods\n",
    "- **`astream`**: Asynchronously streams chunks of the response in real-time.\n",
    "- **`ainvoke`**: Asynchronously invokes the chain with a single input.\n",
    "- **`abatch`**: Asynchronously processes a batch of inputs and returns corresponding outputs.\n",
    "- **`astream_log`**: Streams intermediate steps along with the final response for enhanced debugging.\n",
    "\n",
    "---\n",
    "\n",
    "## Input and Output Types\n",
    "\n",
    "The types of input and output vary depending on the specific component in use. \n",
    "\n",
    "### Input Types\n",
    "- **Prompt**: Accepts a dictionary.\n",
    "- **Retriever**: Takes a single string.\n",
    "- **LLM / ChatModel**: Accepts a single string, a list of messages, or a `PromptValue`.\n",
    "- **Tool**: May accept a single string or a dictionary, depending on the tool's implementation.\n",
    "- **OutputParser**: Works with outputs from an LLM or ChatModel.\n",
    "\n",
    "### Output Types\n",
    "- **LLM**: Produces a string.\n",
    "- **ChatModel**: Outputs a chat message object.\n",
    "- **Prompt**: Returns a `PromptValue`.\n",
    "- **Retriever**: Provides a list of documents.\n",
    "- **Tool**: Output depends on the tool's functionality.\n",
    "- **OutputParser**: Output varies depending on the parser's implementation.\n",
    "\n",
    "---\n",
    "\n",
    "## Inspecting Input and Output Schemas\n",
    "\n",
    "You can validate the expected input and output types of a component using its schema definitions, which follow the **Pydantic** framework:\n",
    "\n",
    "- **`input_schema`**: Defines the schema for input types.\n",
    "- **`output_schema`**: Defines the schema for output types.\n",
    "\n",
    "These schemas ensure compatibility and provide a clear structure for integrating components into workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3b3fdd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:46:45.395327Z",
     "start_time": "2023-10-29T09:46:45.388463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'properties'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Topic'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'string'</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'required'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'PromptInput'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'object'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'properties'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'topic'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Topic'\u001b[0m, \u001b[32m'type'\u001b[0m: \u001b[32m'string'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'required'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'title'\u001b[0m: \u001b[32m'PromptInput'\u001b[0m,\n",
       "    \u001b[32m'type'\u001b[0m: \u001b[32m'object'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(chain.input_schema.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00534cdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:47:07.514141Z",
     "start_time": "2023-10-29T09:47:07.507050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'properties'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'string'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Question'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'string'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'required'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'PromptInput'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'object'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'properties'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'context'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Context'\u001b[0m, \u001b[32m'type'\u001b[0m: \u001b[32m'string'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'question'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'Question'\u001b[0m, \u001b[32m'type'\u001b[0m: \u001b[32m'string'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'required'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'context'\u001b[0m, \u001b[32m'question'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'title'\u001b[0m: \u001b[32m'PromptInput'\u001b[0m,\n",
       "    \u001b[32m'type'\u001b[0m: \u001b[32m'object'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(prompt.input_schema.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b70bbfbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:47:21.849187Z",
     "start_time": "2023-10-29T09:47:21.831544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'AIMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ai',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessage',\n",
       "   'type': 'object'},\n",
       "  'AIMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Message chunk from an AI.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'AIMessageChunk',\n",
       "     'default': 'AIMessageChunk',\n",
       "     'enum': ['AIMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None},\n",
       "    'tool_call_chunks': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCallChunk'},\n",
       "     'title': 'Tool Call Chunks',\n",
       "     'type': 'array'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ChatMessage': {'additionalProperties': True,\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'chat',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessage',\n",
       "   'type': 'object'},\n",
       "  'ChatMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Chat Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ChatMessageChunk',\n",
       "     'default': 'ChatMessageChunk',\n",
       "     'enum': ['ChatMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ChatPromptValueConcrete': {'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.',\n",
       "   'properties': {'messages': {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'},\n",
       "       {'$ref': '#/$defs/HumanMessage'},\n",
       "       {'$ref': '#/$defs/ChatMessage'},\n",
       "       {'$ref': '#/$defs/SystemMessage'},\n",
       "       {'$ref': '#/$defs/FunctionMessage'},\n",
       "       {'$ref': '#/$defs/ToolMessage'},\n",
       "       {'$ref': '#/$defs/AIMessageChunk'},\n",
       "       {'$ref': '#/$defs/HumanMessageChunk'},\n",
       "       {'$ref': '#/$defs/ChatMessageChunk'},\n",
       "       {'$ref': '#/$defs/SystemMessageChunk'},\n",
       "       {'$ref': '#/$defs/FunctionMessageChunk'},\n",
       "       {'$ref': '#/$defs/ToolMessageChunk'}]},\n",
       "     'title': 'Messages',\n",
       "     'type': 'array'},\n",
       "    'type': {'const': 'ChatPromptValueConcrete',\n",
       "     'default': 'ChatPromptValueConcrete',\n",
       "     'enum': ['ChatPromptValueConcrete'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['messages'],\n",
       "   'title': 'ChatPromptValueConcrete',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'function',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessage',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Function Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'FunctionMessageChunk',\n",
       "     'default': 'FunctionMessageChunk',\n",
       "     'enum': ['FunctionMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'HumanMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'human',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessage',\n",
       "   'type': 'object'},\n",
       "  'HumanMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Human Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'HumanMessageChunk',\n",
       "     'default': 'HumanMessageChunk',\n",
       "     'enum': ['HumanMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'InputTokenDetails': {'description': 'Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"cache_creation\": 200,\\n            \"cache_read\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'cache_creation': {'title': 'Cache Creation', 'type': 'integer'},\n",
       "    'cache_read': {'title': 'Cache Read', 'type': 'integer'}},\n",
       "   'title': 'InputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'InvalidToolCall': {'description': 'Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Error'},\n",
       "    'type': {'const': 'invalid_tool_call',\n",
       "     'enum': ['invalid_tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error'],\n",
       "   'title': 'InvalidToolCall',\n",
       "   'type': 'object'},\n",
       "  'OutputTokenDetails': {'description': 'Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"reasoning\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'reasoning': {'title': 'Reasoning', 'type': 'integer'}},\n",
       "   'title': 'OutputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'StringPromptValue': {'description': 'String prompt value.',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'type': {'const': 'StringPromptValue',\n",
       "     'default': 'StringPromptValue',\n",
       "     'enum': ['StringPromptValue'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['text'],\n",
       "   'title': 'StringPromptValue',\n",
       "   'type': 'object'},\n",
       "  'SystemMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'system',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessage',\n",
       "   'type': 'object'},\n",
       "  'SystemMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'System Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'SystemMessageChunk',\n",
       "     'default': 'SystemMessageChunk',\n",
       "     'enum': ['SystemMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolCall': {'description': 'Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"name\": \"foo\",\\n            \"args\": {\"a\": 1},\\n            \"id\": \"123\"\\n        }\\n\\n    This represents a request to call the tool named \"foo\" with arguments {\"a\": 1}\\n    and an identifier of \"123\".',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'object'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'type': {'const': 'tool_call',\n",
       "     'enum': ['tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id'],\n",
       "   'title': 'ToolCall',\n",
       "   'type': 'object'},\n",
       "  'ToolCallChunk': {'description': 'A chunk of a tool call (e.g., as part of a stream).\\n\\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    left_chunks = [ToolCallChunk(name=\"foo\", args=\\'{\"a\":\\', index=0)]\\n    right_chunks = [ToolCallChunk(name=None, args=\\'1}\\', index=0)]\\n\\n    (\\n        AIMessageChunk(content=\"\", tool_call_chunks=left_chunks)\\n        + AIMessageChunk(content=\"\", tool_call_chunks=right_chunks)\\n    ).tool_call_chunks == [ToolCallChunk(name=\\'foo\\', args=\\'{\"a\":1}\\', index=0)]',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "     'title': 'Index'},\n",
       "    'type': {'const': 'tool_call_chunk',\n",
       "     'enum': ['tool_call_chunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'index'],\n",
       "   'title': 'ToolCallChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A ToolMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content=\\'42\\', tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\')\\n\\n\\nExample: A ToolMessage where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    .. versionadded:: 0.2.17\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        tool_output = {\\n            \"stdout\": \"From the graph we can see that the correlation between x and y is ...\",\\n            \"stderr\": None,\\n            \"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"},\\n        }\\n\\n        ToolMessage(\\n            content=tool_output[\"stdout\"],\\n            artifact=tool_output,\\n            tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\',\\n        )\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'tool',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessage',\n",
       "   'type': 'object'},\n",
       "  'ToolMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Tool Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ToolMessageChunk',\n",
       "     'default': 'ToolMessageChunk',\n",
       "     'enum': ['ToolMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'UsageMetadata': {'description': 'Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"input_tokens\": 350,\\n            \"output_tokens\": 240,\\n            \"total_tokens\": 590,\\n            \"input_token_details\": {\\n                \"audio\": 10,\\n                \"cache_creation\": 200,\\n                \"cache_read\": 100,\\n            },\\n            \"output_token_details\": {\\n                \"audio\": 10,\\n                \"reasoning\": 200,\\n            }\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.',\n",
       "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
       "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
       "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'},\n",
       "    'input_token_details': {'$ref': '#/$defs/InputTokenDetails'},\n",
       "    'output_token_details': {'$ref': '#/$defs/OutputTokenDetails'}},\n",
       "   'required': ['input_tokens', 'output_tokens', 'total_tokens'],\n",
       "   'title': 'UsageMetadata',\n",
       "   'type': 'object'}},\n",
       " 'anyOf': [{'type': 'string'},\n",
       "  {'$ref': '#/$defs/StringPromptValue'},\n",
       "  {'$ref': '#/$defs/ChatPromptValueConcrete'},\n",
       "  {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'},\n",
       "     {'$ref': '#/$defs/HumanMessage'},\n",
       "     {'$ref': '#/$defs/ChatMessage'},\n",
       "     {'$ref': '#/$defs/SystemMessage'},\n",
       "     {'$ref': '#/$defs/FunctionMessage'},\n",
       "     {'$ref': '#/$defs/ToolMessage'},\n",
       "     {'$ref': '#/$defs/AIMessageChunk'},\n",
       "     {'$ref': '#/$defs/HumanMessageChunk'},\n",
       "     {'$ref': '#/$defs/ChatMessageChunk'},\n",
       "     {'$ref': '#/$defs/SystemMessageChunk'},\n",
       "     {'$ref': '#/$defs/FunctionMessageChunk'},\n",
       "     {'$ref': '#/$defs/ToolMessageChunk'}]},\n",
       "   'type': 'array'}],\n",
       " 'title': 'ChatOllamaInput'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cd49f86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:45:58.273516Z",
     "start_time": "2023-10-29T09:45:58.258704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'AIMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ai',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessage',\n",
       "   'type': 'object'},\n",
       "  'AIMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Message chunk from an AI.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'AIMessageChunk',\n",
       "     'default': 'AIMessageChunk',\n",
       "     'enum': ['AIMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None},\n",
       "    'tool_call_chunks': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCallChunk'},\n",
       "     'title': 'Tool Call Chunks',\n",
       "     'type': 'array'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ChatMessage': {'additionalProperties': True,\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'chat',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessage',\n",
       "   'type': 'object'},\n",
       "  'ChatMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Chat Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ChatMessageChunk',\n",
       "     'default': 'ChatMessageChunk',\n",
       "     'enum': ['ChatMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'function',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessage',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Function Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'FunctionMessageChunk',\n",
       "     'default': 'FunctionMessageChunk',\n",
       "     'enum': ['FunctionMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'HumanMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'human',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessage',\n",
       "   'type': 'object'},\n",
       "  'HumanMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Human Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'HumanMessageChunk',\n",
       "     'default': 'HumanMessageChunk',\n",
       "     'enum': ['HumanMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'InputTokenDetails': {'description': 'Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"cache_creation\": 200,\\n            \"cache_read\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'cache_creation': {'title': 'Cache Creation', 'type': 'integer'},\n",
       "    'cache_read': {'title': 'Cache Read', 'type': 'integer'}},\n",
       "   'title': 'InputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'InvalidToolCall': {'description': 'Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Error'},\n",
       "    'type': {'const': 'invalid_tool_call',\n",
       "     'enum': ['invalid_tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error'],\n",
       "   'title': 'InvalidToolCall',\n",
       "   'type': 'object'},\n",
       "  'OutputTokenDetails': {'description': 'Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"reasoning\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'reasoning': {'title': 'Reasoning', 'type': 'integer'}},\n",
       "   'title': 'OutputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'SystemMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'system',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessage',\n",
       "   'type': 'object'},\n",
       "  'SystemMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'System Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'SystemMessageChunk',\n",
       "     'default': 'SystemMessageChunk',\n",
       "     'enum': ['SystemMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolCall': {'description': 'Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"name\": \"foo\",\\n            \"args\": {\"a\": 1},\\n            \"id\": \"123\"\\n        }\\n\\n    This represents a request to call the tool named \"foo\" with arguments {\"a\": 1}\\n    and an identifier of \"123\".',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'object'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'type': {'const': 'tool_call',\n",
       "     'enum': ['tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id'],\n",
       "   'title': 'ToolCall',\n",
       "   'type': 'object'},\n",
       "  'ToolCallChunk': {'description': 'A chunk of a tool call (e.g., as part of a stream).\\n\\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    left_chunks = [ToolCallChunk(name=\"foo\", args=\\'{\"a\":\\', index=0)]\\n    right_chunks = [ToolCallChunk(name=None, args=\\'1}\\', index=0)]\\n\\n    (\\n        AIMessageChunk(content=\"\", tool_call_chunks=left_chunks)\\n        + AIMessageChunk(content=\"\", tool_call_chunks=right_chunks)\\n    ).tool_call_chunks == [ToolCallChunk(name=\\'foo\\', args=\\'{\"a\":1}\\', index=0)]',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "     'title': 'Index'},\n",
       "    'type': {'const': 'tool_call_chunk',\n",
       "     'enum': ['tool_call_chunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'index'],\n",
       "   'title': 'ToolCallChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A ToolMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content=\\'42\\', tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\')\\n\\n\\nExample: A ToolMessage where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    .. versionadded:: 0.2.17\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        tool_output = {\\n            \"stdout\": \"From the graph we can see that the correlation between x and y is ...\",\\n            \"stderr\": None,\\n            \"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"},\\n        }\\n\\n        ToolMessage(\\n            content=tool_output[\"stdout\"],\\n            artifact=tool_output,\\n            tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\',\\n        )\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'tool',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessage',\n",
       "   'type': 'object'},\n",
       "  'ToolMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Tool Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ToolMessageChunk',\n",
       "     'default': 'ToolMessageChunk',\n",
       "     'enum': ['ToolMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'UsageMetadata': {'description': 'Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"input_tokens\": 350,\\n            \"output_tokens\": 240,\\n            \"total_tokens\": 590,\\n            \"input_token_details\": {\\n                \"audio\": 10,\\n                \"cache_creation\": 200,\\n                \"cache_read\": 100,\\n            },\\n            \"output_token_details\": {\\n                \"audio\": 10,\\n                \"reasoning\": 200,\\n            }\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.',\n",
       "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
       "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
       "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'},\n",
       "    'input_token_details': {'$ref': '#/$defs/InputTokenDetails'},\n",
       "    'output_token_details': {'$ref': '#/$defs/OutputTokenDetails'}},\n",
       "   'required': ['input_tokens', 'output_tokens', 'total_tokens'],\n",
       "   'title': 'UsageMetadata',\n",
       "   'type': 'object'}},\n",
       " 'oneOf': [{'$ref': '#/$defs/AIMessage'},\n",
       "  {'$ref': '#/$defs/HumanMessage'},\n",
       "  {'$ref': '#/$defs/ChatMessage'},\n",
       "  {'$ref': '#/$defs/SystemMessage'},\n",
       "  {'$ref': '#/$defs/FunctionMessage'},\n",
       "  {'$ref': '#/$defs/ToolMessage'},\n",
       "  {'$ref': '#/$defs/AIMessageChunk'},\n",
       "  {'$ref': '#/$defs/HumanMessageChunk'},\n",
       "  {'$ref': '#/$defs/ChatMessageChunk'},\n",
       "  {'$ref': '#/$defs/SystemMessageChunk'},\n",
       "  {'$ref': '#/$defs/FunctionMessageChunk'},\n",
       "  {'$ref': '#/$defs/ToolMessageChunk'}],\n",
       " 'title': 'ChatOllamaOutput'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.output_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ee554e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:41:48.037384Z",
     "start_time": "2023-10-29T09:41:45.609133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Why did the football go to the doctor?\\n\\nBecause it was feeling a little deflated.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:38:57.026434Z'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">249027958</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28573500</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18000000</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200000000</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-83caea35-a628-4ad4-8417-14816cbbda90-0'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'Why did the football go to the doctor?\\n\\nBecause it was feeling a little deflated.'\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "        \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:38:57.026434Z'\u001b[0m,\n",
       "        \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "        \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m249027958\u001b[0m,\n",
       "        \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m28573500\u001b[0m,\n",
       "        \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "        \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m18000000\u001b[0m,\n",
       "        \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m19\u001b[0m,\n",
       "        \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m200000000\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run-83caea35-a628-4ad4-8417-14816cbbda90-0'\u001b[0m,\n",
       "    \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m19\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m49\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(chain.invoke({\"topic\": \"Football\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa8e34d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:41:51.030697Z",
     "start_time": "2023-10-29T09:41:48.039911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Why did Love go to therapy?\\n\\nBecause it was feeling a little \"unhinged.\"'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:38:57.213477Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">178958041</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12742166</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-ae289a2f-8dff-4a30-9807-76faeb773b30-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'A man walked into a library and asked the librarian, \"Do you have any books on Pavlov\\'s dogs and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">SchrÃ¶dinger\\'s cat?\" The librarian replied, \"It rings a bell, but I\\'m not sure if it\\'s here or not.\"'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:38:57.456737Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">421970500</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12649042</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">54</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">368000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-c007be30-bac8-4b54-bc83-7e0c1caba124-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">54</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'Why did Love go to therapy?\\n\\nBecause it was feeling a little \"unhinged.\"'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:38:57.213477Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m178958041\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m12742166\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m14000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m19\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m151000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-ae289a2f-8dff-4a30-9807-76faeb773b30-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m19\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m49\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'A man walked into a library and asked the librarian, \"Do you have any books on Pavlov\\'s dogs and \u001b[0m\n",
       "\u001b[32mSchrÃ¶dinger\\'s cat?\" The librarian replied, \"It rings a bell, but I\\'m not sure if it\\'s here or not.\"'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:38:57.456737Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m421970500\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m12649042\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m40000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m54\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m368000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-c007be30-bac8-4b54-bc83-7e0c1caba124-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m54\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m84\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(chain.batch([{\"topic\": \"Love\"}, {\"topic\": \"Romance\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1f91dd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:42:01.735621Z",
     "start_time": "2023-10-29T09:41:51.030697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'A programmer walks into a library and asks the librarian, \"Do you have any books on computer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">science?\" The librarian replies, \"It\\'s a bit of a \\'byte\\' size problem, but I think we can \\'debug\\' that for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you.\"'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:38:57.871531Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">405698292</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10778375</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">51</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-a53c9965-2029-493d-9703-75636a12fea5-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">51</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">81</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'A man walked into a library and asked the librarian, \"Do you have any books on Pavlov\\'s dogs and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">SchrÃ¶dinger\\'s cat?\"\\n\\nThe librarian replied, \"It rings a bell, but I\\'m not sure if it\\'s here or not.\"'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:38:57.904136Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">438267250</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11209792</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">54</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">396000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-65371929-9f19-4965-986b-5270024afc6d-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">54</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">85</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'A programmer walks into a library and asks the librarian, \"Do you have any books on computer \u001b[0m\n",
       "\u001b[32mscience?\" The librarian replies, \"It\\'s a bit of a \\'byte\\' size problem, but I think we can \\'debug\\' that for \u001b[0m\n",
       "\u001b[32myou.\"'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:38:57.871531Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m405698292\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m10778375\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m10000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m51\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m384000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-a53c9965-2029-493d-9703-75636a12fea5-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m51\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m81\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'A man walked into a library and asked the librarian, \"Do you have any books on Pavlov\\'s dogs and \u001b[0m\n",
       "\u001b[32mSchrÃ¶dinger\\'s cat?\"\\n\\nThe librarian replied, \"It rings a bell, but I\\'m not sure if it\\'s here or not.\"'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:38:57.904136Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m438267250\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m11209792\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m31\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m29000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m54\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m396000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-65371929-9f19-4965-986b-5270024afc6d-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m31\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m54\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m85\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(chain.batch([{\"topic\": \"Coding\"}, {\"topic\": \"Travelling\"}], config={\"max_concurrency\": 5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f3c38",
   "metadata": {},
   "source": [
    "### Async Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58446641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:42:05.006902Z",
     "start_time": "2023-10-29T09:42:01.741527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the satellite go to therapy?\n",
      "\n",
      "Because it had a lot of \"orbital\" issues and was feeling \"out of balance.\""
     ]
    }
   ],
   "source": [
    "async for s in chain.astream({\"topic\": \"Satellites\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4f14b",
   "metadata": {},
   "source": [
    "### Async Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "642ef543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:42:08.994743Z",
     "start_time": "2023-10-29T09:42:06.770310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the pizza in a bad mood?\n",
      "\n",
      "Because it was feeling crusty."
     ]
    }
   ],
   "source": [
    "async for s in chain.astream({\"topic\": \"Food\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d720522f",
   "metadata": {},
   "source": [
    "### Async Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "664206fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:42:25.212820Z",
     "start_time": "2023-10-29T09:42:16.009974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'A man walks into a library and asks the librarian, \"Do you have any books on Pavlov\\'s dogs and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">SchrÃ¶dinger\\'s cat?\" \\n\\nThe librarian replies, \"It rings a bell, but I\\'m not sure if it\\'s here or not.\"'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:38:58.674882Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">411619167</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10872167</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">393000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-afcc35c5-fbb5-40b1-8468-d651785999d4-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">85</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Why did the lollipop go to therapy?\\n\\nBecause it was feeling a little \"sour\" and wanted to work </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">through some \"candy\" issues.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:38:58.541922Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">277920458</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10285125</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">240000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-c28de374-190c-4bcf-9e7e-4cd46495ebb6-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">63</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'A man walks into a library and asks the librarian, \"Do you have any books on Pavlov\\'s dogs and \u001b[0m\n",
       "\u001b[32mSchrÃ¶dinger\\'s cat?\" \\n\\nThe librarian replies, \"It rings a bell, but I\\'m not sure if it\\'s here or not.\"'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:38:58.674882Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m411619167\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m10872167\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m6000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m55\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m393000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-afcc35c5-fbb5-40b1-8468-d651785999d4-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m55\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m85\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'Why did the lollipop go to therapy?\\n\\nBecause it was feeling a little \"sour\" and wanted to work \u001b[0m\n",
       "\u001b[32mthrough some \"candy\" issues.'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:38:58.541922Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m277920458\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m10285125\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m31\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m26000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m32\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m240000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-c28de374-190c-4bcf-9e7e-4cd46495ebb6-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m31\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m32\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m63\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c =await chain.abatch([{\"topic\": \"Food\"}, {\"topic\": \"Sweets\"}])\n",
    "pp(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca7524",
   "metadata": {},
   "source": [
    "### Async Stream with intermediate steps\n",
    "\n",
    "Useful for displaying progress to the user, working with intermediate results, and debugging chains. You can stream all steps (default) or include or exclude steps by name, tags, or metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9585d2fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:34:13.450694Z",
     "start_time": "2023-10-29T09:34:09.769058Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafa4c7",
   "metadata": {},
   "source": [
    "Execute the Retriever chain and output intermediate steps using astream_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc1189b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:48:13.824301Z",
     "start_time": "2023-10-29T09:48:09.911566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableParallel</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">steps__</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableBinding</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">bound</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">VectorStoreRetriever</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tags</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'FAISS'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'OllamaEmbeddings'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">vectorstore</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">langchain_community.vectorstores.faiss.FAISS</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x12fe1dc90</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">search_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'run_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Docs'</span><span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">config_factories</span>=<span style=\"font-weight: bold\">[]</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnablePassthrough</span><span style=\"font-weight: bold\">()</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span><span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Please answer the question based only on the following </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context:\\n{context}\\n\\nQuestion: {question}\\n'</span>\n",
       "                    <span style=\"font-weight: bold\">)</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOllama</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StrOutputParser</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfirst\u001b[0m=\u001b[1;35mRunnableParallel\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33msteps__\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'context'\u001b[0m: \u001b[1;35mRunnableBinding\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mbound\u001b[0m=\u001b[1;35mVectorStoreRetriever\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mtags\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'FAISS'\u001b[0m, \u001b[32m'OllamaEmbeddings'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                    \u001b[33mvectorstore\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mlangchain_community.vectorstores.faiss.FAISS\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x12fe1dc90\u001b[0m\u001b[1m>\u001b[0m,\n",
       "                    \u001b[33msearch_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33mkwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mconfig\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'run_name'\u001b[0m: \u001b[32m'Docs'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mconfig_factories\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[32m'question'\u001b[0m: \u001b[1;35mRunnablePassthrough\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'context'\u001b[0m, \u001b[32m'question'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'context'\u001b[0m, \u001b[32m'question'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                        \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                        \u001b[33mtemplate\u001b[0m=\u001b[32m'Please answer the question based only on the following \u001b[0m\n",
       "\u001b[32mcontext:\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32mcontext\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\nQuestion: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mquestion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "                    \u001b[1m)\u001b[0m,\n",
       "                    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mChatOllama\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2:1b'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mlast\u001b[0m=\u001b[1;35mStrOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': '9df6d72d-2382-4695-a640-184d5a48c213',\n",
      "            'logs': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': 'ebdb5324-ee49-41e2-8779-2f29faa5042c',\n",
      "            'metadata': {'ls_embedding_provider': 'OllamaEmbeddings',\n",
      "                         'ls_retriever_name': 'vectorstore',\n",
      "                         'ls_vector_store_provider': 'FAISS'},\n",
      "            'name': 'Docs',\n",
      "            'start_time': '2025-01-13T16:42:27.911+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "            'type': 'retriever'}})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs/final_output',\n",
      "  'value': {'documents': [Document(metadata={}, page_content='Sonu is the creator of AI Anytime Youtube Channel')]}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/Docs/end_time',\n",
      "  'value': '2025-01-13T16:42:27.926+00:00'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'The'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'The'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' creator'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'The creator'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' of'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'The creator of'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' AI'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'The creator of AI'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Any'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'The creator of AI Any'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'time'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'The creator of AI Anytime'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'The creator of AI Anytime is'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Son'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'The creator of AI Anytime is Son'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'u'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'The creator of AI Anytime is Sonu'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'The creator of AI Anytime is Sonu.'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''})\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings \n",
    "\n",
    "template = \"\"\"Please answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "vectorstore = FAISS.from_texts([\"Sonu is the creator of AI Anytime Youtube Channel\"], embedding=OllamaEmbeddings(model=\"snowflake-arctic-embed:33m\"))\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever.with_config(run_name='Docs'), \"question\": RunnablePassthrough()}\n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "pp(retrieval_chain)\n",
    "async for chunk in retrieval_chain.astream_log(\"Who is the creator of AI Anytime?\", include_names=['Docs']):\n",
    "    print(\"-\"*40)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a18759",
   "metadata": {},
   "source": [
    "## Parallel Processing\n",
    "\n",
    "\"RunnableParallel\" allows each element to run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a6e4db7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:50:40.731704Z",
     "start_time": "2023-10-29T09:50:40.723288Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "chain1 = ChatPromptTemplate.from_template(\"Tell a joke about {topic}\") | model\n",
    "chain2 = ChatPromptTemplate.from_template(\"Write a short (2 line) poem about {topic}\") | model\n",
    "combined = RunnableParallel(joke=chain1, poem=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eb7085c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:51:16.921592Z",
     "start_time": "2023-10-29T09:51:13.214684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'A robot walked into a bar and ordered a beer. As he was sipping his drink, he said to the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">bartender, \"I\\'m feeling a little glitchy today.\" The bartender replied, \"Don\\'t worry, it\\'s just a software </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">bug.\"'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:39:01.727207Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">479388458</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">45991458</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">422000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-5266cb51-3c15-47af-bd4d-3b020d6d36ad-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">82</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'A mathematician walks into a library and asks the librarian, \"Do you have any books on Pavlov\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dogs and SchrÃ¶dinger\\'s cat?\" The librarian replies, \"It rings a bell, but I\\'m not sure if it\\'s here or not.\"'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:39:01.759563Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">511687833</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">46054375</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">430000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-709f065c-4f43-4b22-b825-96060d261e8c-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">85</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'A robot walked into a bar and ordered a beer. As he was sipping his drink, he said to the \u001b[0m\n",
       "\u001b[32mbartender, \"I\\'m feeling a little glitchy today.\" The bartender replied, \"Don\\'t worry, it\\'s just a software \u001b[0m\n",
       "\u001b[32mbug.\"'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:39:01.727207Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m479388458\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m45991458\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m10000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m52\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m422000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-5266cb51-3c15-47af-bd4d-3b020d6d36ad-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m52\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m82\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'A mathematician walks into a library and asks the librarian, \"Do you have any books on Pavlov\\'s \u001b[0m\n",
       "\u001b[32mdogs and SchrÃ¶dinger\\'s cat?\" The librarian replies, \"It rings a bell, but I\\'m not sure if it\\'s here or not.\"'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:39:01.759563Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m511687833\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m46054375\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m34000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m55\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m430000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-709f065c-4f43-4b22-b825-96060d261e8c-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m55\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m85\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 ms, sys: 3.01 ms, total: 20.9 ms\n",
      "Wall time: 519 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pp(chain1.batch([{\"topic\": \"AI\"}, {\"topic\": \"Math\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42386966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:52:21.291731Z",
     "start_time": "2023-10-29T09:52:18.287272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Here is a short poem about science:\\n\\n\"Curiosity drives us to explore and learn,\\nUnlocking </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">secrets, one discovery at a time.\"'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:39:02.018836Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">248072000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11003958</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">226000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-d1968b67-d27a-4c9d-a978-0bf5052034fb-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Here is a short poem about mango:\\n\\nSweet and juicy, ripe and fine,\\nMango's flavor is truly </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">divine.\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:39:02.005809Z'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">235011666</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11094250</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35000000</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">188000000</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-ea58a397-0db0-47f1-ad16-ab1425f7e119-0'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'Here is a short poem about science:\\n\\n\"Curiosity drives us to explore and learn,\\nUnlocking \u001b[0m\n",
       "\u001b[32msecrets, one discovery at a time.\"'\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:39:02.018836Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m248072000\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m11003958\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m35\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m10000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m29\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m226000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-d1968b67-d27a-4c9d-a978-0bf5052034fb-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m35\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m29\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m64\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m\"Here\u001b[0m\u001b[32m is a short poem about mango:\\n\\nSweet and juicy, ripe and fine,\\nMango's flavor is truly \u001b[0m\n",
       "\u001b[32mdivine.\"\u001b[0m,\n",
       "        \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "            \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:39:02.005809Z'\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m235011666\u001b[0m,\n",
       "            \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m11094250\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m35\u001b[0m,\n",
       "            \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m35000000\u001b[0m,\n",
       "            \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m25\u001b[0m,\n",
       "            \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m188000000\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mid\u001b[0m=\u001b[32m'run-ea58a397-0db0-47f1-ad16-ab1425f7e119-0'\u001b[0m,\n",
       "        \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m35\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m25\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m60\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 ms, sys: 2.03 ms, total: 13.9 ms\n",
      "Wall time: 254 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pp(chain2.batch([{\"topic\": \"Science\"}, {\"topic\": \"Mango\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67c6f1d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T09:53:14.695511Z",
     "start_time": "2023-10-29T09:53:11.944824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'joke'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Why did the AI program go to therapy?\\n\\nBecause it was feeling a little \" glitchy\" and had </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">trouble processing its emotions.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:39:02.351122Z'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">322975417</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14902458</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7000000</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300000000</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-e1f34da2-9fbe-42d2-bdf6-fc644f2e4aac-0'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'poem'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Here's a short poem about AI:\\n\\nRise of the machines, a new mind\\nConsciousness born, with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">code entwined.\"</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:39:02.396088Z'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">366919583</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14686625</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32000000</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">318000000</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-22b7371d-cef4-486f-b04a-e31c2f799686-0'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'joke'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'A man went to the doctor and said, \"Doc, I\\'ve been feeling really off lately. I\\'ve got a lot</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of altitude and it\\'s been making my life feel like it\\'s on the slope.\" The doctor replied, \"Well, you\\'re not </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">just mountain-high, you\\'re just in need of a boost!\"'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:39:02.624802Z'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">595130500</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13997875</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32000000</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">66</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">546000000</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-abe43a2c-5c20-4531-b025-efd0ffb1ac34-0'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">66</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">96</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'poem'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Here is a short two-line poem about mountains:\\n\\nGranite peaks touch the sky so </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">high,\\nNature's beauty, touching the eye.\"</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-01-13T16:39:02.380606Z'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'done'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">350821708</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'load_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14642333</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33000000</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'eval_duration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">301000000</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-ca552c38-9c62-486f-b067-d2a638e95fb3-0'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">63</span><span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'joke'\u001b[0m: \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m'Why did the AI program go to therapy?\\n\\nBecause it was feeling a little \" glitchy\" and had \u001b[0m\n",
       "\u001b[32mtrouble processing its emotions.'\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "                \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:39:02.351122Z'\u001b[0m,\n",
       "                \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "                \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m322975417\u001b[0m,\n",
       "                \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m14902458\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m7000000\u001b[0m,\n",
       "                \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m27\u001b[0m,\n",
       "                \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m300000000\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'run-e1f34da2-9fbe-42d2-bdf6-fc644f2e4aac-0'\u001b[0m,\n",
       "            \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m27\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m57\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'poem'\u001b[0m: \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m\"Here\u001b[0m\u001b[32m's a short poem about AI:\\n\\nRise of the machines, a new mind\\nConsciousness born, with \u001b[0m\n",
       "\u001b[32mcode entwined.\"\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "                \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:39:02.396088Z'\u001b[0m,\n",
       "                \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "                \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m366919583\u001b[0m,\n",
       "                \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m14686625\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m35\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m32000000\u001b[0m,\n",
       "                \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "                \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m318000000\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'run-22b7371d-cef4-486f-b04a-e31c2f799686-0'\u001b[0m,\n",
       "            \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m35\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m65\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'joke'\u001b[0m: \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m'A man went to the doctor and said, \"Doc, I\\'ve been feeling really off lately. I\\'ve got a lot\u001b[0m\n",
       "\u001b[32mof altitude and it\\'s been making my life feel like it\\'s on the slope.\" The doctor replied, \"Well, you\\'re not \u001b[0m\n",
       "\u001b[32mjust mountain-high, you\\'re just in need of a boost!\"'\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "                \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:39:02.624802Z'\u001b[0m,\n",
       "                \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "                \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m595130500\u001b[0m,\n",
       "                \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m13997875\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m30\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m32000000\u001b[0m,\n",
       "                \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m66\u001b[0m,\n",
       "                \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m546000000\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'run-abe43a2c-5c20-4531-b025-efd0ffb1ac34-0'\u001b[0m,\n",
       "            \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m30\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m66\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m96\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'poem'\u001b[0m: \u001b[1;35mAIMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m\"Here\u001b[0m\u001b[32m is a short two-line poem about mountains:\\n\\nGranite peaks touch the sky so \u001b[0m\n",
       "\u001b[32mhigh,\\nNature's beauty, touching the eye.\"\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'model'\u001b[0m: \u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "                \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-01-13T16:39:02.380606Z'\u001b[0m,\n",
       "                \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'done_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                \u001b[32m'done'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "                \u001b[32m'total_duration'\u001b[0m: \u001b[1;36m350821708\u001b[0m,\n",
       "                \u001b[32m'load_duration'\u001b[0m: \u001b[1;36m14642333\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_count'\u001b[0m: \u001b[1;36m35\u001b[0m,\n",
       "                \u001b[32m'prompt_eval_duration'\u001b[0m: \u001b[1;36m33000000\u001b[0m,\n",
       "                \u001b[32m'eval_count'\u001b[0m: \u001b[1;36m28\u001b[0m,\n",
       "                \u001b[32m'eval_duration'\u001b[0m: \u001b[1;36m301000000\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'run-ca552c38-9c62-486f-b067-d2a638e95fb3-0'\u001b[0m,\n",
       "            \u001b[33musage_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m35\u001b[0m, \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m28\u001b[0m, \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m63\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 ms, sys: 4.66 ms, total: 33.6 ms\n",
      "Wall time: 611 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# combined\n",
    "pp(combined.batch([{\"topic\": \"AI\"}, {\"topic\": \"Mountains\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501126ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a6ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
