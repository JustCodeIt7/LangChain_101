{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from rich import print as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With stream "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| color| of| the| sky| can| vary| depending| on| the| time| of| day|,| atmospheric| conditions|,| and| other| factors|.| At| sunrise| and| sunset|,| the| sky| can| take| on| a| range| of| colors|,| from| pink| to| orange| to| purple|,| due| to| the| scattering| of| light| by| atmospheric| particles|.\n",
      "\n",
      "|During| the| daytime|,| when| the| sun| is| overhead|,| the| sky| typically| appears| blue| to| our| eyes|.| This| is| because| blue| light| is| scattered| in| all| directions| by| the| tiny| molecules| of| gases| in| the| Earth|'s| atmosphere|,| such| as| nitrogen| and| oxygen|.\n",
      "\n",
      "|However|,| if| there| are| clouds| or| other| objects| in| the| air|,| they| can| scatter| light| in| different| ways|,| which| can| cause| the| sky| to| take| on| a| range| of| colors|,| including| gray|,| white|,| or| even| red|.||"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for chunk in model.stream(\"what color is the sky?\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with astream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| color| of| the| sky| can| vary| depending| on| the| time| of| day|,| atmospheric| conditions|,| and| other| factors|.| During| the| daytime|,| when| the| sun| is| overhead|,| the| sky| typically| appears| blue| due| to| a| phenomenon| called| Ray|leigh| scattering|,| in| which| shorter| wavelengths| of| light| (|like| blue| and| violet|)| are| scattered| more| than| longer| wavelengths| (|like| red| and| orange|)| by| the| tiny| molecules| of| gases| in| the| atmosphere|.\n",
      "\n",
      "|However|,| at| sunrise| and| sunset|,| the| sky| can| take| on| a| range| of| colors|,| from| pink| and| redd|ish| to| orange| and| purple|,| due| to| the| scattering| of| light| by| atmospheric| particles| and| dust|.| This| is| known| as| M|ie| scattering|.\n",
      "\n",
      "|In| general|,| though|,| the| sky| is| usually| blue| during| the| daytime|.||"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "async for chunk in model.astream(\"what color is the sky?\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessageChunk</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'run-accac619-e706-49b0-8ce1-b46d600d2535'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mAIMessageChunk\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'The'\u001b[0m,\n",
       "    \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'run-accac619-e706-49b0-8ce1-b46d600d2535'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why| did| the| par|rot| go| to| the| doctor|?\n",
      "\n",
      "|Because| it| had| a| f|owl| temper|.||"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "async for chunk in chain.astream({\"topic\": \"parrot\"}):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Input Streams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'countries': []}\n",
      "{'countries': [{}]}\n",
      "{'countries': [{'name': ''}]}\n",
      "{'countries': [{'name': 'France'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67}]}\n",
      "{'countries': [{'name': 'France', 'population': 67.2}]}\n",
      "{'countries': [{'name': 'France', 'population': 67200000.0}]}\n",
      "{'countries': [{'name': 'France', 'population': 67200000.0}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 67200000.0}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 67200000.0}, {'name': 'Spain'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67200000.0}, {'name': 'Spain', 'population': 46}]}\n",
      "{'countries': [{'name': 'France', 'population': 67200000.0}, {'name': 'Spain', 'population': 46.7}]}\n",
      "{'countries': [{'name': 'France', 'population': 67200000.0}, {'name': 'Spain', 'population': 46700000.0}]}\n",
      "{'countries': [{'name': 'France', 'population': 67200000.0}, {'name': 'Spain', 'population': 46700000.0}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 67200000.0}, {'name': 'Spain', 'population': 46700000.0}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 67200000.0}, {'name': 'Spain', 'population': 46700000.0}, {'name': 'Japan'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67200000.0}, {'name': 'Spain', 'population': 46700000.0}, {'name': 'Japan', 'population': 127}]}\n",
      "{'countries': [{'name': 'France', 'population': 67200000.0}, {'name': 'Spain', 'population': 46700000.0}, {'name': 'Japan', 'population': 127.5}]}\n",
      "{'countries': [{'name': 'France', 'population': 67200000.0}, {'name': 'Spain', 'population': 46700000.0}, {'name': 'Japan', 'population': 127500000.0}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = (\n",
    "    model | JsonOutputParser()\n",
    ")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n",
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\"\n",
    "):\n",
    "    print(text, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-streaming components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={}, page_content='harrison worked at kensho'),\n",
       "  Document(metadata={}, page_content='harrison likes spicy food')]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\", \"harrison likes spicy food\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
