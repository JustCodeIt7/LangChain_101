{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3de1a6-11f8-47c6-a5df-ef2dd741c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Expression Language (LCEL) Tutorial\n",
    "# For YouTube Educational Content\n",
    "# %%\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "\n",
    "# Import LangChain components\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_ollama import ChatOllama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf28b97-9f2e-4cc9-8084-3231b509bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8702e-dcb3-4fdd-b8fd-2972a03b5e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ###############################################################\n",
    "# SECTION 1: Basic LCEL Chain\n",
    "# ###############################################################\n",
    "print(\"-\" * 50, '\\n', \"DEMO 1: Basic LCEL Chain\", '\\n', \"-\" * 50)\n",
    "\n",
    "# Create a simple prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}.\")\n",
    "\n",
    "# Create a basic chain using the | operator (pipe)\n",
    "basic_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run the chain\n",
    "result = basic_chain.invoke({\"topic\": \"programming\"})\n",
    "print(result)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6ac38-f6a7-4b7e-9d8f-8a2786a1a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ###############################################################\n",
    "# SECTION 2: Parallel Processing with LCEL\n",
    "# ###############################################################\n",
    "\n",
    "print(\"-\" * 50, '\\n', \"DEMO 2: Parallel Processing with LCEL\", '\\n', \"-\" * 50)\n",
    "\n",
    "# Create two different prompts\n",
    "pros_prompt = ChatPromptTemplate.from_template(\n",
    "    \"List 3 pros of {technology} technology.\"\n",
    ")\n",
    "cons_prompt = ChatPromptTemplate.from_template(\n",
    "    \"List 3 cons of {technology} technology.\"\n",
    ")\n",
    "\n",
    "# Create chains for each prompt\n",
    "pros_chain = pros_prompt | llm | StrOutputParser()\n",
    "cons_chain = cons_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Create a parallel chain\n",
    "parallel_chain = RunnableParallel(pros=pros_chain, cons=cons_chain)\n",
    "\n",
    "# Run the parallel chain\n",
    "parallel_result = parallel_chain.invoke({\"technology\": \"blockchain\"})\n",
    "print(\"PROS:\")\n",
    "print(parallel_result[\"pros\"])\n",
    "print(\"\\nCONS:\")\n",
    "print(parallel_result[\"cons\"])\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283b300-b2a9-441a-a29f-fd7bfce2f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ###############################################################\n",
    "# SECTION 3: Advanced Chain with Data Transformation\n",
    "# ###############################################################\n",
    "print(\"-\" * 50, '\\n', \"DEMO 3: Advanced Chain with Data Transformation\", '\\n', \"-\" * 50)\n",
    "\n",
    "# Define a function to process input\n",
    "def preprocess_input(input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Process the input data before sending to the LLM.\"\"\"\n",
    "    # Add a timestamp, capitalize the query, etc.\n",
    "    processed_data = input_data.copy()\n",
    "    processed_data[\"query\"] = input_data[\"query\"].upper()\n",
    "    processed_data[\"word_count\"] = len(input_data[\"query\"].split())\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# Define a function to process output\n",
    "def postprocess_output(output: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process the output from the LLM.\"\"\"\n",
    "    lines = output.strip().split(\"\\n\")\n",
    "    return {\n",
    "        \"summary\": lines[0] if lines else \"\",\n",
    "        \"details\": lines[1:],\n",
    "        \"response_length\": len(output),\n",
    "    }\n",
    "\n",
    "\n",
    "# Create an advanced prompt\n",
    "advanced_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following query: {query}\n",
    "\n",
    "    The query has {word_count} words.\n",
    "    Provide a detailed explanation.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create an advanced chain with preprocessing and postprocessing\n",
    "advanced_chain = (\n",
    "    RunnableLambda(preprocess_input)\n",
    "    | advanced_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | RunnableLambda(postprocess_output)\n",
    ")\n",
    "\n",
    "# Run the advanced chain\n",
    "advanced_result = advanced_chain.invoke(\n",
    "    {\"query\": \"how does LCEL improve LLM applications\"}\n",
    ")\n",
    "print(f\"Summary: {advanced_result['summary']}\")\n",
    "print(\"Details:\")\n",
    "for detail in advanced_result[\"details\"]:\n",
    "    if detail.strip():\n",
    "        print(f\"- {detail.strip()}\")\n",
    "print(f\"Response Length: {advanced_result['response_length']} characters\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a04b4e9-df59-4607-9cb7-fb277a7f3a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ###############################################################\n",
    "# SECTION 4: RAG Pattern with LCEL\n",
    "# ###############################################################\n",
    "print(\"-\" * 50, '\\n', \"DEMO 4: RAG Pattern with LCEL (Simulated)\", '\\n', \"-\" * 50)\n",
    "\n",
    "\n",
    "# Simulate a retriever function\n",
    "def simulate_retrieval(query: str) -> List[str]:\n",
    "    \"\"\"Simulate document retrieval based on query.\"\"\"\n",
    "    # In a real application, this would query a vector database\n",
    "    documents = [\n",
    "        \"LCEL allows for declarative chain construction in LangChain.\",\n",
    "        \"LangChain Expression Language simplifies building complex LLM applications.\",\n",
    "        \"LCEL supports streaming, async operations, and parallel execution.\",\n",
    "    ]\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Create a RAG prompt\n",
    "rag_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based on the following context:\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create a RAG chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": RunnableLambda(simulate_retrieval) | RunnableLambda(format_docs),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Run the RAG chain\n",
    "rag_result = rag_chain.invoke(\"What is LCEL and why is it useful?\")\n",
    "print(rag_result)\n",
    "\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
