{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "The **L**ang**C**hain **E**xpression **L**anguage (LCEL) abstracts key Python concepts into a streamlined format, facilitating a \"minimalist\" code layer for constructing chains of LangChain components. LCEL offers robust support for:\n",
    "\n",
    "1. Rapid development of chains.\n",
    "2. Advanced features like streaming, asynchronous processing, parallel execution, and more.\n",
    "3. Seamless integration with LangSmith and LangServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "#     langchain==0.0.345 \\\n",
    "#     anthropic==0.7.7 \\\n",
    "#     cohere==4.37 \\\n",
    "#     docarray==0.39.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import mod\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from rich import print as pp\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Give me video ideas for the topic of {topic}\"\n",
    ")\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IIn a standard LangChain setup, we would connect these components using an LLMChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/yjmck8kn59gc9w3kdklj2lt40000gn/T/ipykernel_99256/4093089756.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n",
      "/var/folders/m7/yjmck8kn59gc9w3kdklj2lt40000gn/T/ipykernel_99256/4093089756.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  out = chain.run(topic=\"Artificial Intelligence\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some video ideas for the topic of Artificial Intelligence:\n",
      "\n",
      "**Explainer Videos**\n",
      "\n",
      "1. \"What is AI?\" - A beginner's guide to understanding artificial intelligence.\n",
      "2. \"How Machine Learning Works\" - An in-depth explanation of machine learning algorithms and their applications.\n",
      "3. \"The History of AI\" - A timeline of major milestones in the development of artificial intelligence.\n",
      "\n",
      "**Technology Showcase**\n",
      "\n",
      "1. \"10 Emerging AI Technologies You Need to Know\" - A showcase of cutting-edge AI technologies, including computer vision, natural language processing, and robotics.\n",
      "2. \"AI-Powered Automation: How Robots Are Changing Industries\" - A video highlighting the role of robots in various industries, such as manufacturing and logistics.\n",
      "3. \"The Future of Virtual Assistants: What You Need to Know\" - An overview of current virtual assistants like Siri, Alexa, and Google Assistant.\n",
      "\n",
      "**Success Stories**\n",
      "\n",
      "1. \"How AI Is Revolutionizing Healthcare\" - A case study on how artificial intelligence is improving medical diagnosis, treatment, and patient care.\n",
      "2. \"AI-Powered Personalized Medicine: Success Story\" - A video highlighting a company that uses AI to develop personalized cancer treatments.\n",
      "3. \"How AI Is Changing Education: Success Stories\" - A showcase of AI-powered educational tools and platforms.\n",
      "\n",
      "**Interviews and Panel Discussions**\n",
      "\n",
      "1. \"Expert Insights on the Future of AI\" - An interview with a leading AI researcher or industry expert on the future of artificial intelligence.\n",
      "2. \"The Ethics of AI: A Panel Discussion\" - A video featuring experts discussing the ethics of artificial intelligence, including bias, accountability, and job displacement.\n",
      "3. \"Women in AI: Success Stories\" - A video showcasing women who have made significant contributions to the field of artificial intelligence.\n",
      "\n",
      "**Myth-Busting Videos**\n",
      "\n",
      "1. \"Debunking AI Myths: Separating Fact from Fiction\" - A video that addresses common misconceptions about artificial intelligence, such as job displacement and bias.\n",
      "2. \"The Truth About Deep Learning: Separating Hype from Reality\" - A video explaining the basics of deep learning and its applications.\n",
      "3. \"AI Is Not Taking Over the World... Yet\" - A lighthearted video that pokes fun at common AI myths and misconceptions.\n",
      "\n",
      "**Practical Guides**\n",
      "\n",
      "1. \"How to Build Your First AI Model with Python\" - A step-by-step guide on building a simple AI model using Python.\n",
      "2. \"AI-Powered Cybersecurity: How to Protect Yourself Online\" - A video highlighting the role of artificial intelligence in cybersecurity and providing tips on how to stay safe online.\n",
      "3. \"The Ultimate Guide to Natural Language Processing\" - A comprehensive guide to natural language processing, including its applications, challenges, and future directions.\n",
      "\n",
      "**Trends and Outlook**\n",
      "\n",
      "1. \"The Rise of Edge AI: What You Need to Know\" - An overview of edge AI and its impact on various industries.\n",
      "2. \"AI-Powered Automation: The Future of Work?\" - A video exploring the implications of AI-powered automation on employment and job displacement.\n",
      "3. \"What to Expect from AI in 2024: Trends and Predictions\" - A video predicting future trends and developments in artificial intelligence.\n",
      "\n",
      "These ideas should provide a good starting point for creating engaging and informative videos about artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=model,\n",
    "    output_parser=output_parser\n",
    ")\n",
    "\n",
    "# and run\n",
    "out = chain.run(topic=\"Artificial Intelligence\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LCEL the format is different, rather than relying on `Chains` we simple chain together each component using the pipe operator `|`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some video ideas on the topic of Artificial Intelligence:\n",
      "\n",
      "**Explainer Videos**\n",
      "\n",
      "1. \"What is Artificial Intelligence?\" - A beginner's guide to AI, explaining its basics and how it works.\n",
      "2. \"How AI Works\" - A step-by-step explanation of the AI process, from data collection to decision-making.\n",
      "3. \"The Difference between Machine Learning and Deep Learning\" - A comparison of these two popular AI subfields.\n",
      "\n",
      "**Case Study Videos**\n",
      "\n",
      "1. \"AI in Healthcare: Success Stories and Challenges\" - Exploring how AI is being used in healthcare, including examples of successful applications and areas for improvement.\n",
      "2. \"AI-Powered Chatbots: The Future of Customer Service\" - Showcasing the benefits and limitations of chatbot technology using AI.\n",
      "3. \"How AI is Revolutionizing Transportation\" - Examining how AI is improving safety, efficiency, and sustainability in transportation.\n",
      "\n",
      "**Interviews and Expert Insights**\n",
      "\n",
      "1. \"AI Industry Leaders Share Their Insights\" - Interviewing experts from various AI-related fields to discuss the latest trends and innovations.\n",
      "2. \"The Ethics of AI: A Conversation with a Leading Ethicist\" - Discussing the moral implications of AI development and deployment.\n",
      "3. \"From AI Researcher to Startup Founder: A Career Path\" - Sharing the experiences of someone who has transitioned from an academic or research role to starting their own AI-powered company.\n",
      "\n",
      "**Innovation Showcase**\n",
      "\n",
      "1. \"AI-Powered Robots: The Future of Manufacturing\" - Highlighting cutting-edge robot technology that uses AI for increased efficiency and productivity.\n",
      "2. \"Intelligent Homes: How AI is Changing Home Automation\" - Exploring the latest advancements in home automation, including smart speakers, voice assistants, and more.\n",
      "3. \"AI-Generated Art: The Rise of Creativity Assistants\" - Showcasing how AI is being used to generate art, music, and other creative works.\n",
      "\n",
      "**Debunking Common Myths**\n",
      "\n",
      "1. \"AI Will Replace Human Jobs\" - Debunking the common myth that AI will replace human workers entirely.\n",
      "2. \"Can AI Truly Think?\" - Exploring the limitations of current AI systems and what they can and cannot do.\n",
      "3. \"Is AI Safe for Everyday Use?\" - Addressing concerns about the safety and security of AI-powered devices.\n",
      "\n",
      "**Fun and Interactive**\n",
      "\n",
      "1. \"AI Trivia Challenge\" - Creating a fun and engaging trivia game that tests viewers' knowledge of AI-related topics.\n",
      "2. \"Can You Solve an AI Puzzle?\" - Presenting a puzzle or brain teaser that requires human intelligence to solve, but with the help of AI.\n",
      "3. \"The Future of Work: Can AI Assist Creative Professionals?\" - Exploring how AI can augment creative work and what this might mean for professionals in fields like writing, art, music, and more.\n",
      "\n",
      "**Educational Series**\n",
      "\n",
      "1. \"AI 101: An Introduction to Machine Learning\" - A series of videos that introduce the basics of machine learning, including supervised and unsupervised learning.\n",
      "2. \"The AI Development Process: From Concept to Deployment\" - Showcasing the entire AI development process, from research and design to testing and deployment.\n",
      "3. \"AI in the Enterprise: How to Implement an AI Strategy\" - Offering guidance on implementing an AI strategy within a business or organization.\n",
      "\n",
      "I hope these video ideas inspire you to create engaging content about Artificial Intelligence!\n"
     ]
    }
   ],
   "source": [
    "lcel_chain = prompt | model | output_parser\n",
    "\n",
    "# and run\n",
    "out = lcel_chain.invoke({\"topic\": \"Artificial Intelligence\"})\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Pipe Operator Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To truly grasp LCEL, let's examine how the pipe operation functions. It takes output from the **right** and feeds it to the **left**—but since this isn't standard Python, how is it implemented? We can create our own version using simple functions.\n",
    "\n",
    "We'll utilize the `__or__` method in Python class objects. When we combine two classes like `chain = class_a | class_b`, the Python interpreter checks for the presence of the `__or__` method in these classes. If it exists, the expression `|` is translated to `chain = class_a.__or__(class_b)`. \n",
    "\n",
    "This means both of the following patterns yield the same result:\n",
    "\n",
    "```python\n",
    "# Object approach\n",
    "chain = class_a.__or__(class_b)\n",
    "chain(\"some input\")\n",
    "\n",
    "# Pipe approach\n",
    "chain = class_a | class_b\n",
    "chain(\"some input\")\n",
    "```\n",
    "\n",
    "With this understanding, we can create a `Runnable` class that takes a function and transforms it into a chainable function using the pipe operator `|`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            return other(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement this to take the value `3`, add `5` (giving `8`), and multiply by `2` (giving `16`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_five(x):\n",
    "    return x + 5\n",
    "\n",
    "def multiply_by_two(x):\n",
    "    return x * 2\n",
    "\n",
    "add_five = Runnable(add_five)\n",
    "multiply_by_two = Runnable(multiply_by_two)\n",
    "\n",
    "chain = add_five.__or__(multiply_by_two)\n",
    "chain(3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `__or__` directly we get the correct answer, now let's try using the pipe operator `|` to chain them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = add_five | multiply_by_two\n",
    "\n",
    "chain(3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using either method we get the same response, at it's core this is what LCEL is doing, but there is more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL Deep Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand what this syntax is doing under the hood, let's explore it within the context of LCEL and see a few of the additional methods that LangChain has provided to maximize flexibility when working with LCEL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with LCEL we may find that we need to modify the structure or values being passed between components — for this we can use _runnables_. Let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Create an embedding model\n",
    "embedding = OllamaEmbeddings(model=\"snowflake-arctic-embed:33m\")\n",
    "\n",
    "# Create two vector stores with different texts\n",
    "vecstore_a = InMemoryVectorStore.from_texts(\n",
    "    [\"The capital of France is Paris\", \"The Eiffel Tower is in Paris\"],\n",
    "    embedding=embedding\n",
    ")\n",
    "vecstore_b = InMemoryVectorStore.from_texts(\n",
    "    [\"The Louvre is the world's largest art museum\", \"Paris is known for its cafe culture\"],\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating retrievers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_core.runnables import (\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough\n",
    ")\n",
    "\n",
    "retriever_a = vecstore_a.as_retriever()\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "prompt_str =\"\"\" Answer the following questions about Paris:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Asnwer: \"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "\n",
    "retriever = RunnableParallel(\n",
    "    {'context': retriever_a, 'question': RunnablePassthrough()},\n",
    ")\n",
    "\n",
    "chain = retrieval | prompt | model | output_parser\n",
    "\n",
    "pp(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is located in France.\n"
     ]
    }
   ],
   "source": [
    "out = chain.invoke(\"Where is Paris located?\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have used `RunnableParallel` to create two parallel streams of information, one for `\"context\"` that is information fed in from `retriever_a`, and another for `\"question\"` which is the _passthrough_ information, ie the information that is passed through from our `chain.invoke(\"when was James born?\")` call.\n",
    "\n",
    "Using this information the chain is close to answering the question but it doesn't have enough information, it is missing the information that we have stored in `retriever_b`. Fortunately, we can have multiple parallel information streams using the `RunnableParallel` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"Answer the question below using the context:\n",
    "\n",
    "Context:\n",
    "{context_a}\n",
    "{context_b}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context_a\": retriever_a, \"context_b\": retriever_b,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")\n",
    "\n",
    "chain = retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context, Paris is known for its cafe culture.\n"
     ]
    }
   ],
   "source": [
    "out = chain.invoke(\"What is Paris known for?\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnable Lambdas\n",
    "The `RunnableLambda` is a LangChain abstraction that allows us to turn Python functions into pipe-compatible function, similar to the `Runnable` class we created near the beginning of this notebook.\n",
    "\n",
    "Let's try it out with our earlier `add_five` and `multiply_by_two` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_root(x):\n",
    "    return x ** 0.5\n",
    "\n",
    "def calc_square(x):\n",
    "    return x ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# wrap the functions with RunnableLambda\n",
    "add_five = RunnableLambda(calc_root)\n",
    "multiply_by_two = RunnableLambda(calc_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with our earlier `Runnable` abstraction, we can use `|` operators to chain together our `RunnableLambda` abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableLambda</span><span style=\"font-weight: bold\">(</span>calc_root<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableLambda</span><span style=\"font-weight: bold\">(</span>calc_square<span style=\"font-weight: bold\">))</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\u001b[33mfirst\u001b[0m=\u001b[1;35mRunnableLambda\u001b[0m\u001b[1m(\u001b[0mcalc_root\u001b[1m)\u001b[0m, \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mlast\u001b[0m=\u001b[1;35mRunnableLambda\u001b[0m\u001b[1m(\u001b[0mcalc_square\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chain = add_five | multiply_by_two\n",
    "pp(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike our `Runnable` abstraction, we cannot run the `RunnableLambda` chain by calling it directly, instead we must call `chain.invoke`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9999999999999996"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can see the same answer. Naturally we can feed custom functions into our chains using this approach. Let's try a short chain and see where we might want to insert a custom function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Tell me an short fact about {topic}'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOllama</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span><span style=\"font-weight: bold\">)]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StrOutputParser</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfirst\u001b[0m=\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mtemplate\u001b[0m=\u001b[32m'Tell me an short fact about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mChatOllama\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mlast\u001b[0m=\u001b[1;35mStrOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_str = \"Tell me an short fact about {topic}\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "pp(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here's a short fact about Artificial Intelligence <span style=\"font-weight: bold\">(</span>AI<span style=\"font-weight: bold\">)</span>:\n",
       "\n",
       "The first AI program, called Logical Theorist, was developed in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1956</span> by Alan Newell and Herbert Simon. It was able \n",
       "to reason and solve problems using logical deduction, marking the beginning of the field of Artificial Intelligence\n",
       "research.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here's a short fact about Artificial Intelligence \u001b[1m(\u001b[0mAI\u001b[1m)\u001b[0m:\n",
       "\n",
       "The first AI program, called Logical Theorist, was developed in \u001b[1;36m1956\u001b[0m by Alan Newell and Herbert Simon. It was able \n",
       "to reason and solve problems using logical deduction, marking the beginning of the field of Artificial Intelligence\n",
       "research.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(chain.invoke({\"topic\": \"Artificial Intelligence\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Artificial Intelligence <span style=\"font-weight: bold\">(</span>AI<span style=\"font-weight: bold\">)</span> was first proposed by Alan Turing in his <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1950</span> paper <span style=\"color: #008000; text-decoration-color: #008000\">\"Computing Machinery and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Intelligence,\"</span> where he posed the question, <span style=\"color: #008000; text-decoration-color: #008000\">\"Can machines think?\"</span> This idea laid the foundation for modern AI \n",
       "research and development.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Artificial Intelligence \u001b[1m(\u001b[0mAI\u001b[1m)\u001b[0m was first proposed by Alan Turing in his \u001b[1;36m1950\u001b[0m paper \u001b[32m\"Computing Machinery and \u001b[0m\n",
       "\u001b[32mIntelligence,\"\u001b[0m where he posed the question, \u001b[32m\"Can machines think?\"\u001b[0m This idea laid the foundation for modern AI \n",
       "research and development.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(chain.invoke({\"topic\": \"Artificial Intelligence\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned text always includes the initial `\"Here's a short fact about ...\\n\\n\"` — let's add a function to split on double newlines `\"\\n\\n\"` and only return the fact itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableSequence</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">first</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'topic'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Tell me an short fact about {topic}'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">middle</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatOllama</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2'</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StrOutputParser</span><span style=\"font-weight: bold\">()]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">last</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RunnableLambda</span><span style=\"font-weight: bold\">(</span>extract_fact<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mRunnableSequence\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfirst\u001b[0m=\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mtemplate\u001b[0m=\u001b[32m'Tell me an short fact about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mmiddle\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mChatOllama\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2'\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mStrOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mlast\u001b[0m=\u001b[1;35mRunnableLambda\u001b[0m\u001b[1m(\u001b[0mextract_fact\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_fact(x):\n",
    "    if \"\\n\\n\" in x:\n",
    "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "get_fact = RunnableLambda(extract_fact)\n",
    "\n",
    "chain = prompt | model | output_parser | get_fact\n",
    "pp(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The term <span style=\"color: #008000; text-decoration-color: #008000\">\"Artificial Intelligence\"</span> <span style=\"font-weight: bold\">(</span>AI<span style=\"font-weight: bold\">)</span> was first coined in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1956</span> by John McCarthy, who used it to describe the goal\n",
       "of creating machines that could simulate human intelligence and think like humans.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The term \u001b[32m\"Artificial Intelligence\"\u001b[0m \u001b[1m(\u001b[0mAI\u001b[1m)\u001b[0m was first coined in \u001b[1;36m1956\u001b[0m by John McCarthy, who used it to describe the goal\n",
       "of creating machines that could simulate human intelligence and think like humans.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(chain.invoke({\"topic\": \"Artificial Intelligence\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Did you know that the first AI program, called <span style=\"color: #008000; text-decoration-color: #008000\">\"Logical Theorist,\"</span> was developed in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1956</span> by Allen Newell and \n",
       "Herbert Simon? This groundbreaking program could reason and solve problems using logical rules, marking the \n",
       "beginning of the field of Artificial Intelligence as we know it today.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Did you know that the first AI program, called \u001b[32m\"Logical Theorist,\"\u001b[0m was developed in \u001b[1;36m1956\u001b[0m by Allen Newell and \n",
       "Herbert Simon? This groundbreaking program could reason and solve problems using logical rules, marking the \n",
       "beginning of the field of Artificial Intelligence as we know it today.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp(chain.invoke({\"topic\": \"Artificial Intelligence\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're getting well formatted outputs using our final custom `get_fact` function.\n",
    "\n",
    "---\n",
    "\n",
    "That covers the essentials you need to getting started and building with the **L**ang**C**hain **E**xpression **L**anguage (LCEL). With it, we can put together chains very easily — and the current focus of the LangChain team is on further LCEL development and support.\n",
    "\n",
    "The pros and cons of LCEL are varied. Those that love it tend to focus on the minimalist code style, LCEL's support for streaming, parallel operations, and async, and LCEL's nice integration with LangChain's focus on chaining components together.\n",
    "\n",
    "There are also people that are less fond of LCEL. Typically people will point to it being yet another abstraction on top of an already very abstract library, that the syntax is confusing, against [the Zen of Python](https://peps.python.org/pep-0020/), and requires too much effort to learn new (or uncommon) syntax.\n",
    "\n",
    "Both viewpoints are entirely valid, LCEL is a very different approach — ofcourse there are major pros and major cons. In either case, if you're willing to spend some time learning the syntax, it allows us to develop very quickly, and with that in mind it's well worth learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
